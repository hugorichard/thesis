% \begin{abstract}
%   Group studies involving large cohorts of subjects are important to draw general conclusions about brain functional organization.
%   %
%   However, the aggregation of data coming from multiple subjects is challenging, since it requires accounting for large variability in anatomy, functional topography and stimulus response across individuals.
%   %
%   Data modeling is especially hard for ecologically relevant  conditions such as  movie watching, where the experimental setup does not imply well-defined cognitive operations.
%   %
%   %
%   We propose a novel MultiView Independent Component Analysis (ICA) model for group studies, where data from each subject are modeled as a linear combination of shared independent sources plus noise.
%   Contrary to most group-ICA procedures, the likelihood of the model is available in closed form.
%   %
%   We develop an alternate quasi-Newton method for maximizing the likelihood, which is robust and converges quickly.
%   %
%   We demonstrate the usefulness of our approach first on fMRI data, where our model demonstrates improved sensitivity in identifying common sources among subjects.
%   %
%   Moreover, the sources recovered by our model exhibit lower between-session variability than other methods.
%   %
%   On magnetoencephalography (MEG) data, our method yields more accurate source localization on phantom data.
%   %
%   Applied on $200$ subjects from the Cam-CAN dataset it reveals a clear sequence of evoked activity in sensor and source space.
% \end{abstract}
%
\section{Introduction}
\label{sec:intro}
The past decade has seen the emergence of two trends in neuroimaging: the collection of massive neuroimaging datasets, containing data from hundreds of participants~\cite{taylor2017cambridge,van2013wu,sudlow2015uk}, and the use of naturalistic stimuli to move closer to a real life experience with dynamic and multimodal stimuli~\cite{Sonkusare-etal:2019}.
%
% The analysis of these datasets is typically unsupervised when task and/or stimulation are difficult to quantify, such as in movie watching, yet this type of analysis offers insights on human brain function and useful individual markers.
%
Large scale datasets provide an unprecedented opportunity to assess the generality and validity of neuroscientific findings across subjects, with the potential of offering novel insights on human brain function and useful medical biomarkers.
%
% However, the analysis of such data is highly nontrivial --- particularly so when task and/or stimulation are difficult to quantify, such as in presence of natural stimuli.
%
However, when using ecological conditions, such as movie watching or simulated driving, 
stimulations are difficult to quantify. Consequently the statistical analysis of the data using
supervised regression-based approaches is difficult.
%
This has motivated the use of unsupervised learning methods that leverage the availability of
data from multiple subjects performing the same experiment; analysis on such large groups boosts statistical
power.

Independent component analysis~\cite{hyvarinen2000independent} (ICA) is a widely used unsupervised method for neuroimaging studies. It is routinely applied on individual subject electroencephalography (EEG)~\cite{makeig1996independent}, magnetoencephalography (MEG)~\cite{vigario1998independent} or functional MRI (fMRI)~\cite{mckeown1998independent} data. 
%
ICA models a set of signals as the product of a \emph{mixing matrix} and a \emph{source} matrix containing independent components.
%
The identifiability theory of ICA states that having non-Gaussian independent sources is a strong enough condition to recover the model parameters~\cite{comon1994independent}.
%
ICA therefore does not make assumptions about what triggers brain activations in the stimuli, unlike confirmatory approaches like the general linear model \cite{friston1994statistical, poline2012general}.
%
This explains why, in fMRI processing, it is a model of choice when analysing
resting state data \cite{beckmann2005investigations} or when subjects are
exposed to natural~\cite{malinen2007towards}~\cite{bartels2005brain} or complex stimuli such as simulated driving \cite{calhoun2002different}.
In M/EEG processing, it is widely used to isolate acquisitions artifacts from neural signal~\cite{jung1998extended}, and to identify brain sources of interest~\cite{vigario2000independent, delorme2012independent}.
% Besides, these models are typically univariate, thus not exploiting structure in the data, and lose statistical power in the presence of structured noise non orthogonal to the data.

%ICA is a tool of interest to conduct group studies.
%\bt{ICA is not primarily a group studies tool}
%
%\bt{From now on, you are implicitly assuming that sources are in the time domain, and mixing is spatial. Maybe worth mentioning.}\pa{On the contrary I think we should not restrict ourselves to such case. To me, we propose a novel way to perform group ica on any type of data.}
%
However, unlike with univariate methods, statistical inference about multiple subjects using ICA is not straightforward: so-called group-ICA is the topic of various studies~\cite{hyvarinen2013independent}.
%
Several works assume that the subjects share a common mixing matrix, but with different sources~\cite{pfister2019robustifying}~\cite{svensen2002ica}.
%
Instead, we focus on a model where the subjects share a common sources matrix, but have different mixing matrices.
%
When the subjects are exposed to the same stimuli, the common source matrix corresponds to the group \emph{shared responses}.
%\bt{Oops: for spatial ICA, this has been done for some time with models such as CanICA}
%
Most methods proposed in this framework proceed in two steps~\cite{calhoun2009review, huster2015group}.
%
First, the data of individual subjects are aggregated into a single dataset, often resorting to dimension reduction techniques like Principal Component Analysis (PCA).
%
Then, off-the-shelf ICA is applied on the aggregated dataset.
%
This popular method has the advantage of being simple and
straightforward to implement since it resorts to customary single-subject
ICA method.
%
However, it is not grounded in a principled probabilistic model of the problem, and does not have strong statistical guarantees like asymptotic efficiency. %\bt{Clarify what guranatees we're talking about ?}.
%
%Some ICA based neuroimaging studies aim at drawing general conclusions on brain organization \citep{allen2014tracking, sockeel2016large} or on brain's response to a presented stimulus \citep{van2008visual} given the observation of a cohort of subjects while others focus on the difference between brains of different groups of patients based for example on age \cite{kohler2008spatiotemporal} or mental health conditions \citep{assaf2010abnormal, jafri2008method, broyd2009default}. These studies use extensions of ICA that allows its application on groups. Note that groups are not necessarily groups of subjects, they can for example be groups of trials or sessions.
%
%Many group ICA methods exist (see the reviews for fMRI \citep{calhoun2009review, schmitthort2004comparison, guo2008unified} and EEG \citep{huster2018tutorial, huster2015group}). Most methods start by reducing the data of each subject separately using for instance a PCA. Then the reduced data are then merged before ICA is applied on the merging. Different techniques for merging lead to different methods the most popular being the GroupICA method implemented in GIFT \cite{calhoun2001method}. In GIFT, reduced data are merged using one or sometimes two \cite{erhardt2011comparison} successive applications of PCA.
%

We propose a novel group ICA method called \emph{MultiView ICA}.
%
It models each subject's dataset as a linear combination of a common
sources matrix with additive Gaussian noise.
%
Importantly, we consider that the noise is on the sources and not on
the sensors.
%
This greatly simplifies the likelihood of the model which can even be
written in closed-form.
%
% Despite its simplicity, our model can account for distinct covariances across subjects.
Despite its simplicity, our model allows for an expressive representation of inter-subject variability through subject-specific functional topographies (mixing matrices) and variability in the individual response (with noise in the source domain).
%
%\bt{no justification from a modeling perspective ?} \pa{I don't have a clear view of a scenario where it is more plausible than noise on the sensors, but it would definitely be good to have such example. Maybe talk about the rosetta stone problem?}
%
To the best of our knowledge, this is the first time that such a tractable likelihood is proposed for multi-subject ICA.
%
The likelihood formulation shares similarities with the usual ICA likelihood, which allows us to develop a fast and robust alternate quasi-Newton method for its maximization.


\textbf{Contribution}
%
In section~\ref{sec:mvica}, we introduce the MultiView ICA model, and show that it is identifiable. We then write its likelihood in closed form, and maximize it using an alternate quasi-Newton method.
%
We also provide a sensitivity analysis for MultiView ICA, and show that the choice of the noise parameter in the algorithm has little influence on the output.
%
In section~\ref{sec:rel_work}, we compare our approach to other group ICA methods.
%
Finally, in section~\ref{sec:expts}, we empirically verify through extensive experiments on fMRI and MEG data that it improves source identification with respect to competing methods, suggesting that the expressiveness and robustness of our model make it a useful tool for multivariate neural signal analysis.

%In section~\ref{sec:mvica}, we present our model and describe the optimization algorithm. We also analyze the robustness of our proposed procedure to model misspecification.
%In section~\ref{sec:rel_work}, we comment on the relationship between our work and other previously proposed models.
%In section~\ref{sec:expts}, we present an extensive experimental validation, on both synthetic and real data.
%In section~\ref{sec:conclusions}, we summarize our conclusions.



% (autism spectrum disorder \cite{assaf2010abnormal}, schizophrenia \cite{jafri2008method} see \cite{broyd2009default} for a complete review on studies linking default mode network abnormalities and mental disorders).

%It is often desirable to draw general conclusions on the response of the brain to a presented stimulus \cite{van2008visual} or on brain organization \cite{}  given the observation of individual responses in a large cohort of subjects.


%Group studies are one of the chief tools for this analysis: the aim is to draw general conclusions on the response of the brain to a presented stimulus given the observation of individual responses in a large cohort of subjects \addref. The availability of data coming from large number of individuals is critical for drawing valid conclusions about the functional organization of the human brain \addref.
%However, the aggregation of data coming from different subjects presents many challenges, ranging from the high variability in both the anatomy and the functional organization of the brain across subjects to the variability in the stimulus elicited response. 

% A first issue is the high variability in both the anatomy and the functional organization of the brain across subjects. While there are many methods to perform anatomical alignment, it is known that anatomical alignment does not correspond to alignment of functional topographies \addref. Furthermore, these procedures can imply a loss of relevant information, particularly when performed on high-resolution data. 
% Other problematic aspects have to do with the necessity of accounting for variability in the stimulus response, and also intra-subject variability as well when considering multiple presentations of the same stimulus --- %. 
% While the notion of a shared response to a given stimulus can be useful
% since the response evoked in each individual by a given stimulus might be affected by contingent factors (e.g. attention level, level of caffeine in the blood etc.), thus making the task of extracting the shared component of the response harder\addref. %Furthermore, the data relative to each of the subjects might be noisy and affected by experimental errors.

% The statistical analysis of data coming from multiple subjects should therefore ideally account for all of the above factors, using data coming from a group of individuals to extract a representation of the shared response. 

% A desirable feature of such representation would be interpretability; that is, distinct functional brain modules involved in the shared response should be distinguishable within the extracted representation.

%A popular alternative approach for tackling this problem is to employ an unsupervised learning scheme to extract the shared response. Independent component analysis \addref models the observed data as the linear mixing of a multivariate latent vector of features, whose components are assumed to be independent. This model has been widely used for feature extraction in neuroimaging studies \addref, has a clear likelihood formulation with strong identifiability guarantees, and efficient optimization procedures have been developed for it. While the basic framework is only directly applicable on individual subjects, many extensions to multi-subject data exist \addref; however, these approaches typically either lack a proper likelihood formulation or are very computationally demanding\addref
%Alternatively, models based on principal component analysis or canonical correlation analysis can be employed --- such as, for example, the shared response model~\cite{chen2017shared} (SRM); %provides a simple but effective framework to analyse fMRI data of subjects exposed to naturalistic stimuli.
%however, in order to retain computational efficiency, strong restrictions in generality are often imposed on the expressiveness of the model, which result in a lack of identifiability \luigi{should we mention it here? The fact that SRM is not identifiable is an original result of Pierre's}.
% namely imposing orthogonality of the unmixing matrices.
%
%
%\section{Shared Response Modeling (SRM)}
%\label{sec:sharedresponsemod}
%\input{sec/sharedresponsemod}
%
%
\section{Multiview ICA for Shared response modelling}
\label{sec:mvica}
\textbf{Notation} The absolute value of the determinant of a matrix $W$ is $|W|$.
%
The $\ell_2$ norm of a vector $\sbb$ is $\|\sbb\|$.
%
For a scalar valued function $f$ and a vector $\sbb \in \bbR^k$, we write $f(\sbb) = \sum_{j=1}^kf(s_j)$ and denote $f'$ the gradient of $f$.
%
\emph{All proofs are deferred to appendix~\ref{sec:app_proofs}}.
%
\subsection{Model, likelihood and approximation}
%
Given $m$ subjects, we model the data $\xb^i\in\bbR^k$ of subject $i$ as
\begin{equation}
\label{eq:ica_model}
\boxed{
    \xb^i = A^i(\sbb + \nb^i), \enspace i=1,\dots, m
    }
\end{equation}
%\bt{Matrices as bold letters ?} \pa{I find it understandable to have nothing but matrices in uppercase}
where $\sbb = [s_1, \dots, s_k]^{\top} \in \bbR^k$ are the shared independent sources, $\nb^i \in \bbR^k$ is individual noise, $A^i \in \bbR^{k\times k}$ are the individual mixing matrices, assumed to be full-rank.
%
We assume that samples are observed i.i.d. For simplicity, we assume that the sources share the same density $d$, so that the independence assumption is $p(\sbb) = \prod_{j=1}^k d(s_j)$. Finally, we assume that the noise is Gaussian decorrelated of variance $\sigma^2$, $\nb^i \sim \mathcal{N}(0, \sigma^2I_k)$, and that the noise is independent across subjects and independent from the sources.
The assumption of additive white noise on the sources models individual deviations from the shared sources $\sbb$.
It is equivalent to having noise on the sensors with covariance $\sigma^2 A^i \left(A^i\right)^{\top}$, i.e. a scaled version of the data covariance without noise.

Since the sources are shared by the subjects, there are many more observed variables than sources in the model: there are $k$ sources, while there are $k \times m$ observations.
%
Therefore, model~\eqref{eq:ica_model} can be seen as an instance of \emph{undercomplete} ICA.
%
The goal of multiview ICA is to recover the mixing matrices $A^i$ from observations of the $\xb^i$.
%
The following proposition extends the standard idenfitiability theory of ICA~\cite{comon1994independent} to multiview ICA, and shows that recovering the sources/mixing matrices is a well-posed problem up to scale and permutation.
%
\begin{proposition}[Identifiability of MultiView ICA]
\label{prop:identifiability}
Consider $\xb^i, \enspace i=1\dots m,$ generated from~\eqref{eq:ica_model}. Assume that $\xb^i = A'^i(\sbb' + \nb'^i)$ for some invertible matrices $A'^i\in \bbR^{k\times k}$, independent non-Gaussian sources $\sbb'\in \bbR^k$ and Gaussian noise $\nb'^i$. Then, there exists a scale and permutation matrix $P\in \bbR^{k\times k}$ such that for all $i$, $A'^i = A^i P$.
\end{proposition}
%
%
%A simple method to solve~\eqref{eq:ica_model} is to perform an individual ICA of each subject, therefore obtaining the individual mixing matrices $A^i$ and the sources corrupted with noise $\sbb + \nb^i$.
%
%The first problem with this approach is that there is an inherent scale and permutation ambiguity in ICA, which means that the recovered sources can be swapped across subjects. 
%
%The sources should therefore be matched.
%
%Second, using the data from one subject at a time instead of all the available data to estimate the mixing matrices leads to a loss in statistical power: one would need more samples to estimate properly the mixing matrices using an individual ICA than by using all datasets. 
%
%This problem is illustrated in the experiments.
%
We propose a maximum-likelihood approach to estimate the mixing matrices. 
We denote by $W^i = (A^i)^{-1}$ the unmixing matrices, and view the likelihood as a function of $W^i$ rather than $A^i$. As shown in Appendix~\ref{sec:appendix:likelihood_transform}, the negative log-likelihood can be written by integrating over the sources
\begin{equation} 
    \label{eq:likelihood}
    \loss(W^1, \dots, W^m) = -\sum_{i=1}^m\log|W^i| - \log\left(\int_{\sbb}\exp\left(-\frac1{2\sigma^2}\sum_{i=1}^m\|W^i\xb^i - \sbb\|^2\right)p(\sbb)d\sbb\right),
\end{equation}
up to additive constants.
%\hr{I think there is a missing term here: I think $log|W_i|$ should actually be $log|\frac{W_i}{\sigma}|$ does not change the optimization though}\pa{yes we should be clearer that we omit everything not depending on W}
%where $f$ is the mapping $f(\sbb) = -\sum_{j=1}^k \log(d(s_j))$.
%
Since this integral factorizes, i.e.\ the integrand is a product of functions of $s_j$, we can perform the integration as shown in Appendix~\ref{sec:appendix:integration}. We define a smoothened version of the logarithm of the source density $d$ by convolution with a Gaussian kernel as
$
    f(s)= \log \left(\int \exp(-\frac{m}{2\sigma^2} z^2) d(s-z) dz\right)
$
%\bt{For clarity, mention that f is a log density, that  of the sources convolved with a gaussian kernel ?}
%In order to obtain an approximation, we use Laplace's method \cite{erdelyi1956asymptotic}, which states that when the noise level is small, we have $$ \log\left(\int_{\sbb}\exp\left(-\frac1{2\sigma^2}\sum_{i=1}^m\|W^i\xb^i - \sbb\|^2 - f(\sbb)\right)d\sbb\right) \simeq-\frac1{2\sigma^2}\sum_{i=1}^m\|W^i\xb^i - \tilde{\sbb}\|^2 - f(\tilde{\sbb}) +  \text{Cst.}, $$
and $\tilde{\sbb} = \frac1m\sum_{i=1}^m W^i\xb^i$ the source estimate.
The negative log-likelihood becomes
\begin{equation}
    \label{eq:cost_function}
    \loss(W^1, \dots, W^m) = -\sum_{i=1}^m \log|W^i| + \frac1{2\sigma^2}\sum_{i=1}^m\|W^i\xb^i - \tilde{\sbb}\|^2 + f(\tilde{\sbb}).
\end{equation}
Multiview ICA is then performed by minimizing $\loss$, and the estimated shared sources are $\tilde{\sbb}$.
The negative log-likelihood $\loss$ is quite simple, and importantly, can be computed easily given the parameters of the model and the data; it does not involve any intractable integral.
%

For one subject ($m=1$), $\loss(W^1)$ simplifies to the negative log-likelihood of ICA and we recover Infomax~\cite{bell1995information,cardoso1997infomax}, where the source log-pdf is replaced with the smoothened $f$.
%\ag{you have no noise in Infomax. I am confused.}\pa{In the one subject case (usual ica), if you assume gaussian noise on the sources, you recover the same cost function as usual infomax, but with a different non-linearity}
%
\subsection{Alternate quasi-Newton method for MultiView ICA}
%
The parameters of the model are estimated by minimizing $\loss$.
%
We propose a combination of quasi-Newton method and alternate minimization for this task.
%
First, $\mathcal{L}$ is non-convex: it is only defined when the $W^i$ are invertible, which is a non-convex set.
%
Therefore, we only look for local minima as usual in ICA.
%
We propose an alternate minimization scheme, where $\loss$ is alternatively diminished with respect to each $W^i$. 
%
When all matrices $W^1, \dots, W^m$ are fixed but one, $W^i$, $\loss$ can be rewritten, up to an additive constant 
\begin{equation}
    \label{eq:indiv_loss}
    \loss^i(W^i) = -\log|W^i| + \frac{1 - 1/m}{2\sigma^2}\|W^i\xb^i - \frac{m}{m-1}\tilde{\sbb}^{-i}\|^2 + f(\frac1m W^i \xb^i +\tilde{\sbb}^{-i}), 
\end{equation}
with $\tilde{\sbb}^{-i} = \frac1m \sum_{j \neq i}W^j \xb^j$.
%
This function has the same structure as the usual maximum-likelihood ICA cost function: it is written $\loss^i(W^i) = -\log|W^i| + g(W^i\xb^i)$, where $g(\yb) = \sum_{j=1}^kf(\frac{y_j}{m} + \tilde{\sbb}^{-i}_j) + \frac{1 - 1/m}{2\sigma^2}(y_j - \frac{m}{m-1}\tilde{\sbb}^{-i}_j)^2$.
%
Fast quasi-Newton algorithms ~\cite{zibulevsky2003blind, ablin2018faster} have been proposed for minimizing such functions.
%
We employ a similar technique as~\cite{zibulevsky2003blind}, which we now describe.

Quasi-Newton methods are based on approximations of the Hessian of $\loss^i$.
%
The relative gradient (resp. Hessian)~\cite{amari1996new, cardoso1996equivariant} of $\loss^i$ is defined as the matrix $G^i\in \bbR^{k \times k}$ (resp. tensor $\mathcal{H}^i \in \bbR^{k\times k\times k\times k}$) such that as the matrix $E\in\bbR^{k\times k}$ goes to $0$, we have $\loss^i((I_k + E)W^i) \simeq \loss^i(W^i) + \langle G^i, W^i\rangle + \frac12\langle E, \mathcal{H}^iE\rangle$. Standard manipulations yield:
\begin{equation}
    \label{eq:gradient}
    G^i = \frac1mf'(\tilde{\sbb})(\yb^i)^{\top} + \frac{1 - 1 /m}{\sigma^2}(\yb^i - \frac{m}{m-1}\tilde{\sbb}^{-i})(\yb^i)^{\top} - I_k, \text{ where } \yb^i= W^i\xb^i
\end{equation}
\begin{equation}
    \label{eq:hessian}
    \mathcal{H}^i_{abcd} = \delta_{ad}\delta_{bc} + \delta_{ac}\left(\frac{1}{m^2}f''(\tilde{\sbb}_a) + \frac{1 - 1/m}{\sigma^2}\right)\yb^i_{b}\yb^i_d,\enspace \text{for }a, b, c, d =1\dots k
\end{equation}

Newton's direction is then $-\left(\mathcal{H}^i\right)^{-1}G^i$. However, this Hessian is costly to compute (it has $\simeq k^3$ non-zero coefficients) and invert (it can be seen as a big $k ^2\times k^2$ matrix). Furthermore, to enforce that Newton's direction is a descent direction, the Hessian matrix should be regularized in order to eliminate its negative eigenvalues~\cite{nocedal2006numerical}, and $\mathcal{H}^i$ is not guaranteed to be positive definite.
%
These obstacles render the computation of Newton's direction impractical.
%
Luckily, if we assume that the signals in $\yb^i$ are independent, severall coefficients cancel, and the Hessian simplifies to the approximation
%\bt{IIUC if the sources are independent this is not an approximation} \pa{It is an approximation because even if the sources are independent, you have $y^i = s^i$ only when $W^i$ is the true unmixing matrix, i.e. when the algo has converged}
\begin{equation}
    \label{eq:hessian_approx}
    H^i_{abcd} = \delta_{ad}\delta_{bc} + \delta_{ac}\delta_{bd}\Gamma^i_{ab}\enspace \text{with  }\Gamma^i_{ab} = \left(\frac{1}{m^2}f''(\tilde{\sbb}_a) + \frac{1 - 1/m}{\sigma^2}\right)\left(\yb^i_{b}\right)^2.
\end{equation}
This approximation is sparse: it only has $k(2k -1)$ non-zero coefficients.
%
In order to better understand the structure of the approximation, we can compute the matrix $\left(H^iM\right)$ for $M\in \bbR^{k\times k}$. 
%
We find $\left(H^iM\right)_{ab} = \Gamma^i_{ab}M_{ab} + M_{ba}$: $H^iM_{ab}$ only depends on $M_{ab}$ and $M_{ba}$, indicating a simple block diagonal structure of $H^i$.
%
The tensor $H^i$ is therefore easily regularized and inverted:
$\left((H^i)^{-1}M\right)_{ab} = \frac{\Gamma^i_{ba}M_{ab} - M_{ba}}{\Gamma^i_{ab}\Gamma^i_{ba} - 1}$.
%
Finally, since this approximation is obtained by assuming that the $\yb^i$ are independent, the direction $-\left(H^i\right)^{-1}G^i$ is close to Newton's direction when the $\yb^i$ are close to independence, leading to fast convergence.
%
Algorithm~\ref{algo:mv_ica} alternates one step of the quasi-Newton method for each subject until convergence.
%
A backtracking line-search is used to ensure that each iteration leads to a decrease of $\loss^i$.
%
The algorithm is stopped when maximum norm of the gradients over one pass on each subject is below some tolerance level, indicating that the algorithm is close to a stationary point.

\begin{algorithm}[H]
\label{algo:mv_ica}
\SetAlgoLined
\KwIn{Dataset $(\xb^i)_{i=1}^m$, initial unmixing matrices $W^i$, noise parameter $\sigma$, function $f$,  tolerance $\varepsilon$}
Set tol$=+\infty$, $\tilde{\sbb} = \frac1m\sum_{i=1}^kW^i\xb^i$\\
 \While{\text{tol}$>\varepsilon$}{
 tol = 0 \\
  \For{$i=1\dots m$}{
  Compute $\yb^i = W^i \xb^i$, $\tilde{\sbb}^{-i} = \tilde{\sbb} - \frac1m\yb^i$, gradient $G^i$ (eq.~\eqref{eq:gradient}) and Hessian $H^i$ (eq.~\eqref{eq:hessian_approx})\\
  Compute the search direction $D = -\left(H^i\right)^{-1}G^i$\\
  Find a step size $\rho$ such that $\loss^i((I_k + \rho D)W^i) < \loss^i(W^i)$ with line search\\
  Update $\tilde{\sbb} = \tilde{\sbb} + \frac{\rho}{m} DW^i \xb^i$, $W^i = (I_k + \rho D)W^i$, tol$=\max($tol$,\|G^i\|)$\\
  }
 }
 \Return{Estimated unmixing matrices $W^i$, estimated shared sources $\tilde{\sbb}$}
 \caption{Alternate quasi-Newton method for MultiView ICA}
\end{algorithm}
%
%
%
%
\subsection{Robustness to model misspecification}
Algorithm~\ref{algo:mv_ica} has two hyperparameters: $\sigma$ and the function $f$.
%
The latter is usual for an ICA algorithm, but the former is not.
%
We study the impact of these parameters on the separation capacity of the algorithm, when these parameters do not correspond to those of the generative model~\eqref{eq:ica_model}.
%
\begin{proposition}
\label{prop:robust}
We consider the cost function $\loss$ in eq.~\eqref{eq:cost_function} with noise parameters $\sigma$ and function $f$.
%
Assume sub-linear growth on $f'$: $|f'(x)|\leq c|x|^{\alpha} + d$ for some $c, d > 0$ and $0<\alpha<1$.
%
Assume that $\xb^i$ is generated following model~\eqref{eq:ica_model}, with noise parameter $\sigma'$ and density of the source $d'$ which need not be related to $\sigma$ and $f$.
%
Then, there exists a diagonal matrix $\Lambda$ such that $(\Lambda (A^1)^{-1}, \dots, \Lambda (A^m)^{-1})$ is a stationary point of $\loss$, that is $G^1,\dots, G^m =0$ at this point.
\end{proposition}
%
The sub-linear growth of $f'$ is a customary hypothesis in ICA which implies that $d$ has heavier-tails than a Gaussian, and in appendix~\ref{ref:robust} we provide other conditions for the result to hold.
%
In this setting, the shared sources estimated by the algorithm are $\tilde{\sbb} = \Lambda (\sbb + \frac1m \sum_{i=1}^m \nb^i)$, which is a scaled version of the best estimate of the shared sources under the Gaussian noise hypothesis.

This proposition shows that, up to scale, the true unmixing matrices are a stationary point for Algorithm~\ref{algo:mv_ica}: if the algorithm starts at this point it will not move.
%
The question of stability is also interesting: if the algorithm is initialized ~\emph{close} to the true unmixing matrices, will it converge to the true unmixing matrix?
%
In the appendix~\ref{sec:stability}, we provide an analysis similar to~\cite{cardoso1998blind}, and derive sufficient numerical conditions for the unmixing matrices to be local minima of $\mathcal{L}$.
%
We also study the practical impact of changing the hyperparameter $\sigma$ on the accuracy of a machine learning pipeline based on MultiviewICA on real fMRI data in the appendix Sec.~\ref{sec:app_sigma_impact}.
%
As expected from the theoretical study, the performance of the algorithm is barely affected by $\sigma$.
\subsection{Dimensionality reduction}
%
So far, we have assumed that the dimensionality of each view (subject) and that of the sources is the same. This reflects the standard practice in ICA of having equal number of observations and sources. 
%
In practice, however, we might want to estimate fewer sources than there are observations per view; the original dimensionality of the data %(number of voxels, sensors) 
might in practice not be computationally tractable.
%
The problem of how to perform subject-wise dimensionality reduction in group studies 
% of data for each of the individuals while still considering them jointly and preserving the signal shared across them
is an interesting one \emph{per se}, and out of the main scope of this work. For our purposes, it can be considered as a preprocessing step for which well-known various solutions can be applied. % step prior to the application of our method,. 
We discuss this further in section~\ref{sec:rel_work} and in appendix~\ref{sec:app_rel_work}.
%
%
%
\section{Related Work}
\label{sec:rel_work}
Many methods for data-driven multivariate analysis of neuroimaging group studies have been proposed. We summarize the characteristics of some of the most commonly used ones. A more thorough description of these methods can be found in appendix~\ref{sec:app_rel_work}.
%
For completeness, we start by describing PCA. For a zero-mean data matrix $X$ of size $p\times n$ with $p \leq n$, we denote $X= UD V^{\top}$ the singular value decomposition of $X$ where $U \in \bbR^{p\times p}$, $V \in \bbR^{n \times p}$ are orthogonal and $D$ the diagonal matrix of singular values ordered in decreasing order.
%
The PCA of $X$ with $k$ components is $Y\in\bbR^{k\times n}$ containing the first $k$ rows of $DV^{\top}$, and it does not hold in general that $YY^{\top}=I_k$: for the rest of the paper, what we call PCA does not include whitening of the signals.


\textbf{Group ICA} When datasets are high-dimensional, a three steps procedure is often used: first dimensionality reduction is performed on data of each subject  separately; then the reduced data are merged into a common representation; finally, an ICA algorithm is applied for shared source extraction. The merging of the reduced data is often done by PCA \cite{calhoun2001method} or multi set CCA \cite{varoquaux2009canica}.
%Note that even with large datasets, it can still be computationally feasible to do group level reduction in one step (see \cite{chen2015reduced} or \cite{smith2014group}).
This is a popular method for fMRI~\cite{calhoun2009review} and EEG~\cite{eichele2011eegift} group studies.
These methods directly recover only group level, shared sources; when individual sources are needed, additional steps are required (back-projection \cite{calhoun2001method} or dual-regression \cite{beckmann2009group}).
%
In contrast, MultiView ICA finds individual and shared independent components in a single step.
%
%
Finally, in contrast to the methods described above, our method maximizes a likelihood, which brings statistical guarantees like consistency or asymptotic efficiency.
%
The SR-ICA approach of \cite{zhang2016searchlight} performs dimension reduction, merging and independent component estimation. It is therefore similar to our method.
%
However, they propose to modify the FastICA algorithm~\cite{hyvarinen1999fast} in a rather heuristic way, without specifying an optimization problem, let alone maximizing a likelihood. In the experiments on fMRI data in appendix~\ref{appendix_reproduce}, we obtain better performance with MultiView ICA than the reported performance of SR-ICA.
%
%

\textbf{Likelihood-based models} One can consider the more general model $\xb^i = A^i\sbb^i + \nb^i$, where the noise covariance can be learned from the data~\cite{guo2008unified}.
%
The likelihood for this model involves an intractable high dimensional integral that is cumbersome to evaluate, and is then optimized with the Expectation-Maximization (EM) algorithm, which is known to converge slowly and unreliably~\cite{bermond1999approximate, petersen2005slow}.
%
Having the simpler model~\eqref{eq:ica_model} leads to a closed-form likelihood, that can then be optimized by more efficient means than the EM algorithm.
In model~\eqref{eq:ica_model}, the noise can be interpreted as individual variability rather than sensor noise. %It offers a way to capture more structured noise as is often the case in brain signals.
% It offers a way to capture more structured noise, which is often present in neuroimaging recordings~\cite{engemann2015automated}.
In appendix~\ref{app:complex_cov}, we generate data following model $\xb^i = A^i\sbb^i + \nb^i$ and report the reconstruction error. The difference in performance between algorithms is small. 
%
%
%

\textbf{Structured mixing matrices} One strength of our model is that we only assume that the mixing matrices are invertible and still enjoy identifiability whereas some other approaches impose additional constraints. For instance tensorial methods~\cite{beckmann2005tensorial} assume that the mixing matrices are the same up to diagonal scaling.
%
Other methods impose a common mixing matrix~\cite{cong2013validating, grin2010independent, calhoun2001fmri, Monti18UAI}. Like PCA, the Shared Response Model~\cite{chen2015reduced} (SRM) assumes orthogonality of the mixing matrices. While the model defines a simple likelihood and provides an efficient way to reduce dimension, the SRM model is not identifiable as shown in appendix~\ref{sec:app_identifiability}, and the orthogonal constraint may not be plausible.
%

\textbf{Matching sources a posteriori} A different path to multi-subject ICA is to extract independent components with individual ICA in each subject and align them. We propose a simple baseline approach to do so called \emph{PermICA}.
Inspired by the heuristic of the hyperalignment method~\cite{haxby2011common} we choose a reference subject and first match the sources of all other subjects to the sources of the reference subject. The process is then repeated multiple times, using the average of previously aligned sources as a reference. Finally, group sources are given by the average of all aligned sources. We use the Hungarian algorithm to align pairs of mixing matrices~\cite{tichavsky2004optimal}.
%
Alternative approaches involving clustering have also been developed~\cite{esposito2005independent,bigdely2013measure}.

\textbf{Deep Learning} Deep Learning methods, such as convolutional auto-encoders (CAE), can also be used to find the subject specific unmixing~\cite{chen2016convolutional}. While these nonlinear extensions of the aforementioned methods are interesting, these models are hard to train and interpret. In the experiments on fMRI data in appendix~\ref{appendix_reproduce}, we obtain better accuracy with MultiView ICA than that of CAE reported in~\cite{chen2016convolutional}.

\textbf{Correlated component analysis} Other methods can be used to recover the shared neural responses such as the correlated component approach of Dmochowski~\cite{dmochowski2012correlated}. We benchmark our method against its probabilistic version~\cite{kamronn2015multiview} called BCorrCA in Figure~\ref{fig:meg}. Our method yields much better results. 

\textbf{Autocorrelation} Another way to perform ICA is to leverage spectral diversity of the sources rather than non-Gaussianity.
%
These methods are popular alternative to non-Gaussian ICA in the single-subject setting~\cite{tong1991indeterminacy, belouchrani1997blind, pham1997blind} and they output significantly different sources than non-Gaussian ICA~\cite{delorme2012independent}.
%
Extensions to multiview problems have been proposed~\cite{lukic2002ica, congedo2010group}.
\vspace{-5pt}
%
%
%Our method is demonstrated on multiple neuroscience data, where the use of ICA is widespread.
%
%Applied to a group study of hundreds of subjects, it has the potential to give unprecedented insights on the human brain.