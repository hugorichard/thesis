In chapter~\ref{ch:fastsrm1} and chapter~\ref{ch:fastsrm2}, we have
introduced a fast version of the shared response model (SRM). While
SRM provides a useful dimension reduction framework, it assumes
orthogonality of the mixing matrices, which is not biologically
plausible.

In this chapter, we propose a novel group ICA method called \emph{MultiView ICA}.
In contrast to most Group ICA methods, MultiViewICA is grounded in a
probabilistic model of the problem and comes with statistical guarantees such as
asymptotic efficiency.

MultiViewICA models each subject's dataset as a linear combination of a common
components matrix with additive Gaussian noise.
% 
Importantly, we consider that the noise is on the components and not on
the sensors.
% 
This greatly simplifies the likelihood of the model which can even be
written in closed-form.

Despite its simplicity, MultiView ICA allows for an expressive representation of inter-subject variability through subject-specific functional topographies (mixing matrices) and variability in the individual response (with noise in the component domain).
% 
To the best of our knowledge, this is the first time that such a tractable likelihood is proposed for multi-subject ICA.
% 
The likelihood formulation shares similarities with the usual ICA likelihood, which allows us to develop a fast and robust alternate quasi-Newton method for its maximization.

We first introduce the MultiView ICA model, and show that it is identifiable. We then write its likelihood in closed form, and maximize it using an alternate quasi-Newton method.
%
We also provide a sensitivity analysis for MultiView ICA, and show that the choice of the noise parameter in the algorithm has little influence on the output.
\section{Multiview ICA for Shared response modelling}
\label{sec:mvica}
\subsection{Model, likelihood and approximation}
%
Given $m$ subjects, we model the data of subject $i$ as a random vector
$\xb_i\in\bbR^p$  such that:
\begin{equation}
\label{eq:ica_model}
    \xb_i = A_i(\sbb + \nb_i), \enspace i=1,\dots, m
\end{equation}
%\bt{Matrices as bold letters ?} \pa{I find it understandable to have nothing but matrices in uppercase}
where $\sbb \in \bbR^p$ are the shared independent components, $\nb_i \in \bbR^p$ is individual noise, $A_i \in \bbR^{p\times p}$ are the individual mixing matrices, assumed to be full-rank.
%
We assume that samples are observed i.i.d. For simplicity, we assume that the components share the same density $\delta$, so that the independence assumption is $p(\sbb) = \prod_{j=1}^p \delta(s_j)$. Finally, we assume that the noise is Gaussian decorrelated with variance $\sigma^2$, $\nb_i \sim \mathcal{N}(0, \sigma^2I_p)$, and that the noise is independent across subjects and independent from the components.
The assumption of additive white noise on the components models individual deviations from the shared components $\sbb$.
It is equivalent to having noise on the sensors with covariance $\sigma^2 A_i \left(A_i\right)^{\top}$, i.e. a scaled version of the data covariance without noise.

Since the components are shared by the subjects, there are many more observed variables than components in the model: there are $p$ components, while there are $p \times m$ observations.
%
Therefore, model~\eqref{eq:ica_model} can be seen as an instance of \emph{undercomplete} ICA.
%
The goal of multiview ICA is to recover the mixing matrices $A_i$ from observations of the $\xb_i$.
%
The following proposition extends the standard idenfitiability theory of ICA~\cite{comon1994independent} to multiview ICA, and shows that recovering the components/mixing matrices is a well-posed problem up to scale and permutation.
%
\begin{proposition}[Identifiability of MultiView ICA]
\label{prop:identifiability}
Consider $\xb_i, \enspace i=1\dots m,$ generated from~\eqref{eq:ica_model}. Assume that $\xb_i = A'_i(\sbb' + \nb'_i)$ for some invertible matrices $A'_i\in \bbR^{p\times p}$, independent non-Gaussian components $\sbb'\in \bbR^p$ and Gaussian noise $\nb'_i$. Then, there exists a scale and permutation matrix $P\in \bbR^{p\times p}$ such that for all $i$, $A'_i = A_i P$.
\end{proposition}
The proof is available in appendix~\ref{app:proof:mvica:identifiability}.

We propose a maximum-likelihood approach to estimate the mixing matrices. 
We denote by $W_i = (A_i)^{-1}$ the unmixing matrices, and view the likelihood
as a function of $W_i$ rather than $A_i$.

To derive the likelihood, we start by conditioning on $\sbb$. Then, we make a variable transformation from $\xb_i$ to $\nb_i=W_i\xb_i-\sbb$, as opposed to the transformation to $\sbb$ as is usual in ICA. Using the probability transformation formula, we obtain
\begin{equation}
p_{\xb_i|\sbb}(\xb_i|\sbb)=|W_i|p_{\nb_i}(W_i\xb_i-\sbb)    
\end{equation}
where $p_{\nb_i}$ is the density of $\nb_i$. Note that the $\xb_i$ are conditionally independent given $\sbb$, so we have:
\begin{equation}
  p_{\xb|\sbb}(\xb|\sbb)=\prod_{i=1}^m  |W_i| p_{\nb_i}(W_i\xb_i-\sbb)
\end{equation}
and we next get the joint density as:
\begin{equation}
  p_{\xb, \sbb}(\xb,\sbb)=p_{\sbb}(\sbb) \prod_{i=1}^m  |W_i| p_{\nb^i}(W_i\xb_i-\sbb)
\end{equation}

Integrating out $\sbb$ and taking the log gives the negative likelihood:
\begin{align} 
  \label{eq:likelihood}
 &\loss(W_1, \dots, W_m) =  \nonumber
  \\& -\sum_{i=1}^m\log|W_i| -  \log\left(\int_{\sbb}\exp\left(-\frac1{2\sigma^2}\sum_{i=1}^m\|W_i\xb_i - \sbb\|^2\right)p(\sbb)d\sbb\right)
\end{align}
up to additive constants.

The integral in~\ref{eq:likelihood} after factorization, is given by
\begin{equation}
\int_{\sbb} \prod_{j=1}^p \exp \left( -\frac{1}{2\sigma^2} \sum_{i=1}^m (W_{ij}^{\top}\xb_i-s_j)^2 \right) \delta(s_j) d\sbb
\end{equation}
where $W_{ij}$ is the $j$-th line of $W_i$. Denote $y_{ij}=W_{ij}^{\top}\xb_i$ and $\tilde{s_j}=\frac1m\sum_{i=1}^m y_{ij}$.  Fix $j$, and drop it to simplify notation. Then we need to solve the integral
\begin{align*}
   &\int_s \exp \left(-\frac{1}{2\sigma^2} \sum_{i=1}^m (y_i-s)^2 \right) \delta(s)ds\\
   &=\int_s \exp \left(-\frac{1}{2\sigma^2} [ m(\tilde{s}-s)^2 + \sum_{i=1}^m (y_i-\tilde{s})^2] \right) \delta(s)ds \\ 
&= \exp \left(-\frac{1}{2\sigma^2}\sum_{i=1}^m (y_i-\tilde{s})^2 \right) 
\int_z \exp \left(-\frac{m}{2\sigma^2} z^2 \right) \delta(\tilde{s}-z) dz
\end{align*}

where we have made the change of variable $z=\tilde{s}-s$. The remaining integral simply means that $\delta$ is smoothed by a Gaussian kernel, which can be computed exactly if $\delta$ is a Gaussian mixture. We therefore define $f(s) = -\log \left(\int_z \exp \left(-\frac{m}{2\sigma^2} z^2 \right) \delta(s-z) dz\right)$.

The negative log-likelihood becomes
\begin{equation}
    \label{eq:cost_function}
    \loss(W_1, \dots, W_m) = -\sum_{i=1}^m \log|W_i| + \frac1{2\sigma^2}\sum_{i=1}^m\|W_i\xb_i - \tilde{\sbb}\|^2 + f(\tilde{\sbb}).
\end{equation}
Multiview ICA is then performed by minimizing $\loss$, and the estimated shared components are $\tilde{\sbb}$.
The negative log-likelihood $\loss$ is quite simple, and importantly, can be computed easily given the parameters of the model and the data; it does not involve any intractable integral.
%

For one subject ($m=1$), $\loss(W_1)$ simplifies to the negative log-likelihood of ICA and we recover Infomax~\cite{bell1995information,cardoso1997infomax}, where the component log-pdf is replaced with the smoothed $f$.
%

\subsection{Alternate quasi-Newton method for MultiView ICA}
%
The parameters of the model are estimated by minimizing $\loss$.
%
We propose a combination of quasi-Newton method and alternate minimization for this task.
%
First, $\mathcal{L}$ is non-convex: it is only defined when the $W_i$ are invertible, which is a non-convex set.
%
Therefore, we only look for local minima as usual in ICA.
%
We propose an alternate minimization scheme, where $\loss$ is alternatively diminished with respect to each $W_i$. 
%
When all matrices $W_1, \dots, W_m$ are fixed but one, $W_i$, $\loss$ can be rewritten, up to an additive constant 
\begin{align}
    \label{eq:indiv_loss}
    &\loss_i(W_i) = \nonumber  \\&-\log|W_i| + \frac{1 - 1/m}{2\sigma^2}\|W_i\xb_i - \frac{m}{m-1}\tilde{\sbb}_{-i}\|^2 + f(\frac1m W_i \xb_i +\tilde{\sbb}_{-i}), 
\end{align}
with $\tilde{\sbb}_{-i} = \frac1m \sum_{j \neq i}W_j \xb_j$.
%
This function has the same structure as the usual maximum-likelihood ICA cost
function: it is written $\loss_i(W_i) = -\log|W_i| + g(W_i\xb_i)$, where $g(\yb)
= \sum_{j=1}^pf(\frac{y_j}{m} + \tilde{\sbb}_{-i, j}) + \frac{1 -
  1/m}{2\sigma^2}(y_j - \frac{m}{m-1}\tilde{\sbb}_{-i, j})^2$ where
$\tilde{\sbb}_{-i, j}$ is the $j$-th component of $\tilde{\sbb}_{-i}$.
%
Fast quasi-Newton algorithms ~\cite{zibulevsky2003blind, ablin2018faster} have been proposed for minimizing such functions.
%
We employ a similar technique as~\cite{zibulevsky2003blind}, which we now describe.

Quasi-Newton methods are based on approximations of the Hessian of $\loss_i$.
%
The relative gradient (resp. Hessian)~\cite{amari1996new, cardoso1996equivariant} of $\loss_i$ is defined as the matrix $G_i\in \bbR^{p \times p}$ (resp. tensor $\mathcal{H}_i \in \bbR^{p\times p\times p\times p}$) such that as the matrix $E\in\bbR^{p\times p}$ goes to $0$, we have $\loss_i((I_p + E)W_i) \simeq \loss_i(W_i) + \langle G_i, W_i\rangle + \frac12\langle E, \mathcal{H}_iE\rangle$. Standard manipulations yield:
\begin{equation}
    \label{eq:gradient}
    G_i = \frac1mf'(\tilde{\sbb})(\yb_i)^{\top} + \frac{1 - 1 /m}{\sigma^2}(\yb_i - \frac{m}{m-1}\tilde{\sbb}_{-i})(\yb_i)^{\top} - I_p 
\end{equation}
where $\yb_i= W_i\xb_i$.
\begin{align}
    \label{eq:hessian}
    (\mathcal{H}_i)_{abcd} &= \delta_{ad}\delta_{bc} + \delta_{ac}\left(\frac{1}{m^2}f''(\tilde{s}_a) + \frac{1 - 1/m}{\sigma^2}\right)y_{ib}y_{id} \\ &\text{for }a, b, c, d =1\dots p \nonumber
\end{align}

Newton's direction is then $-\left(\mathcal{H}_i\right)^{-1}G_i$. However, this Hessian is costly to compute (it has $\simeq p^3$ non-zero coefficients) and invert (it can be seen as a big $p ^2\times p^2$ matrix). Furthermore, to enforce that Newton's direction is a descent direction, the Hessian matrix should be regularized in order to eliminate its negative eigenvalues~\cite{nocedal2006numerical}, and $\mathcal{H}_i$ is not guaranteed to be positive definite.
%
These obstacles render the computation of Newton's direction impractical.
%
Luckily, if we assume that the signals in $\yb_i$ are independent, several coefficients cancel, and the Hessian simplifies to the approximation
%\bt{IIUC if the sources are independent this is not an approximation} \pa{It is an approximation because even if the sources are independent, you have $y_i = s^i$ only when $W_i$ is the true unmixing matrix, i.e. when the algo has converged}
\begin{align}
    \label{eq:hessian_approx}
    (H_i)_{abcd} &= \delta_{ad}\delta_{bc} + \delta_{ac}\delta_{bd}\Gamma^i_{ab} \\
    &\text{with  }(\Gamma_i)_{ab} = \left(\frac{1}{m^2}f''(\tilde{s}_a) + \frac{1 - 1/m}{\sigma^2}\right)\left(y_{ib}\right)^2 \nonumber
\end{align}
This approximation is sparse: it only has $p(2p -1)$ non-zero coefficients.
%
In order to better understand the structure of the approximation, we can compute the matrix $\left(H_iM\right)$ for $M\in \bbR^{p\times p}$. 
%
We find $\left(H_iM\right)_{ab} = (\Gamma_i)_{ab}M_{ab} + M_{ba}$: $H_iM_{ab}$ only depends on $M_{ab}$ and $M_{ba}$, indicating a simple block diagonal structure of $H_i$.
%
The tensor $H_i$ is therefore easily regularized and inverted:
$\left((H_i)^{-1}M\right)_{ab} = \frac{\Gamma^i_{ba}M_{ab} - M_{ba}}{\Gamma^i_{ab}\Gamma^i_{ba} - 1}$.
%
Finally, since this approximation is obtained by assuming that the $\yb_i$ are independent, the direction $-H_i^{-1}G_i$ is close to Newton's direction when the $\yb_i$ are close to independence, leading to fast convergence.
%
Algorithm~\ref{algo:mv_ica} alternates one step of the quasi-Newton method for each subject until convergence.
%
A backtracking line-search is used to ensure that each iteration leads to a decrease of $\loss_i$.
%
The algorithm is stopped when the maximum norm of the gradients over one pass on each subject is below some tolerance level, indicating that the algorithm is close to a stationary point.

\begin{algorithm}[H]
\label{algo:mv_ica}
\SetAlgoLined
\KwIn{Dataset $(\xb_i)_{i=1}^m$, initial unmixing matrices $W_i$, noise parameter $\sigma$, function $f$,  tolerance $\varepsilon$}
Set tol$=+\infty$, $\tilde{\sbb} = \frac1m\sum_{i=1}^pW_i\xb_i$\\
 \While{\text{tol}$>\varepsilon$}{
 tol = 0 \\
  \For{$i=1\dots m$}{
  Compute $\yb_i = W_i \xb_i$, $\tilde{\sbb}_{-i} = \tilde{\sbb} - \frac1m\yb_i$, gradient $G_i$ (eq.~\eqref{eq:gradient}) and Hessian $H_i$ (eq.~\eqref{eq:hessian_approx})\\
  Compute the search direction $D = -H_i^{-1}G_i$\\
  Find a step size $\rho$ such that $\loss_i((I_p + \rho D)W_i) < \loss_i(W_i)$ with line search\\
  Update $\tilde{\sbb} = \tilde{\sbb} + \frac{\rho}{m} DW_i \xb_i$, $W_i = (I_p + \rho D)W_i$, tol$=\max($tol$,\|G_i\|)$\\
  }
 }
 \Return{Estimated unmixing matrices $W_i$, estimated shared components $\tilde{\sbb}$}
 \caption{Alternate quasi-Newton method for MultiView ICA}
\end{algorithm}
%
%
%
%
\subsection{Robustness to model misspecification}
Algorithm~\ref{algo:mv_ica} has two hyperparameters: $\sigma$ and the function $f$.
%
The latter is usual for an ICA algorithm, but the former is not.
%
We study the impact of these parameters on the separation capacity of the algorithm, when these parameters do not correspond to those of the generative model~\eqref{eq:ica_model}.
%
\begin{proposition}
\label{prop:robust}
We consider the cost function $\loss$ in eq.~\eqref{eq:cost_function} with noise parameters $\sigma$ and function $f$.
%
Assume sub-linear growth on $f'$: $|f'(x)|\leq c|x|^{\alpha} + d$ for some $c, d > 0$ and $0<\alpha<1$.
%
Assume that $\xb_i$ is generated following model~\eqref{eq:ica_model}, with noise parameter $\sigma'$ and density of the component $d'$ which need not be related to $\sigma$ and $f$.
%
Then, there exists a diagonal matrix $\Lambda$ such that $(\Lambda (A^1)^{-1}, \dots, \Lambda (A^m)^{-1})$ is a stationary point of $\loss$, that is $G^1,\dots, G^m =0$ at this point.
\end{proposition}
The proof is available in appendix~\ref{ref:robust}.

%
The sub-linear growth of $f'$ is a customary hypothesis in ICA which implies that $d$ has heavier-tails than a Gaussian, and in appendix~\ref{ref:robust} we provide other conditions for the result to hold.
%
In this setting, the shared components estimated by the algorithm are $\tilde{\sbb} = \Lambda (\sbb + \frac1m \sum_{i=1}^m \nb_i)$, which is a scaled version of the best estimate of the shared components under the Gaussian noise hypothesis.

This proposition shows that, up to scale, the true unmixing matrices are a stationary point for Algorithm~\ref{algo:mv_ica}: if the algorithm starts at this point it will not move.
%
The question of stability is also interesting: if the algorithm is initialized ~\emph{close} to the true unmixing matrices, will it converge to the true unmixing matrix?
%
In the appendix~\ref{sec:stability}, we provide an analysis similar to~\cite{cardoso1998blind}, and derive sufficient numerical conditions for the unmixing matrices to be local minima of $\mathcal{L}$.
%

\section{Related Work}
In contrast to ConcatICA or CanICA (see section~\ref{sec:canicaandconcatica}),
MultiView ICA maximizes a likelihood, which brings statistical guarantees like
consistency or asymptotic efficiency. Furthermore MultiViewICA finds individual
and shared independent components in a single step. This differs from ConcatICA or
GroupICA that require additional steps when individual components are needed
such as back-projection~\cite{calhoun2001method} or dual-regression~\cite{beckmann2009group}.

The approach of~\cite{guo2008unified} (see section~\ref{sec:guo}) optimizes the more
general model $\xb_i = A_i\sbb + \nb_i$. 
The likelihood for this model involves an intractable high dimensional integral
that is cumbersome to evaluate, and is then optimized with an EM algorithm using
an inexact E-step.
Having the simpler model $\xb_i = A_i(\sbb + \nb_i)$ leads to a closed-form likelihood,
that can then be optimized by more efficient means.
Note that in MultiView ICA, the noise can be interpreted as individual variability rather than sensor noise. %It offers a way to capture more structured noise as is often the case in brain signals.


The SR-ICA approach of \cite{zhang2016searchlight} performs dimension reduction,
merging of individual data and independent component estimation. It is therefore similar to our method.
However, they propose to modify the FastICA algorithm~\cite{hyvarinen1999fast} in a rather heuristic way, without specifying an optimization problem, let alone maximizing a likelihood. In the experiments on fMRI data in appendix~\ref{appendix_reproduce}, we obtain better performance with MultiView ICA than the reported performance of SR-ICA.

One strength of our model is that we only assume that the mixing matrices are invertible and still enjoy identifiability whereas some other approaches impose additional constraints. For instance tensorial methods~\cite{beckmann2005tensorial} assume that the mixing matrices are the same up to diagonal scaling.
Other methods impose a common mixing matrix~\cite{cong2013validating, grin2010independent, calhoun2001fmri, Monti18UAI}. Like PCA, the Shared Response Model~\cite{chen2015reduced} (SRM) assumes orthogonality of the mixing matrices. While the model defines a simple likelihood and provides an efficient way to reduce dimension, the orthogonal constraint may not be plausible.

Deep Learning methods, such as convolutional auto-encoders (CAE), can also be used to find the subject specific unmixing~\cite{chen2016convolutional}. While these nonlinear extensions of the aforementioned methods are interesting, these models are hard to train and interpret. In the experiments on fMRI data in appendix~\ref{appendix_reproduce}, we obtain better accuracy with MultiView ICA than that of CAE reported in~\cite{chen2016convolutional}.

A different path to multi-subject ICA is to extract independent components with individual ICA in each subject and align them. We propose a simple baseline approach to do so called \emph{PermICA}.
Inspired by the heuristic of the hyperalignment method~\cite{haxby2011common} we choose a reference subject and first match the components of all other subjects to the components of the reference subject. The process is then repeated multiple times, using the average of previously aligned components as a reference. Finally, group components are given by the average of all aligned components. We use the Hungarian algorithm to align pairs of mixing matrices~\cite{tichavsky2004optimal}.
Alternative approaches involving clustering have also been developed~\cite{esposito2005independent,bigdely2013measure}.

Lastly, IVA based methods (see section~\ref{sec:IVA}) do not specifically model
the shared response which is the quantity of interest in this work.

\section{Conclusion}
In this chapter, we have proposed a novel unsupervised algorithm that reveals latent sources observed through different views. Using an independence assumption, 
we have demonstrated that the model is identifiable.
% 
In contrast to previous approaches, the proposed model leads to a closed-form likelihood, which we then optimize efficiently using a dedicated alternate quasi-Newton approach.
% 
Our approach enjoys the statistical guarantees of maximum-likelihood theory, while still being tractable.
% 

In the next chapter, we evaluate the performance of MultiView ICA on synthetic
data, on EEG and fMRI data and compare its performance to other GroupICA methods.
