In the previous chapter, we have decribed multiple unsupervised methods to analyze
multiview data such as fMRI data that are of very high dimension.
When working with high dimensional data, the shared response model~\cite{chen2015reduced} (SRM) is particularly
interesting as it provides a principled way to perform dimension reduction. Note that this
contrasts with ICA-like methods that do not incorporate dimension reduction in their model.

However SRM has initially been designed to work within regions of interest using
only few subjects. When using full brain data, computational costs are still
high. Lastly, memory requirements are difficult to meet since the full dataset needs to hold
in memory.

Fortunately, these high costs can be reduced. Intuitively, since the shared
response lives in a reduced space, a compressed representation of the input is
good enough to find a suitable estimate.
FastSRM implements this idea. It turns out that there exists an optimal
compression of the input from which we can obtain the same solution as
with the full data. We also present methods using approximations of the
optimal dimension reduction trading accuracy for efficiency.

\section{The FastSRM algorithm}
The goal of deterministic srm (respectively probabilistic srm) is to recover mixing matrices $A_i$ and shared
components $\sbb$ (respectively mixing matrices $A_i$, noise levels
$\sigma_i$ and components covariance $\Sigma_s$). 
Let us consider a set of view specific atlases $U_i \in \RR^{v, r}$ such that
$U_i^{\top}U_i = I_r$.
Data are reduced using $\zb_i = U_i^{\top} \xb_i$ and an SRM algorithm is applied
on data $\zb_i$ yielding parameters $A_i', \sbb$ (respectively $A_i',
\sigma_i', \Sigma_s$). Then we set $A_i = U_i A'_i$ to recover the mixing matrices.
As we will see in the next section, SRM algorithms may have to be slightly
modified in order to obtain guarantees on the recovered parameters.

The figure~\ref{fig:srm:conceptual} illustrates this process.
\begin{figure}
  \centering
  \includegraphics[scale=0.34]{figures/srm/conceptual_figure2.pdf}
  \caption{\textbf{FastSRM algorithm} In step 1, data are projected onto an atlas (top). In step 2 a SRM algorithm is applied on reduced data to compute the shared response.}
  \label{fig:srm:conceptual}
\end{figure}

From a computational stand point, the dimension reduction provides
a large reduction in memory usage. Indeed as the original data are seen only
once, it is no longer necessary to keep the full dataset in memory (we can load
data $\xb_i$ one after the other and similarly for the atlases $U_i$. Therefore the memory consumption is only in $\bigO(vn)$ which is lower than classical SRM algorithms by a factor of $m$. This yields a practical benefit: on most fMRI datasets, one no longer need a large cluster to run the shared response model but only a modern laptop.
Additionally, low memory consumption reduces the
risk of thrashing~\cite{denning1968thrashing}, a phenomenon that causes large
increase in computation time when the memory used is close to the total available
memory in the hardware.

After preprocessing, the reduced representation $\zb_i$ is used instead of the
original data $\xb_i$ yielding a time complexity of $\bigO(
\mathrm{T_{preprocessing}} + \mathrm{n_{iter}} mpnr)$.
Let us highlight that in practice, it often happens that an experiment is run
multiple times such as when cross validated results are needed. In these cases,
the pre-processing is performed only once and the apparent complexity becomes
$\bigO(\mathrm{n_{iter}} mpnr)$ which is faster than traditional SRM methods by
a factor of $\frac{v}{r}$.

In general, working with reduced data induces a discrepancy between the result
of the traditional shared response model  and our FastSRM algorithm.
In the next section, we show that there exists an optimal atlas in the sense
that the traditional shared response model and FastSRM yield the same
results whether it is applied on full data or on reduced data.

\section{Optimal atlases}
Let us consider $\xb_i = U_{\xb_i} \zb_i$ a PCA of $\xb_i$. As the number of
samples $n$ is lower than the number of features $U_{\xb_i} \in \RR^{v, n}$ and
$\zb_i \in \RR^{n}$.  We also have $U_{\xb_i}^{\top}U_{\xb_i} = I$.

As the next property shows, $U_{\xb_i}$ constitutes an optimal atlas when
fitting deterministic SRM.
\begin{prop}[Optimal dimension reduction via PCA for deterministic SRM]
  Let $(A_i)_i, \sbb$ be the solution obtained by deterministic SRM on data
  $\xb_i$ and $(A_i')_i, \sbb'$ the solution obtained by deterministic SRM on
  data $\zb_i = U_{\xb_i}^{\top} \xb_i$. Then $A_i = U_{\xb_i}A_i'$ and $\sbb = \sbb'$. 
\label{prop:optimaldetsrm}
\end{prop}
\begin{proof}
Updates of the mixing matrices $A_i$ in deterministic SRM
equation~\eqref{eq:detsrm:Aiupdate} can be written:
\begin{align}
  A_i &\leftarrow \Pcal(\xb_i \sbb^T)= U_{\xb_i}\Pcal(\zb_i \sbb^T)
\end{align}
\textbf{XXX what does $\Pcal()$ mean ?}


Therefore we can look for $A_i$ as $A_i = U_{\xb_i} \tilde{A_i}$. $\tilde{A_i}$ is
orthogonal. Indeed
\begin{align}
  &A_i^{\top} A_i = I_p \\
  & \implies \tilde{A_i}^{\top}U_{\xb_i}^{\top} U_{\xb_i} \tilde{A_i} = I_p \\
  & \implies \tilde{A_i}^{\top} \tilde{A_i} = I_p
\end{align}

Then, we use the fact that
\begin{align}
  \|\xb_i - A_i \sbb \|^2 = \| U_{\xb_i}\zb_i - U_{\xb_i}\tilde{A_i} \sbb\|^2 = \| \zb_i - \tilde{A_i} \sbb \|^2
  \label{eq:equality:xy}
\end{align}
Therefore, the solution of deterministic SRM on data $(\zb_i)_i$ and
$(\xb_i)_i$ are linked by the change of parameters $A_i = U_{\xb_i}A_i'$ and
$\sbb = \sbb'$ where $A_i' = \tilde{A_i}$. This concludes the proof.
\end{proof}
In the case of probabilistic SRM we can obtain very similar results however the
algorithm applied on reduced data need to be slightly modified.
We call probSRM($\psi$) the probabilistic SRM algorithm modified such that
updates
\begin{align}
\sigma_i^2 \leftarrow \frac1{v} (\| \xb_i - A_i \EE[\sbb|\xb]\|^2 + \| \diag(\VV[\sbb | \xb]) \|^2)
\end{align}
are replaced by updates
\begin{align}
  \sigma_i^2 \leftarrow \frac1{\psi} (\| \xb_i - A_i \EE[\sbb|\xb]\|^2 + \| \diag(\VV[\sbb | \xb]) \|^2)
\end{align}

We have the following result:
\begin{prop}[Optimal dimension reduction via PCA for probabilistic SRM]
  Let $(A_i)_i, \sigma_i, \Sigma_s$ be the solution obtained by probabilistic SRM on data
  $\xb_i$ and $(A_i')_i, \sigma_i', \Sigma_s'$ the solution obtained by ProbSRM($v$) on
  data $\zb_i = U_{\xb_i}^{\top} \xb_i$.Then $A_i = U_{\xb_i}A_i'$, $\sigma_i =
  \sigma_i'$ and $\Sigma_s = \Sigma_s'$. 
  \label{prop:optimalprobsrm}
\end{prop}
\begin{proof}
  A similar reasoning can be done with probabilistic SRM starting from
  equation~\eqref{eq:srm:Aiupdate} gives:
  \begin{align}
    A_i &\leftarrow U_{\xb_i}\Pcal(\zb_i \EE[\sbb| \xb_i]^T)
  \end{align}
  so we can look for $A_i$ as $A_i = U_{\xb_i} \tilde{A_i}$ and, as in the
  deterministic case, $\tilde{A_i}$ is orthogonal.
  Therefore equality~\eqref{eq:equality:xy} holds.
  
  Then we consider the negative log-likelihood of probabilistic srm:
  \begin{align}
    \loss &= \sum_i \frac12 v\log(\sigma_i^2) + \frac12 \log(|\Sigma_s|) + \int_{\sbb} \sum_i \frac1{2 \sigma_i^{2}}\|\xb_i - A_i \sbb \|^2 \\& \enspace + \frac12 \langle \sbb , \Sigma_s^{-1} \sbb \rangle  d\sbb \\
          &= \sum_i \frac12 v \log(\sigma_i^2) + \frac12 \log(|\Sigma_s|) + \int_{\sbb} \sum_i \frac1{2 \sigma_i^{2}}\|\zb_i - \tilde{A_i} \sbb \|^2 \\& \enspace+ \frac12 \langle \sbb , \Sigma_s^{-1} \sbb \rangle  d\sbb
  \end{align}
  where we use equality~\eqref{eq:equality:xy}.
  Optimizing the log-likelihood via expectation maximization yields the exact
  same updates as probabilistic srm on data $\zb_i$
  except that updates
  \begin{align}
    \sigma_i^2 \leftarrow \frac1{t} (\| \zb_i - \tilde{A_i} \EE[\sbb|\zb]\|^2 + \| \diag(\VV[\sbb | \zb]) \|^2)
  \end{align}
  are replaced by updates
  \begin{align}
    \sigma_i^2 \leftarrow \frac1{v} (\| \zb_i - \tilde{A_i} \EE[\sbb|\zb]\|^2 + \| \diag(\VV[\sbb | \zb]) \|^2)
  \end{align}

  The updates in both algorithms are linked  by $A_i = U_{\xb_i}A_i'$ where
  $A_i' = \tilde{A_i}$, $\sigma_i' =
  \sigma_i$ and $\Sigma_s'  = \Sigma_s$.

  This concludes the proof.
\end{proof}

The properties~\ref{prop:optimaldetsrm} and~\ref{prop:optimalprobsrm} show that
no information is lost by replacing $\xb_i \in \RR^v$ by its reduced representation $\zb \in \RR^n$.
A key property of the optimal atlas $U_{\xb_i}$ is that it is valid whether or
not the model for deterministic (respectively probabilistic) SRM holds.

A complexity analysis  shows that finding the optimal atlas becomes the limiting step of the pipeline. The time complexity becomes $\bigO(mvn^2)$ which is faster than classical SRM implementations only if $n < n_{iter} p$. However, in popular
linear algebra pipeline such as scipy~\cite{2020SciPy-NMeth}, the code
performing singular value decomposition is highly optimized therefore the
constant in front of the $\bigO$ is low. Lastly, if more memory is available, finding the optimal atlas can be done in parallel for each view, trading memory for time.

In the next section, we investigate approaches to reduce the preprocessing time.

\section{Efficient approximation of optimal atlases}
As we have seen in the previous section, if $U_i = U_{\xb_i}$ where $\xb_i =
U_{\xb_i} \zb_i$ is a PCA of $\xb_i$, then $U_i$ is an optimal atlas.
A natural follow-up is to look for atlases that are close to optimal in the
sense that the inertia $\| (I - U_i U_i^{\top}) \xb_i \|$ is low but are fast to
compute and such that projecting data on the atlas is a cheap operation.

Deterministic atlases are interesting as they allow for fast data projection
(computing $U_i \xb_i$ only costs $\bigO(v)$). Furthermore, they are thought to
be well-suited to fMRI data as they reduce noise via smoothing (this argument is
detailed in~\cite{hoyos2018recursive}).
%
A first approach is to use existing deterministic atlases for fMRI data such as 
~\cite{schaefer2017local} or~\cite{bellec2010multi}. As theses atlases
attempt to reduce the dimension of fMRI signals without losing too much signal,
there is hope for the inertia to be small.
RENA, the clustering approach
of~\cite{hoyos2018recursive}, is also appealing as it is optimized to give low
inertia while the cost of finding the deterministic atlas is low (computing $U_i$ only costs
$\bigO(vn)$). As the number $r$ of regions increases, the inertia decreases
and the approximation becomes better and better. An other possibility is to
perform a PCA with a number of components $r < n$.

The approaches previously described attempt to approximate $U_i$ by a
deterministic atlas. Another possibility is to approximate directly $\zb_i$ via
random projection.
Let us call $X_i \in \RR^{v,
  n}$ our data matrix. Following~\cite{mahoney2016lecture} (section 14.4), we
can approximate $X_i^{\top} X_i$ by $\tilde{C} = \sum_{j \in \mathcal{J}} \frac1{p_j r} \xb_{ij}^{\top} \xb_{ij}$ where $\xb_{ij}$ is the $j$-th line of
  $X_i$ and $\mathcal{J}$ is a set of $r$ indexes such that $\mathcal{J}$ contains
  index $j$ with probability $p_j$. The most simple choice is $p_j=\frac1{v}$
  which we use in our implementations. Other choices are possible, we refer the reader to section 15
  of~\cite{mahoney2016lecture} for more details.  Computing an approximation of $X_i^{\top} X_i$ is interesting
  because the SVD of $X_i^{\top} X_i$ allows us to recover the reduced data
  $Z_i$ up to an irrelevant sign indeterminacy. 
  If spatial maps $A_i$ are needed, they can be recovered from the data using the formula
  $A_i \leftarrow \mathcal{P}(\xb_i \sbb^{\top})$ (in the deterministic SRM case) or
  $A_i \leftarrow \mathcal{P}(\xb_i \EE[\sbb^{\top}|\xb])$ (in the probabilistic SRM case).

  Up to now, we have assumed that the covariance of components is diagonal in
  probabilistic SRM. In the next section we justify this assumption.

\section{Identifiablity of the shared response model}
We first show why deterministic SRM and probabilistic SRM without any
restriction on the covariance of the components can
only recover unmixing matrices up to an arbitrary rotation.

Let us consider data $\xb_i$ generated from deterministic SRM with
mixing matrices $A_i$, shared response $\sbb$ and noise covariance $\sigma$.
Then the parameters $A'_i = A_i \theta$ and $\sbb' = \theta^{\top} \sbb$ also
generate $\xb_i$ according to the deterministic SRM model.
\textbf{XXX I guess that $\theta$ is assumed orthogonal ?}

Similarly, in the probabilistic SRM case, if data $\xb_i$ are generated from
parameters $A_i$, $\Sigma_s$, $\sigma_i^2$ (where $\Sigma_s$ can be any positive
matrix) then $A_i\theta$, $\theta^{\top}
\Sigma_s \theta$, $\sigma_i^2$ also generate $\xb_i$.

Then, we show that if the covariance of the components are diagonal, under some
assumptions probabilistic SRM is identifiable:
\begin{prop}[Identifiability of probabilistic SRM]
  \label{prop:fastsrm:identifiability}
  Let $\xb_i$ be generated from the probabilistic SRM model with parameters 
  $A_i$, $\Sigma_s$, $\sigma_i^2$ (where $\Sigma_s$ is diagonal positive with
  distinct values on the diagonal) and assume there exists another set of parameters $A_i'$, $\Sigma_s'$,
  ${\sigma_i'}^2$ (where $\Sigma_s'$ is diagonal positive with
  distinct values on the diagonal) that also generate $\xb_i$.
Then if $m\geq 3$, $A_i' = A_i P^{\top}$, $\Sigma_s'= P\Sigma_sP^{\top}$ and
  ${\sigma_i^2}' = {\sigma_i}^2$ where $P$ is a sign and permutation matrix
  independent (the same for all subjects).
\end{prop}
\begin{proof}
  Let us consider $\EE[\xb_i \xb_j^{\top}]$ for $i \neq j$.
  We have:
  \begin{equation}
  A_i \Sigma_s A_j^{\top} = A_i' \Sigma_s' {A_j'}^{\top}
  \label{eq:fastsrm:svd}
  \end{equation}
  up to re-ordering, equation~\eqref{eq:fastsrm:svd} gives two singular value
  decompositions of the same matrix.
  Therefore, by unicity of the singular value decomposition we have:
  $\Sigma'_s = P_i \Sigma_s P_j^{\top}$ and $A_i' = A_i P_i^{\top}$ and $A_j =
  A_j P_j^{\top}$ where $P_i$ and $P_j$ are sign and permutation matrices.
  Since there are more than three subjects, there exists subject $z$ such that
  $\Sigma'_s = P_i \Sigma_s P_z^{\top}$ and therefore
  $P_i \Sigma_s P_z^{\top} =  P_i \Sigma_s P_j^{\top}$ and therefore $P_j =
  P_z$. Therefore, all sign and permutations are the same and we call $P$ their
  common value.
  Then we consider
  $\EE[\xb_i \xb_j^{\top}] = A_i \Sigma A_i^{\top} + \sigma I_v = A_i' \Sigma_s'
  {A_i'}^{\top} + {\sigma^2}' I_v = A_i \Sigma_s
  {A_i}^{\top} + {\sigma^2}' I_v$
  so we get ${\sigma^2}' = {\sigma^2}$
\end{proof}

Proposition~\eqref{prop:fastsrm:identifiability} justifies that
$\Sigma_s$ should be assumed to be diagonal in probabilistic SRM.
In addition, working with a diagonal covariance matrix allows to speed up
slightly the computations (although this does not change the time complexity of
the algorithm).

In the next chapter, we investigate how FastSRM works in practice compared to
available implementations.
