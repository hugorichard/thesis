In this chapter, we present the probabilistic modeling and
optimization background needed for the present thesis. The material presented in
section~\ref{sec:probgenmodel} is inspired
from the section 4 of~\cite{levy2012probabilistic}, the section 1.1
of~\cite{ablin2019exploration}, the chapter 17 of~\cite{ferguson2017course}
and~\cite{stein1956inadmissibility}. The material presented in
section~\ref{sec:optimintro} comes from the chapter 9 of~\cite{boyd2004convex}
and from~\cite{neal1998view}.

\section{Probabilistic generative models}
\label{sec:probgenmodel}
In a probabilistic generative modeling framework, learning from experiments means identifying the underlying process that generates the data we observe.

More formally, consider $\Xcal = \xb^{(1)}, \dots, \xb^{(n)} \in \RR^k$, $n$ random
vectors with joint density $\nu^*_{\Xcal}$ and consider the available data $X \in \RR^{k,
  n}$ as an observation of $\Xcal$. We also say that data $X$ are \emph{generated from} $\nu^*_{\Xcal}$.
The broad goal of probabilistic generative modeling is to recover $\nu^*_{\Xcal}$ from $X$. When samples $\xb^{(1)},
\dots, \xb^{(n)}$ are independent and identically distributed,
$\nu^*_{\Xcal} = \otimes_{i=1}^n \nu^*_{\xb}$ so that
$X$ can be seen as $n$ observations of the random variable $\xb$ with density
$\nu^*_{\xb}$. In the remaining of the thesis, we use the same notation $\xb^{(i)}$ for the random variable associated to the $i$-th sample and its observation (the $i$-th column
of $X$). In addition, we use $\nu^*$ to denote both $\nu^*_{\xb}$ and $\nu^*_{\Xcal}$ 
depending on whether samples are independent and identically
distributed or not.

In practice, we assume a model for the true density $\nu^*$ meaning that we
assume that $\nu^*$ belongs to a family of densities $\mathcal{F}$.
When it is indeed true that $\nu^* \in \Fcal$, we say that \emph{the model holds}.

Often, we assume $\Fcal$ to be a set of parametric densities so
that any density in $\Fcal$ can be written as $\nu_{\thetab}$ where
$\thetab \in \Theta$ is a set of parameters and $\Theta$ is the set of all
possible $\thetab$. When the model holds, there exists an optimal
set of parameters $\thetab_*$ such that $\nu^* = \nu_{\thetab_*}$. The goal is then
to find $\thetab_*$.


Before we even try to find $\thetab_*$, it is instructive to wonder whether the
problem is well defined. Ideally we would like our model to be such that if
$\nu_{\thetab_1} = \nu_{\thetab_2}$ then $\thetab_1=\thetab_2$.
When this is the case, we say that the model is \emph{identifiable}.

In all the proofs in this section, we assume that the integration and
differentiation operators can always be exchanged and that all quantities
introduced are well defined. The set of parameters $\thetab$ is viewed as a
vector.

\subsection{Desirable properties of estimators}
\label{sec:desirable}
An \emph{estimator} $\hat{\thetab}_n$ of $\thetab_*$ is a function of the
observations $\Xcal = \xb^{(1)}, \dots, \xb^{(n)}$ that aims at finding $\thetab_*$.
An estimator is a random variable, as it depends on $\Xcal$. Therefore, it can almost never be perfectly accurate and that
is why we need a criterion to measure its inaccuracy.

A common choice is the mean squared error criterion given by:
\begin{align}
  \EE[\|\hat{\thetab}_n - \thetab_*\|^2] &= \EE[\|\hat{\thetab}_n - \EE[\hat{\thetab}_n] + \EE[\hat{\thetab}_n] - \thetab_*\|^2] \\
                                   &= \EE[\|\hat{\thetab}_n - \EE[\hat{\thetab}_n]\|^2] + \|\EE[\hat{\thetab}_n] - \thetab_*\|^2  \\
                                         &= \tr(\VV(\hat{\thetab}_n)) + \|\EE[\hat{\thetab}_n] - \thetab_*\|^2 \label{eq:mse:biasvariance}
\end{align}

where the right hand side in equation~\ref{eq:mse:biasvariance} gives the
\emph{bias-variance decomposition}. The left term is the trace of the \emph{covariance} $\VV(\hat{\thetab}_n) = \Cov(\hat{\thetab}_n -
\EE[\hat{\thetab}_n], \hat{\thetab}_n -
\EE[\hat{\thetab}_n])$ where $\Cov(\ab, \bb) = \EE[(\ab - \EE[\ab]) (\bb -
\EE[\bb])^{\top}]$ and the right term is the squared norm of the \emph{bias} given by $\EE[\hat{\thetab}_n] - \thetab_*$ .

The norm of the bias can be minimized exactly and such estimators that achieve $\EE[\hat{\thetab}_n] - \thetab_* = \bzero$ are called \emph{unbiased}.

In Example~\ref{ex:biased:var}, we study the bias of the sample mean and sample variance. The sample mean is shown to be unbiased. In
contrast, the sample variance is biased: we show how to
correct the estimator of the variance so that it becomes unbiased.

\begin{example}[Biased and unbiased estimate of the parameters of a 1D Gaussian]
  \label{ex:biased:var}
  Consider $n$ observations $x^{(1)},
  \dots x^{(n)}$ of $x$ with mean $\mu_*$ and variance $\sigma_*^2$.
  Consider the sample mean: $\hat{\mu}_e = \frac1{n}
  \sum_{i=1}^n x^{(i)}$. This estimate is unbiased as $\EE[\hat{\mu}_e] = \frac1{n}
  \sum_{i=1}^n \EE[x^{(i)}] = \mu_*$.
  

  Consider the sample variance:
  $\hat{\sigma}_e^2 = \frac1{n} \sum_{i=1}^n (x^{(i)} - \sum_{z=1}^n \frac1{n} x^{(z)})^2$. 

  We have
  \begin{align}
    \EE[\hat{\sigma}_e^2] &= \frac1{n} \sum_{i=1}^n \EE[ (x^{(i)} - \sum_{z=1}^n \frac1{n} x^{(z)})^2] \\
                        &= \frac1{n} \sum_{i=1}^n \EE [(x^{(i)})^2 - 2 x^{(i)} (\sum_{z=1}^n \frac1{n} x^{(z)} ) +(\sum_{z=1}^n \frac1{n} x^{(z)} )^2] \\
                        &= \frac1{n} \sum_{i=1}^n \EE [(x^{(i)})^2 - 2 x^{(i)} (\sum_{z=1}^n \frac1{n} x^{(z)} ) + \frac1{n^2}\sum_{y=1, z=1}^n  x^{(y)} x^{(z)} ] \\
                        &= \frac1{n} \sum_{i=1}^n (\sigma_*^2 + \mu_*^2 - \frac{2}{n}\sigma_*^2 -2\mu_*^2 +  \frac1{n^2} (\sum_{z=1}^n \sigma_*^2 + n^2 \mu_*^2)) \\
                        &= \sigma_*^2 - \frac1{n} \sigma_*^2 \\
                        &= \frac{n-1}{n}\sigma_*^2
  \end{align}
  so the sample covariance is biased.
  In contrast, if we consider the estimator
  \begin{align}
    \hat{\sigma}_u^2 = \frac{n}{n-1}\hat{\sigma_e}^2
  \end{align}
  we can see that it is unbiased.
\end{example}

However the variance of an estimator cannot be arbitrarily low as shown by
Proposition~\ref{prop:cramerrao} (Cramer-Rao bound).
\begin{proposition}[Cramer-Rao bound]
  \label{prop:cramerrao}
  Let $\hat{\thetab}_n$ be an estimator of $\thetab_*$.
  Then, $\VV(\hat{\thetab}_n) \succeq
  \partialfrac{\thetab}{\EE[\hat{\thetab}_n]}I_n(\thetab_*)^{-1} (\partialfrac{\thetab}{\EE[\hat{\thetab}_n]})^{\top}$
\end{proposition}
where $A \succeq B$ is understood as $A-B$ is positive semi-definite.
We introduced $I_n(\thetab)$, the Fisher information matrix given by
\begin{align}
  I_n(\thetab) = \EE[\partialfrac{\thetab}{\log(\nu_{\thetab}(\Xcal))} \partialfrac{\thetab}{\log(\nu_{\thetab}(\Xcal))}^{\top}]
\end{align}
Lastly, the quantity $l(\xb, \thetab) = \log(\nu_{\thetab}(\xb))$ is called
\emph{log-likelihood of $\xb$} and its derivative $\psib(\xb, \thetab) =
\partialfrac{\thetab}{\log(\nu_{\thetab}(\xb))}$ is called the \emph{score
  function of $\xb$}. 
\begin{proof}[Proof of Cramer-Rao bound]
  First let us show that when the optimal parameter is used, the expected score
  function cancels:
  \begin{align}
    \EE_{\Xcal}[\psib(\thetab_*)] 
    &= \EE_{\Xcal}[\partialfrac{\thetab}{\log(\nu_{\thetab_*}(\Xcal))}] \\
    &= \EE_{\Xcal}[\frac1{\nu_{\thetab_*}(\Xcal)}\partialfrac{\thetab}{\nu_{\thetab_*}(\Xcal)}] \\
    &= \int_{\Xcal} \frac1{\nu_{\thetab_*}(\Xcal)}\partialfrac{\thetab}{\nu_{\thetab_*}(\Xcal)} \nu_{\thetab_*}(\Xcal) d\Xcal \\
    &= \int_{\Xcal} \partialfrac{\thetab}{\nu_{\thetab_*}(\Xcal)} d\Xcal \\
    &= \partialfrac{\thetab}{\int_{\Xcal} \nu_{\thetab_*}(\Xcal)d\Xcal}  \\
    &= \partialfrac{\thetab}{1} \\
    &= \bzero
  \end{align}
  so that $\VV[\psib(\thetab_*)] = I_n(\thetab_*)$.

  We also have that:
  \begin{align}
    \Cov(\hat{\thetab}_n, \psib(\thetab_*)) &= \EE[ (\hat{\thetab}_n - \EE[\hat{\thetab}_n]) (\psib(\thetab_*) -  \EE[\psib(\thetab_*)])^{\top}]\\
                                           &= \EE[ \hat{\thetab}_n \psib(\thetab_*)^{\top}]\\
                                           &= \int_{\Xcal} \hat{\thetab}_n \partialfrac{\thetab}{\nu_{\thetab_*}(\Xcal)}^{\top} d\Xcal \\
                                           &= \partialfrac{\thetab}{\int_{\Xcal} \hat{\thetab}_n \nu_{\thetab_*}(\Xcal) d\Xcal} \\
                                           &= \partialfrac{\thetab}{\EE[\hat{\thetab}_n]}
  \end{align}
  and similarly $\Cov(\psib(\thetab_*), \hat{\thetab}_n) = (\partialfrac{\thetab}{\EE[\hat{\thetab}_n]})^{\top}$.

  Then we apply the following Cauchy Schwartz inequality for random vectors:
  \begin{equation}
  \forall \xb, \yb \enspace \Var(\yb) \succeq  \Cov(\yb, \xb) \Var(\xb)^{-1} \Cov(\xb, \yb)
  \end{equation}
(see a proof in \cite{tripathi1999matrix})
and therefore get the expected result:
  \begin{align}
    \Var(\hat{\thetab}_n) \succeq  \partialfrac{\thetab}{\EE[\hat{\thetab}_n]} I_n(\thetab_*)^{-1} (\partialfrac{\thetab}{\EE[\hat{\thetab}_n]})^{\top} 
  \end{align}
\end{proof}

When an estimator is both unbiased and reaches the Cramer-Rao bound we call the
estimator \emph{efficient}.
If we assume that samples are independent and identically distributed we
have: $\psi(\Xcal, \theta) = \sum_{i=1}^n \psi(\xb^{(i)}, \theta)$ and therefore
\begin{align}
  I_n(\thetab_*) &= \EE[\psi(\Xcal, \theta_*) \psi(\Xcal, \theta_*)^{\top}] \\
                 &= \sum_{i=1}^n \EE[\psi(\xb^{(i)}, \theta_*) \psi(\xb^{(i)}, \theta_*)^{\top}] \\
               &= n I(\thetab_*)
\end{align}
where
\begin{align}
  I(\thetab_*) = \EE_{\xb \sim \nu_{\theta_*}} [\psi(\xb, \theta_*) \psi(\xb, \theta_*)^{\top}]
\end{align}

When samples are independent and identically distributed and $\hat{\thetab}_n$ is unbiased the Cramer-Rao
bound is given by:
\begin{equation}
  \Var(\hat{\thetab}_n) \succeq \frac1{n} I(\theta_*)^{-1}
\end{equation}

The Example~\ref{ex:efficient_mean} hows that the sample mean
is efficient.
\begin{example}[Sample mean is efficient]
  \label{ex:efficient_mean}
  Consider $n$ observations $x^{(1)},
  \dots x^{(n)}$ of $x$ generated from $\nu_{\mu_*}(x) = \Ncal(x; \mu_*, \sigma_*^2)$
  and consider the sample mean $\hat{\mu}_e = \frac1{n} \sum_{i=1}^n x^{(i)}$.
  The variance is given by:
  \begin{align}
    \VV[\hat{\mu}_e] &= \VV[\frac1{n} \sum_{i=1}^n x^{(i)}] \\
    &= \frac{\sigma_*^2}{n}
  \end{align}

  The Fisher information matrix is given by
  \begin{align}
    I &= \EE[(\partialfrac{\mu}{\log(\nu_{\mu_*}(x))})^2] \\
      &= \EE[(\partialfrac{\mu}{-\frac1{2 \sigma_*^2} (x - \mu_*)^2 - \frac12\log(2\pi \sigma_*^2)})^2] \\
      &= \EE[(-\frac1{\sigma_*^2} (\mu_* - x))^2] \\
      &= \frac1{\sigma_*^4}\EE [(\mu_* - x)^2] \\
      &= \frac1{\sigma_*^2}
  \end{align}
  and therefore we have
  \begin{equation}
  \VV[\hat{\mu}_e]  = \frac1{n} I^{-1}
  \end{equation}
  so the sample mean is efficient.
\end{example}
In practice, efficient estimators are extremely rare. In the next section, we
introduce the maximum likelihood estimator which is not always unbiased nor
efficient in the finite sample case, but satisfies these properties asymptotically.

\subsection{Maximum likelihood}
\label{sec:mlintro}
The maximum likelihood estimates the parameters $\hat{\thetab}$ such that the
density $\nu_{\hat{\thetab}}$ at $\xb^{(1)}, \dots, \xb^{(n)}$ is the highest among all possible
values for $\thetab \in \Theta$:
\begin{align}
  \hat{\thetab}_n = \argmax_{\thetab \in \Theta} \nu_{\thetab}(\xb^{(1)}, \dots, \xb^{(n)})
\end{align}

The quantity $\nu_{\thetab}(\xb^{(1)}, \dots, \xb^{(n)})$ is called the
\emph{likelihood} and we call $\hat{\thetab}_n$ the \emph{maximum likelihood estimator}.

It is often assumed that samples are independent and identically distributed so that
the joint density can be written:
$\nu_{\thetab}(\xb^{(1)}, \dots, \xb^{(n)}) = \prod_{i=1}^n \nu_{\thetab}(\xb^{(i)})$

Let us define the \emph{empirical expected log-likelihood}:
\begin{align}
  l_n(\thetab) &= \frac1n
  \sum_{i=1}^n l(\xb^{(i)}, \thetab) \\
               &= \EE_n l(\xb, \thetab)
\end{align}
where $\EE_n$ is the empirical expectation operator defined by
\begin{align}
\EE_n f(\xb) = \frac1{n} \sum_{i=1}^n f(\xb^{(i)})
\end{align}


The next lines show that finding the maximum likelihood is done by optimizing
the empirical expected log-likelihood.


\begin{align}
  \hat{\thetab}_n &= \argmax_{\thetab \in \Theta} \nu_{\thetab}(\xb^{(1)}, \dots, \xb^{(n)}) \\
               &= \argmax_{\thetab \in \Theta} \prod_{i=1}^n \nu_{\thetab}(\xb^{(i)}) \\ 
               &= \argmax_{\thetab \in \Theta} \sum_{i=1}^n \log(\nu_{\thetab}(\xb^{(i)}))  \\
               &= \argmax_{\thetab \in \Theta} \frac1n \sum_{i=1}^n l(\xb^{(i)}, \thetab) \\ 
                  &= \argmax_{\thetab \in \Theta} l_n(\thetab) \\ 
\end{align}

By the law of large numbers, as the number of samples increases, the empirical
expected likelihood converges almost surely to the \emph{expected
  log-likelihood}: $l(\thetab) = \EE_{\xb}[l(\xb, \thetab)]$.

Example~\ref{ex:mse:variance} shows that, under a Gaussian model, the maximum likelihood estimator of the
mean and the variance is the sample mean and sample variance respectively.
\begin{example}
  \label{ex:mse:variance}
  Consider $n$ observations $x^{(1)}, \dots, x^{(n)}$ of an unknown random variable $x$
  and consider the model $x \sim \Ncal(\mu, \sigma^2)$.
  The empirical expected log-likelihood is:
  \begin{align}
    l_n(\mu, \sigma^2) = -\frac1{n} \sum_{i=1}^n \frac1{2 \sigma^2}(x^{(i)} - \mu)^2 - \frac12 \log(2\pi \sigma^2)
  \end{align}
  First order conditions
  yield:
  \begin{align}
    &\sum_{i=1}^n (\mu - x^{(i)}) = 0 \iff \mu = \frac1{n} \sum_{i=1}^n x^{(i)} \\
    &\frac1{n} \sum_{i=1}^n \frac1{(\sigma^2)^2}(x^{(i)} - \mu)^2 - \frac1{\sigma^2} = 0 \iff \sigma^2 = \frac1{n} \sum_{i=1}^n(x^{(i)} - \mu)^2
  \end{align}
  So the maximum likelihood estimators of the mean and the variance are the
  sample mean and sample variance respectively.
\end{example}

The maximum likelihood estimator $\hat{\thetab}_n$ is a random variable as it depends on
$\Xcal = \xb^{(1)}, \dots, \xb^{(n)}$. As the number of samples $n$ increases,
we expect $\hat{\thetab}_n$ to get closer to the optimal set of parameters
$\thetab_*$.
In Proposition~\ref{prop:mse_consistent}, it is shown that the
maximum likelihood estimator is \emph{consistent} meaning $\hat{\thetab}_n
\xrightarrow[n \rightarrow \infty]{P}  \thetab_*$ where $\hat{\thetab}_n
\xrightarrow[n \rightarrow \infty]{P}  \thetab_*$ denotes the convergence in
probability defined by $\forall \epsilon > 0, \enspace p(\| \hat{\thetab}_n - \thetab_* \| < \epsilon) \xrightarrow[n \rightarrow \infty]{} 1$. 

\begin{proposition}[Consistency of the maximum likelihood estimator]
  \label{prop:mse_consistent}
  Assume $\Theta$ is compact, 
  Assume $l_n$ converge uniformly in probability to $l$
  and assume $l$ is continuous.
  Lastly, assume that the model is identifiable.
  Then, the maximum likelihood estimator  $\hat{\thetab}_n$ is consistent
  meaning that $\hat{\thetab}_n \xrightarrow[n \rightarrow \infty]{P}  \thetab_*$.
\end{proposition}
where uniform convergence in probability means $sup_{\thetab} \|l_n(\thetab) -
l(\thetab)\| \xrightarrow[n \rightarrow \infty]{P} 0$.
\begin{proof}
  We first show that $l$ is maximum at $\thetab_*$:
  \begin{align}
    l(\thetab_*) - l(\thetab) &= \EE_{\xb \sim \nu_{\thetab_*}}[\log \nu_{\thetab_*}(\xb) - \log(\nu_{\thetab}(\xb)] \\
                            &= D_{KL}(\nu_{\thetab_*}, \nu_{\thetab}) \\ \label{kl-likelihood}
                            &\geq 0
  \end{align}
  where $D_{KL}$ is the Kullback-Leibler divergence that is always positive.
  The maximum is unique. This comes from the identifiability of the model that
  implies
  \begin{align}
    l(\thetab_*) = l(\thetab) \implies \thetab_* = \thetab
  \end{align}

  Let $\eps > 0$ and define $V_{\eps} = \{\thetab, \|\thetab - \thetab_*\| <
  \eps\}$ an open neighborhood of $\thetab_*$.
  Because $\Theta$ is compact and $V_{\eps}$ open, $\Theta \cap
  V_{\eps}^C$ is compact and since $l$ is continuous, $max_{\thetab \in \Theta
    \cap V_{\eps}^C} l(\thetab)$ is reached for a value $\thetab_0 \in \Theta \cap V_{\eps}^C$.
  
  Let us define $\delta = l(\thetab_*) - l(\thetab_0) > 0$ and consider the events
  \begin{align}
    A_n &= sup_{\thetab \in \Theta \cap V_{\eps}^C} \|l_n(\thetab) - l(\thetab)\| < \frac{\delta}{2} \\
    B_n &= sup_{\thetab \in V_{\eps}} \|l_n(\thetab) - l(\thetab)\| < \frac{\delta}{2}
  \end{align}

  We have
  \begin{align}
        A_n &\implies \forall \thetab \in \Theta \cap V_{\eps}^C,  \enspace l_n(\thetab) - l(\thetab) < \frac{\delta}{2} \\
        &\implies \forall \thetab \in \Theta \cap V_{\eps}^C,  \enspace l_n(\thetab) < \frac{\delta}{2} + l(\thetab_0) \\
        &\implies \forall \thetab \in \Theta \cap V_{\eps}^C,  \enspace l_n(\thetab) < -\frac{\delta}{2} + l(\thetab_*)
  \end{align}
  and 
  \begin{align}
    B_n &\implies \forall \thetab \in V_{\eps}, \enspace l_n(\thetab) > -\frac{\delta}{2} + l(\thetab) \\
        &\implies l_n(\thetab_*) > -\frac{\delta}{2} + l(\thetab_*) \\
  \end{align}

  Then consider $\hat{\thetab}_n = \argmax_{\theta \in
    \Theta}(l_n(\theta))$ and assume $\hat{\thetab}_n \in V_{\eps}^C \cap \Theta$.
  We have
  \begin{align}
    A_n \cap B_n \implies l_n(\hat{\thetab}_n) < -\frac{\delta}{2} + l(\thetab_*) < l_n(\thetab_*)
  \end{align}
  So if $\hat{\thetab}_n \in V_{\eps}^C \cap \Theta$, $A_n \cap B_n$ contradicts the fact that $\hat{\thetab}_n = \argmax_{\theta \in
    \Theta}(l_n(\theta))$.

  Therefore $A_n \cap B_n \implies \hat{\thetab}_n \in V_{\eps} $.
  From the uniform convergence in probability of $l_n$ to $l$ we have
  $p(A_n \cap B_n) \xrightarrow[n \rightarrow \infty]{} 1$ and therefore
  $p(\hat{\thetab}_n \in V_{\eps}) \xrightarrow[n \rightarrow \infty]{} 1$ which
  means $\hat{\thetab}_n \xrightarrow[n \rightarrow \infty]{P} \thetab_*$.
\end{proof}

As we have ween in Example~\ref{ex:biased:var}, the sample variance is biased.
However, in the large sample limit the bias disappears. Is this a consequence of
consistency ? Intuitively, consistency  seems to be a stronger condition than asymptotic
unbiasedness since consistency implies convergence of the random variable
$\hat{\thetab}_n$ whereas asymptotic unbiasedness only implies convergence of the mean $\EE[\hat{\thetab}_n]$.
Proposition~\ref{prop:consistency_unbiasedness} shows that this intuition is correct as long as the variance is bounded.

\begin{proposition}[Consistency implies asymptotic unbiasedness]
  \label{prop:consistency_unbiasedness}
  Assume $\thetab_n\xrightarrow[n \rightarrow \infty]{P} \thetab_*$ and assume
  $\exists M \in \RR, \enspace \EE[\|\thetab_n - \thetab_*\|^2] < M$. \\
  Then $\EE[\|\thetab_n - \thetab_*\|] \xrightarrow[n \rightarrow \infty]{} 0$
\end{proposition}
\begin{proof}
  Set $\eps > 0$, we have
  \begin{align}
    \thetab_n\xrightarrow[n \rightarrow \infty]{P} \thetab_* &\implies p(\|\thetab_n - \thetab_*\| > \frac{\eps}{2}) \xrightarrow[n \rightarrow
                                                             \infty]{}0\\
                                                           &\implies \exists N \in \NN, \forall n > N, \enspace
                                                             p(\|\thetab_n - \thetab_*\| > \frac{\eps}{2}) < \frac{\eps}{M}
  \end{align}

  So $\forall n > N$ we get:
  \begin{align}
    \EE[\|\thetab_n - \thetab_*\|] &= \EE[\|\thetab_n - \thetab_*\| \oneb_{\|\thetab_n - \thetab_*\| \leq \frac{\eps}{2}}] + \EE[\|\thetab_n - \thetab_*\| \oneb_{\|\thetab_n - \thetab_*\|  > \frac{\eps}{2}}] \\
                                 &\leq  \frac{\eps}{2} + \EE[\|\thetab_n - \thetab_*\|^2]p(\|\thetab_n - \thetab_*\|  > \frac{\eps}{2}) \label{consistencyunbiasedcs} \\
                                   &< \eps
  \end{align}
  Where equation~\eqref{consistencyunbiasedcs} follows from Cauchy-Schwarz, for
  the scalar product $\ab, \bb \rightarrow \EE[\ab \bb^{\top}]$.
\end{proof}

While Proposition~\ref{prop:mse_consistent} states that $\hat{\thetab}_n
\xrightarrow[n \rightarrow \infty]{P} \thetab_*$, 
Proposition~\ref{prop:asymp:norm} goes further in the analysis. It states that
$\sqrt{n}\|\hat{\thetab}_n - \thetab_*\|$ approaches a Gaussian density as $n$
gets large.
This property is called \emph{asymptotic normality}.

\begin{proposition}[Asymptotic normality of maximum likelihood estimators]
  \label{prop:asymp:norm}
  We assume the same hypothesis as in Proposition~\ref{prop:mse_consistent}.
  We further assume that $\thetab_*$ is in the interior of $\Theta$.
  Lastly, denoting $J_n(\thetab_n) =
  \partialfrac{\thetab^2}{^2l_n}(\thetab_n)$ and $J(\thetab_*) =
  \partialfrac{\thetab^2}{^2l}(\thetab_*)$ we assume that \\
  $\thetab_n \xrightarrow[n \rightarrow
  \infty]{P} \thetab_* \implies J_n(\thetab_n) \xrightarrow[n \rightarrow
  \infty]{P} J(\thetab_*)$\\
  Then, \\
  $\sqrt{n} \| \hat{\thetab}_n - \thetab_* \| \xrightarrow[n \rightarrow
  \infty]{D}  \Ncal(0,
  I(\thetab_*)^{-1})$
\end{proposition}
where the convergence in distribution $ x^{(n)} \xrightarrow[n \rightarrow \infty]{D} x$ means  $\forall t, \enspace F_{x^{(n)}}(t)
\xrightarrow[n \rightarrow \infty]{} F_x(t)$ where $F_x$ is the distribution
function of $x$.
\begin{proof}

  First order conditions give $\partialfrac{\thetab}{l_n(\hat{\thetab}_n)} = \bzero$.

  From the mean value theorem there exists $\thetab_0$ such that
  \begin{align}
    &\partialfrac{\thetab}{l_n(\hat{\thetab}_n)} - \partialfrac{\thetab}{l_n(\thetab_*)} = J_n(\thetab_0)(\hat{\thetab}_n - \thetab_*) \\
    &\iff -\frac1{\sqrt{n}} \sum_{i=1}^n  \psi_{\thetab_*}(\xb^{(i)}) = J_n(\thetab_0)\sqrt{n}(\hat{\thetab}_n - \thetab_*)
  \end{align}

  We know from the proof of the Cramer-Rao bound
  (Proposition~\ref{prop:cramerrao}) that $\EE_{\xb} [\psi_{\thetab_*}(\xb)]=
  0$.  We can use the central limit theorem and write:
  \begin{align}
  \frac1{\sqrt{n}} \sum_{i=1}^n  \psi_{\thetab_*}(\xb^{(i)}) \xrightarrow[]{D}
  \Ncal(0, I(\thetab_*))
  \end{align}

  Then, we have:
  \begin{align}
  J(\thetab_*) &= \EE_{\xb}[\partialfrac{\thetab}{\psi_{\thetab_*}(\xb)}] \\
              &=  \EE_{\xb} [\partialfrac{\thetab}{ \frac1{\nu_{\thetab_*}(\xb)} \partialfrac{\thetab}{ \nu_{\thetab_*}(\xb)}} ] \\
              &= \EE_{\xb}[\frac1{\nu_{\thetab_*}(\xb)} \partialfrac{\thetab^2}{^2 \nu_{\thetab_*}(\xb)}-\frac{\partialfrac{\thetab}{ \nu_{\thetab_*}(\xb)}}{\nu_{\thetab_*}(\xb)}   \frac{\partialfrac{\thetab}{\nu_{\thetab_*}(\xb)}}{\nu_{\thetab_*}(\xb)}^{\top}  ] \\
              &= - I(\thetab_*) \\
  \end{align}

  Then since $\hat{\thetab}_n \xrightarrow[n \rightarrow \infty]{P} \thetab_*$, $\thetab_0
  \xrightarrow[n \rightarrow \infty]{P} \thetab_*$ and from our assumption about
  $J_n$ we get
  $J_n(\thetab_0) \xrightarrow[n \rightarrow \infty]{P} J(\thetab_*) =
  -I(\thetab_*)$.

  From Slutsky's theorem we have
  \begin{align}
    &I(\thetab_*) \sqrt{n}(\hat{\thetab}_n - \thetab_*) \xrightarrow[]{D}\Ncal(0, I(\thetab_*)) \\
    &\iff \sqrt{n}(\hat{\thetab}_n - \thetab_*) \xrightarrow[]{D}\Ncal(0, I(\thetab_*)^{-1})
  \end{align}
\end{proof}

Looking at the variance of $\hat{\thetab}_n - \thetab_*$, we see that it behaves like $\frac1{n}I(\theta_*)^{-1}$ which is
the quantity in the Cramer-Rao bound for unbiased estimators (see Proposition~\ref{prop:cramerrao}).
Because of this, the maximum likelihood estimator is called \emph{asymptotically
  efficient}.

\subsection{Some shortcomings}
It may look like an efficient and unbiased estimator should always yield the
best possible mean squared error.
However, this is not the case. A striking example is given by the Stein paradox~\cite{stein1956inadmissibility}.
As can be seen in Example~\ref{ex:stein}, there exists an estimator of the mean
that achieves a strictly better mean squared error than the sample mean.
However, we have shown that the sample mean (which is also the maximum
likelihood estimator) is unbiased and efficient. Stein's estimate shows that
biased estimators can sometimes achieve lower mean squared error than unbiased ones.
\begin{example}[Stein's estimate of the mean]
  \label{ex:stein}
  Let us consider a single observation $\xb$ generated from a multivariate Gaussian
  of dimension $v \geq 3$: $\Ncal(\mub, I)$ where $I$ is the identity matrix and
  $\mub \in \RR^v$ is the unknown mean that we want to estimate.
  As in the one-dimensional case, the sample mean $e(\xb) =\xb$ is unbiased and efficient.
  The mean squared error is given by:
  \begin{align}
    \EE[(e(\xb) -  \mub )^2] = \tr(V(e(\xb))) = v
  \end{align}
  Now consider the estimator $s(\xb) = \xb - (v-2) \frac{\xb}{\|\xb\|^2}$.
  The mean squared error is given by:
  \begin{align}
    &\EE[(s(\xb) - \mub)^2] \\ &= \EE[(\xb - (v-2) \frac{\xb}{\|\xb\|^2} - \mub)^2] \\
                        &= \EE[(\xb - \mub)^2] -2 (v-2)\EE[\frac{(\xb - \mub)^{\top}\xb}{\|\xb\|^2}] + (v-2)^2 \EE[\frac1{\| \xb \|^2}] \\
                            &= \EE[(\xb - \mub)^2] -2 (v-2) \sum_{j=1}^v \EE[\frac{(x_j - \mu_j)x_j}{\|\xb\|^2}] + (v-2)^2 \EE[\frac1{\| \xb \|^2}]
  \end{align}
  where $x_j$ is the $j$-th coordinate of $\xb$

  By integration by part 
  \begin{align}
    \EE[\frac{(x_j - \mu_j)x_j}{\|\xb\|^2}] &= \EE[\partialfrac{x_j}{\frac{x_j}{\|\xb\|^2}}]  \\
    &= \EE[\frac1{\|\xb\|^2} - \frac{2x_j^2}{\|\xb\|^4}]
  \end{align}

  so that 
  \begin{align}
    &\EE[(s(\xb) - \mub)^2] \\
    &= \EE[(\xb - \mub)^2] -2 (v-2) \sum_{j=1}^v \EE[\frac1{\|\xb\|^2} - \frac{2x_j^2}{\|\xb\|^4}] + (v-2)^2 \EE[\frac1{\| \xb \|^2}] \\
    &= \EE[(\xb - \mub)^2] -2 (v-2) \EE[\frac{v}{\|\xb\|^2} - \frac{2 \sum_{j=1}^v x_j^2}{\|\xb\|^4}] + (v-2)^2 \EE[\frac1{\| \xb \|^2}] \\
    &= \EE[(\xb - \mub)^2] -2 (v-2)^2 \EE[\frac1{\|\xb\|^2}] + (v-2)^2 \EE[\frac1{\| \xb \|^2}] \\
    &= v - (v-2)^2 \EE[\frac1{\|\xb\|^2}]
 \end{align}
 Therefore $s(\xb)$ always yields lower expected mean squared error than
 $e(\xb)$.
\end{example}

Example~\ref{ex:stein} shows that shrinking the sample mean towards the
origin decreases the mean squared error of the estimate in high dimensions. A similar technique can also be used for estimating the covariance of variables. For instance, Ledoit
and Wolf showed in~\cite{ledoit2004well} that shrinking the sample covariance
towards identity decreases lower expected mean squared error in high dimensions.

\section{Optimization}
\label{sec:optimintro}
The maximum likelihood estimator gives a natural way to obtain estimators that
are consistent and asymptotically efficient.
However, it requires finding the maximum of the empirical expected
log-likelihood.
This can rarely be done using a closed form formula and one almost always have
to resort to iterative methods.


\subsection{Some iterative optimization algorithms}
Let us consider a function $f: \RR^v \rightarrow \RR$ that we
want to minimize. $f(\thetab)$, where $\thetab \in \RR^v$, can be seen as the
negative expected likelihood.


Optimization algorithms that only use first order derivatives to make a step are
called \emph{first order methods}. We will begin by presenting the famous
\emph{gradient descent}. Then, we move on to second order methods with \emph{Newton} and
\emph{quasi-Newton methods}. This section closely follows the chapter~9 of~\cite{boyd2004convex}.

\subsubsection{Gradient descent}
\label{sec:gd}
We assume that $f$ is differentiable everywhere.
From a point $\thetab_0$, assuming a small \emph{step size} $\alpha$ and a direction
$\db$ such that $\|\db\| = 1$, a Taylor decomposition at first order gives:
\begin{align}
f(\thetab_0 + \alpha \db) = f(\thetab_0) + \dotp{ \partialfrac{\thetab}{f(\thetab_0)} }{ \alpha \db } + o(\alpha) 
\end{align}
The best direction is the one that minimizes $f(\thetab_0 + \alpha \db)$. If we
neglect the terms in $o(\alpha)$, it is given by
\begin{align}
  \argmin_{\db, \|\db\| = 1} f(\thetab_0) + \dotp{ \partialfrac{\thetab}{f(\thetab_0)} }{ \alpha \db } = \frac{-\partialfrac{\thetab}{f(\thetab_0)}}{\|\partialfrac{\thetab}{f(\thetab_0)}\|}
\end{align}

Therefore, gradient descent updates are given by:
\begin{align}
  \thetab_{k+1} = \thetab_k -\alpha \partialfrac{\thetab}{f(\thetab_k)} \label{eq:update:gd}
\end{align}
where $\alpha$ is a small quantity.

Under certain conditions that we specify in Proposition~\ref{prop:conv:gd},
gradient descent converges to the minimum.

\begin{proposition}[Convergence of gradient descent]
  \label{prop:conv:gd}
  Assume that $f$ is twice differentiable and $\mu$-strongly convex:
  \begin{equation}
    \partialfrac{\thetab^2}{^2f(\thetab)} \succeq \mu I
  \end{equation}
  In addition, assume that $f$ is $\ell$-smooth:
  \begin{equation}
    \partialfrac{\thetab^2}{^2f(\thetab)} \preceq \ell I
  \end{equation}
  where $I$ is the identity matrix.
  From $\thetab_0 \in \RR^v$ and given $\alpha \in \RR$ such that $\frac1{\ell} \geq \alpha > 0$, the iterates
  $\thetab_{k+1} = \thetab_k -\alpha \partialfrac{\thetab}{f(\thetab_k)}$ converge to the
  minimum $\thetab_*$ according to
  \begin{equation}
    \| \thetab^{k+1} - \thetab_* \|^2 \leq (1 - \alpha \mu)^{k + 1} \| \thetab_0 - \thetab_* \|^2
  \end{equation}
\end{proposition}
\begin{proof}
  We have 
  \begin{align}
    \| \thetab^{k+1} - \thetab_* \|^2 &= \| \thetab_k -\alpha \partialfrac{\thetab}{f(\thetab_k)} - \thetab_* \|^2 \\
                            &= \| \thetab_k - \thetab_* \|^2  - 2 \dotp{ \thetab_k - \thetab_* }{ \alpha \partialfrac{\thetab}{f(\thetab_k)} } + \|\alpha \partialfrac{\thetab}{f(\thetab_k)} \|^2  \label{eq:gd:itdiff}
 \end{align}

 From $\mu$-strong convexity and Lagrange inequality we get:
 \begin{align}
   &f(\thetab_*) \geq f(\thetab_k) + \dotp{ \partialfrac{\thetab_k}{f(\thetab_k)} }{ \thetab_* - \thetab_k } + \frac{\mu}{2} \|\thetab_* - \thetab_k \|^2
 \end{align}
 so that 
 \begin{align}
   -2 \alpha \dotp{ \partialfrac{\thetab_k}{f(\thetab_k)} }{ \thetab_k -  \thetab_*} \leq  -2 \alpha(f(\thetab_k) - f(\thetab_*)) - \mu \alpha \|\thetab_* - \thetab_k \|^2 \label{eq:mu:conv:ineq}
   \end{align}

   and using the fact that $f$ is $\ell$-smooth we have: 
   \begin{align}
     &\forall \thetab, \yb, \enspace f(\yb) \leq f(\thetab) + \partialfrac{\thetab}{f(\thetab)} (\yb - \thetab) + \frac{\ell}{2} \| \yb - \thetab \|^2 \\
     &\implies \forall \thetab, \enspace f(\thetab - \frac1{\ell} \partialfrac{\thetab}{f(\thetab)}) - f(\thetab) \leq -\frac1{2\ell} \| \partialfrac{\thetab}{f(\thetab)} \|^2 \\
     &\implies \forall \thetab, \enspace f(\thetab_*) - f(\thetab) \leq -\frac1{2\ell} \| \partialfrac{\thetab}{f(\thetab)} \|^2  \\
     &\implies \|\alpha \partialfrac{\thetab}{f(\thetab_k)} \|^2 \leq 2\alpha^2 l(f(\thetab_k) - f(\thetab_*)) \label{eq:lsmooth:ineq}
   \end{align}

   So from~\eqref{eq:gd:itdiff} using the inequalities~\eqref{eq:mu:conv:ineq}
   and~\eqref{eq:lsmooth:ineq} we get
   \begin{align}
     \| \thetab^{k+1} - \thetab_* \|^2 \leq (1 - \alpha \mu)\| \thetab_k - \thetab_* \|^2 -2 \alpha (1 - \alpha \ell) (f(\thetab_k) - f(\thetab_*))
   \end{align} 
   and since $0 < \alpha < \frac1{\ell}$ we get that 
   \begin{align}
     \| \thetab^{k+1} - \thetab_* \|^2 \leq (1 - \alpha \mu)\| \thetab_k - \thetab_* \|^2
   \end{align}
   which by induction yields the desired result.
\end{proof}
The type of convergence we get in Proposition~\ref{prop:conv:gd} is called a
\emph{linear convergence} (because $\| \thetab^{k+1} - \thetab_*
\|$ is bounded by a linear function of  $\|\thetab^{k} - \thetab_* \|$) .

\subsubsection{Newton method and quasi-Newton methods}
\label{sec:qn}
We assume that $f$ is twice differentiable everywhere.
As in gradient descent, we depart from a point $\thetab_0$, assume a small
\emph{step size} $\alpha$ and consider a direction
$\db$ such that $\|\db\| = 1$.
A Taylor decomposition at the second order gives:
\begin{align}
  f(\thetab_0 + \alpha \db) = f(\thetab_0) + \dotp{ \partialfrac{\thetab}{f(\thetab_0)} }{ \alpha \db } + \dotp{ \alpha \db }{ \partialfrac{\thetab^2}{^2 f(\thetab_0)} \alpha \db } + o(\alpha^2) 
\end{align}
If we neglect the terms in $o(\alpha^2)$, the best direction is given by
\begin{align}
  &\argmin_{\db, \|\db\| = 1} f(\thetab_0) + \dotp{ \partialfrac{\thetab}{f(\thetab_0)} }{ \alpha \db } + \dotp{ \alpha \db }{ \partialfrac{\thetab^2}{^2 f(\thetab_0)} \alpha \db } \\
  &= - \frac{(\partialfrac{\thetab^2}{^2 f(\thetab_0)})^{-1}\partialfrac{\thetab}{f(\thetab_0)}}{\| (\partialfrac{\thetab^2}{^2 f(\thetab_0)})^{-1} \partialfrac{\thetab}{f(\thetab_0)}\|}
\end{align}


Therefore, Newton updates are given by:
\begin{align}
  \thetab_{k+1} = \thetab_k -\alpha (\partialfrac{\thetab^2}{^2 f(\thetab_k)})^{-1} \partialfrac{\thetab}{f(\thetab_k)} \label{eq:update:newton}
\end{align}
where $\alpha$ is a small quantity.

Unfortunately, Newton's method is not guaranteed to converge. This contrasts with the gradient descent method that is guaranteed to converge when the step-size is small enough.
In order to converge, Newton's method needs that each step yields a sufficient decrease of the loss. This is done by a line search.

The \emph{exact line search} sets $\alpha = \argmin_{t \in [0, 1]} f(\thetab_k - t
(\partialfrac{\thetab^2}{^2 f(\thetab_0)})^{-1}
\partialfrac{\thetab}{f(\thetab_k)})$ while
the \emph{backtracking line search} is an iterative procedure where one starts
with $\alpha=1$ and repeatedly halves $\alpha$ until $f(\thetab_{k+1}) < f(\thetab_k)$.

As shown by Proposition~\ref{eq:conv:newton}, Newton's method is guaranteed to converge when an
exact line search is used.

\begin{proposition}[Convergence of Newton's method]
  \label{eq:conv:newton}
  Assume that $f$ is twice differentiable, $\mu$-strongly convex and $\ell$-smooth:
  \begin{equation}
    \mu I \preceq \partialfrac{\thetab^2}{^2 f(\thetab)} \preceq \ell I
  \end{equation}
  where $I$ is the identity matrix.
  Assume that the Hessian is Lipschitz with constant $h$:
  \begin{equation}
  \| \partialfrac{\thetab^2}{^2 f(\thetab)} - \partialfrac{\thetab^2}{^2 f(\thetab')} \| \leq h \| \thetab
  - \thetab' \|
 \end{equation}

  From $\thetab_0 \in \RR^v$  the iterates
  $\thetab_{k+1} = \thetab_k -\alpha (\partialfrac{\thetab^2}{^2 f(\thetab_0)})^{-1}
  \partialfrac{\thetab}{f(\thetab_k)}$ where $\alpha$ is chosen using an exact line
  search converge to the
  minimum $\thetab_*$.
  Depending on the norm of the gradient two phases exist:
  \begin{itemize}
  \item The damped phase where $\| \partialfrac{\thetab}{f(\thetab)} \| \geq \frac{\mu^2}{h}$.
    In this phase we have $f(\thetab_{k+1}) - f(\thetab_k) \leq \frac{\mu^5}{2h^2 \ell^2}$.
    Therefore the number of iterations in this phase $i$ cannot be larger than
    $\frac{2 h^2 \ell^2}{\mu^5}(f(\thetab_0) - f(\thetab_*))$.
  \item The pure Newton phase where $\| \partialfrac{\thetab}{f(\thetab)} \| < \frac{\mu^2}{h}$.
      In this phase $\|f(\thetab_{k}) - f(\thetab_*)\| \leq \frac{2 \mu^3}{h^2}
      (\frac12)^{2^{k -i +1}}$
 \end{itemize}
 Once the Newton phase is reached, it continues until convergence of the algorithm.
\end{proposition}
 \begin{proof}
   In the damped phase we have  $\| \partialfrac{\thetab}{f(\thetab)} \| \geq
   \frac{\mu^2}{h}$.
   From Lagrange inequality and $\ell$-smoothness, denoting $\db_k = -(\partialfrac{\thetab^2}{^2 f(\thetab)})^{-1} \partialfrac{\thetab}{f(\thetab)}$ we have:
   \begin{align}
     f(\thetab_k + t \db_k) & \leq f(\thetab) + t \dotp{ \partialfrac{\thetab}{f(\thetab)} }{ \db_k }  +  \frac{t^2 \ell}{2} \|\db_k\|^2 \\
                        & \leq f(\thetab) + t \dotp{ \partialfrac{\thetab}{f(\thetab)} }{ \db_k }  -  \frac{t^2 \ell}{2 \mu} \dotp{ \partialfrac{\thetab}{f(\thetab)} }{ \db_k } \\
   \end{align}
   We then have 
   \begin{align}
     &  f(\thetab_k + \alpha \db_k)  \leq f(\thetab) + t \dotp{ \partialfrac{\thetab}{f(\thetab)} }{ \db_k }  -  \frac{t^2 \ell}{2 \mu} \dotp{ \partialfrac{\thetab}{f(\thetab)} }{ \db_k } \\
   \end{align}
   setting $t = \frac{\mu}{\ell}$ gives
   \begin{align}
       f(\thetab_k +\alpha \db_k)  &\leq f(\thetab) + \frac{\mu}{2\ell} \dotp{ \partialfrac{\thetab}{f(\thetab)} }{ \db_k }  \\
     &\leq f(\thetab) - \frac{\mu}{2\ell^2} \|\partialfrac{\thetab}{f(\thetab)}\|^2  \\
     &\leq f(\thetab) - \frac{\mu^5}{2h \ell^2} \\
   \end{align}
   which gives the desired result.

   In the pure Newton phase, we assume $\alpha=1$ (the exact line search can
   only be better than this).
   We have
   \begin{align}
     \partialfrac{\thetab}{f(\thetab_{k+1})} &= \partialfrac{\thetab}{f(\thetab_{k} + \db_k)} -\partialfrac{\thetab}{f(\thetab_k)} -  \partialfrac{\thetab^2}{^2 f(\thetab_k)} \db_k \\
                                     &= \int_{t \in [0, 1]} \partialfrac{\thetab^2}{^2 f(\thetab_{k} + t \db_k)} \db_k dt  -  \partialfrac{\thetab^2}{^2 f(\thetab_k)} \db_k \\
                                     &=\int_{t \in [0, 1]} (\partialfrac{\thetab^2}{^2 f(\thetab_{k} + t \db_k)} -  \partialfrac{\thetab^2}{^2 f(\thetab_k)}) \db_k dt  \\
   \end{align}
   Then 
   \begin{align}
     \|\partialfrac{\thetab}{f(\thetab_{k+1})}\| &\leq \int_{t \in [0, 1]} \|(\partialfrac{\thetab^2}{^2 f(\thetab_{k} + t \db_k)} -  \partialfrac{\thetab^2}{^2 f(\thetab_k)}) \db_k\| dt  \\
     &\leq \int_{t \in [0, 1]} \|(\partialfrac{\thetab^2}{^2 f(\thetab_{k} + t \db_k)} -  \partialfrac{\thetab^2}{^2 f(\thetab_k)})\| \| \db_k\| dt  \\
                                         &\leq  \int_{t \in [0, 1]} t h\|\db_k\|^2 dt  \\
                                         &\leq  \frac{h}{2 \mu^2}\|\partialfrac{\thetab}{f(\thetab_{k})}\|^2 \\
   \end{align}
   Since $\|\partialfrac{\thetab}{f(\thetab_{k})}\| \leq \frac{\mu^2}{h}$ the sequence
   of gradients converges to zero. The convergence is quadratic.
   At iteration $k$ we therefore have:
   \begin{align}
   \frac{h}{2 \mu^2} \|\partialfrac{\thetab}{f(\thetab_{k})}\|
     &\leq  (\frac12)^{2^{k-i}} \\
   \end{align}
   Using strong convexity we conclude that
   \begin{align}
     f(\thetab_k) - f(\thetab_*) \leq \frac1{2 \mu} \|\partialfrac{\thetab}{f(\thetab_{k})}\|^2 \leq  2 \frac{\mu^3}{h^2} (\frac12)^{2^{k-i}}
   \end{align}
 \end{proof}

In the pure Newton phase, the convergence is quadratic. This is a very strong
advantage of Newton methods.
A second advantage of Newton method is its \emph{equivariance} as demonstrated
in Proposition~\ref{prop:equivariance:qn}. Equivariance means that for any
invertible matrix $A$, working with
parameters $A \thetab$ instead of parameters $\thetab$ has no impact on the result given
by the algorithm.
\begin{proposition}[Newton algorithms are equivariant]
  \label{prop:equivariance:qn}
  Let us consider $\thetab_k$ the current estimate of the minimum of $f$ obtained by
  the Newton method after $k$ iterations starting from $\thetab_0$.
  For any invertible matrix $A$, consider $\yb_k$ the current estimate of the
  minimum of $g(\yb) = f(A\yb)$
  obtained by the Newton method after $k$ iterations starting from $\yb_0 = A^{-1}\thetab_0$.
  Then, $\yb_k = A^{-1} \thetab_k$.
\end{proposition}
\begin{proof}
  The relation holds for $k=0$ and assuming $\yb_k = A^{-1} \thetab_k$ we have:
\begin{align}
  \yb_{k+1} &= \yb_k -\alpha (\partialfrac{\yb^2}{^2 g(\yb_k)})^{-1} \partialfrac{\yb}{g(\yb_k)} \\
  &= \yb_k -\alpha A^{-2}(\partialfrac{\thetab^2}{^2 f(\thetab_k)})^{-1} A \partialfrac{\thetab}{f(\thetab_k)} \\
  &= A^{-1} (\thetab_k -\alpha (\partialfrac{\thetab^2}{^2 f(\thetab_k)})^{-1} \partialfrac{\thetab}{f( \thetab_k)}) \\
            &= A^{-1} \thetab_{k+1} \\
\end{align}
\end{proof}
In contrast, gradient descent is not equivariant.
However, despite its attractive properties, Newton's method is rarely used in
practice because it is often intractable. Indeed inverting
the Hessian $\partialfrac{\thetab^2}{^2 f}$ is difficult as it is an operator in
dimension $\RR^{v \times v \times v \times
  v}$.
In some cases, it is possible to construct an approximation of the inverse of
the Hessian in a reasonable time. This can be done iteratively by building a
sequence of matrices $B_k$ that approaches the inverse of the Hessian as $k$ grows.
Methods that use an approximation of the inverse instead of the true inverse are
called \emph{quasi-Newton} methods~\cite{gill1972quasi}. 

\subsection{EM and generalized EM}
In the maximum likelihood framework, our goal is to find the parameters
$\theta_*$ that maximize the expected likelihood $l(\theta)$ of
observations $\Xcal = \xb^{(1)}, \dots, \xb^{(n)}$ assuming a model
$\nu_{\theta}(\Xcal)$.
In the previous section, we have presented some iterative
optimization methods that can be used to directly maximize the log-likelihood.
In this section, we present an alternative technique suited to
\emph{latent variable models}.
Latent variable models include, in addition to observed data $\xb$, unobserved data $\zb$ called \emph{latent variables}. The observed data $\xb$ is assumed to depend on the
unobserved data $\zb$.
With some abuse of notation we denote $\nu_{\theta}(\zb)$
the likelihood of $\zb$, $\nu_{\theta}(\xb | \zb)$ the density of $\xb | \zb$
evaluated at $\xb, \zb$ and $\nu_{\theta}(\xb, \zb) =
\nu_{\theta}(\zb) \nu_{\theta}(\xb | \zb)$ the joint likelihood of $\xb, \zb$ called the
\emph{complete likelihood}.  

We can relate the likelihood and the completed likelihood by:
\begin{align}
  \nu_{\theta}(\xb) = \int_{\zb} \nu_{\theta}(\xb, \zb) d\zb
\end{align}

The EM algorithm maximizes an expression
that depends on the complete log-likelihood rather than the log-likelihood so it
is useful when the former is much simpler to maximize than the later.
This section follows the work in~\cite{neal1998view}.

In order to make this more concrete, let us focus on the Gaussian mixture model
in Example~\ref{ex:em:gaussmixt}.
\begin{example}[Gaussian mixture models]
  \label{ex:em:gaussmixt}
  Let us consider a latent variable $z$
  sampled from a Bernoulli distribution with parameter $\phi$, 
  and $x$ is given by
  $x|z=0 \sim \mathcal{N}(\mu^0, 1)$ and 
  and $x | z=1 \sim \mathcal{N}(\mu^1, 1)$.

  We call $x^{(1)}, \dots, x^{(n)}$ the observed samples and $z^{(1)}, \dots,
  z^{(n)}$ the corresponding unobserved latent variables. 

  The log-likelihood of $x^{(i)}$ is given by:
  \begin{align}
    &l(x^{(i)}, (\mu^0, \mu^1, \phi)) \\ &= \log(\phi \mathcal{N}(x^{(i)}, \mu^1, 1) + (1 - \phi) \mathcal{N}(x^{(i)}, \mu^0, 1)) \\
                                                   &= \log\Big( \phi\frac{\exp(-\frac{(x^{(i)} - \mu^1)^2}{2})}{\sqrt{2 \pi}} + (1 - \phi)\frac{\exp(-\frac{(x^{(i)} - \mu^0)^2}{2})}{\sqrt{2 \pi}} \Big)
  \end{align}

  Whereas the completed log-likelihood of $(x^{(i)}, z^{(i)})$ is given by
  \begin{align}
    &l((x^{(i)}, z^{(i)}), (\mu^0, \mu^1, \phi)) \\
    &= \log((\phi \mathcal{N}(x^{(i)}, \mu^1, 1))^{z^{(i)}} ((1 - \phi) \mathcal{N}(x^{(i)}, \mu^0, 1))^{1- z^{(i)}}) \\
    &= z^{(i)} ( -\frac{(x^{(i)} - \mu^1)^2}{2} + \log(\phi)) \\ &\enspace \enspace + (1 - z^{(i)}) ( -\frac{(x^{(i)} - \mu^0)^2}{2} + \log(1 -\phi)) + c
  \end{align}
  where $c$ is a constant that does not depend on $\mu^1$ or $\mu_2$.

  It is easy to see that the expected completed log-likelihood will be
  much easier to maximize than the expected log-likelihood.
\end{example}
This example shows that unsurprisingly, it would be easier to find the parameters of a
Gaussian mixture model if we knew the component from which are generated each sample.

Instead of optimizing the expected log-likelihood $l(\theta)$ directly, the EM algorithm optimizes the following function:
\begin{align}
  F(q, \theta) &= \EE_q[\log( \nu_{\theta}(\xb, \zb))] + H_q \label{eq:em:fdef}
\end{align}
where $q$ is a density and 
\begin{align}
  &\EE_q[\log( \nu_{\theta}(\xb, \zb))] = \int_{\zb} \log( \nu_{\theta}(\xb, \zb)) q(\zb) d\zb  \\
  &H_q = \int_{\zb} -\log(q(\zb)) q(\zb) d\zb 
\end{align}
$H_q$ is called the entropy of $q$ and is always positive.
Let us introduce $\nu_{\theta}(\zb | \xb)$ the density of $\zb | \xb$, we have:
$\nu_{\theta}(\xb, \zb) = \nu_{\theta}(\zb | \xb) \nu_{\theta}(\xb)$ and
therefore
\begin{align}
  F(q, \theta) &= \EE_q[\log( \nu_{\theta}(\zb| \xb)) + \log( \nu_{\theta}(\xb))] + \EE_q[-\log(q)] \\
  &= \log(\nu_{\theta}(\xb)) - \EE_q[ \log(q) - \log( \nu_{\theta}(\zb| \xb))] \\
               &= l(\xb, \theta) - D_{KL}(q, \nu_{\theta}(\zb| \xb))
\end{align}
From the fact that $D_{KL}$ is positive and $D_{KL}(a, b) = 0 \iff a = b$ we
have that
\begin{align}
  &F(q, \theta) \leq l(\xb, \theta) \\
  &F(\nu_{\theta}(\zb| \xb), \theta) = l(\xb, \theta) \label{eq:maxem:q}
\end{align}
and therefore
\begin{align}
  \max_{q, \theta} F(q, \theta) = \max_{\theta} (\max_q F(q, \theta)) = \max_{\theta} l(\xb, \theta)
\end{align}

Any EM algorithm maximizes $l$ by maximizing $F$.
The most common practice is to maximize alternatively $F$ with respect to
$q$ and $\theta$.
At iteration $k$, let us call $\theta_k$ the current estimate of $\theta$.
According to equation~\eqref{eq:maxem:q}, the maximum with
respect to $q$ is given by $q=\nu_{\theta_k}(\zb| \xb)$. Then, we have to
maximize equation~\eqref{eq:em:fdef} with respect to $\theta$.
As the entropy $H$ does not depend on $\theta$ the function to maximize is given
by:
\begin{align}
  Q(\theta ) =  \EE_{\zb \sim \nu_{\theta_k }(\zb | \xb)}[\log( \nu_{\theta }(\xb, \zb))]
\end{align}
Computing $Q$ is called the \emph{E-step}. Then we maximize $Q$ and set
\begin{align}
  \theta_{k+1} = \argmax_{\theta} Q(\theta) \label{eq:exact:mstep}
\end{align}
This step is called the \emph{M-step}.

In Example~\ref{ex:em::gaussmixt:2} we us use the EM algorithm to optimize the Gaussian mixture model introduced in Example~\ref{ex:em:gaussmixt}.
\begin{example}[Optimizing the Gaussian mixture via EM]
  \label{ex:em::gaussmixt:2}
  Let us take the same data as in Example~\ref{ex:em:gaussmixt}. We call
  $\mu^0_k, \mu^1_k, \phi_k$ the estimates of $\mu^0, \mu^1, \phi$ at
  iteration $k$ and define:
  \begin{align}
    \gamma^{(i)}_k = p(z^{(i)} = 1 | x^{(i)}) &= \frac{\phi_k \Ncal(x^{(i)}; \mu^0_k, 1)}{ \phi_k \Ncal(x^{(i)}; \mu^0_k, 1) + (1 - \phi_k) \Ncal(x^{(i)}; \mu^1_k, 1)} 
  \end{align}
  The E-step is given by:
  \begin{align}
    Q(\mu^0, \mu^1, \phi) &= \sum_{i=1}^n \Big( \gamma^{(i)}_k \log(\nu_{\mu^0, \mu^1, \phi}(x^{(i)}, 1)) \\ &\enspace \enspace + (1 - \gamma^{(i)}_k) \log(\nu_{\mu^0, \mu^1, \phi}(x^{(i)}, 0)) \Big) \\
              &= \sum_{i=1}^n \Big( \gamma^{(i)}_k( -\frac{(x^{(i)} - \mu^1)^2}{2} + \log(\phi)) \\ &\enspace \enspace + (1 - \gamma^{(i)}_k) ( -\frac{(x^{(i)} - \mu^0)^2}{2} + \log(1 -\phi)) + c \Big)
 \end{align}
 The M-step is given by:
 \begin{align}
   &\partialfrac{\phi}{Q}(\phi^{k + 1}) = 0 \iff \phi^{k + 1} = \frac{\sum_{i=1}^n \gamma^{(i)}_k}{n} \\
   &\partialfrac{\phi}{Q}(\mu^{k+1}_0) = 0 \iff \mu^{k + 1}_0 = \frac{\sum_{i=1}^n (1 - \gamma^{(i)}_k) x^{(i)}}{n} \\
   &\partialfrac{\phi}{Q}(\mu^{k+1}_1) = 0 \iff \mu^{k + 1}_1 = \frac{\sum_{i=1}^n \gamma^{(i)}_k x^{(i)}}{n}
 \end{align}
\end{example}


The maximization of $F$  via the \emph{E-step} and \emph{M-step}, like in
Example~\ref{ex:em::gaussmixt:2}, is the
historical version of the EM algorithm. However, other optimization techniques
referred to as \emph{EM variants}
are possible. For instance, one could replace the maximization in
equation~\eqref{eq:exact:mstep} by just one step of an iterative optimization
algorithm giving a set of parameters $\theta_{k+1}$ that verifies:
\begin{align}
  Q(\theta_{k+1}) > Q(\theta_k) 
\end{align}
When the M-step is performed in such an approximated way, the EM algorithm
is renamed \emph{generalized EM}~\cite{dempster1977maximum}.
Similarly, one could replace the exact \emph{E-step} by just improving the
current density estimate with respect to the value function $q \rightarrow F(q,
\theta)$~\cite{harmeling2010multiframe}.

\section{Conclusion}
In this chapter, we have introduced the maximum-likelihood estimator and have shown
that it is consistent and asymptotically efficient. This estimator is the
solution of an optimization problem. We have presented gradient descent and
Newton's method, two popular iterative optimization methods that can be used to
maximize the likelihood directly. Lastly, we explored the special case of latent variable models,
where the maximum likelihood estimator can be estimated via the EM algorithm.

In the next chapter, we introduce the necessary background on neuro-imaging.
