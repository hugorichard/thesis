When exposed to naturalistic stimuli (e.g. movie watching or simulated driving), subjects' experience is closer to their every-day life than with classical
psychological experiments.
% 
This makes naturalistic paradigms an attractive class of
stimulation protocols for brain imaging.
%
However, such stimulations are difficult to model, therefore the statistical analysis of the data using supervised regression-based approaches is challenging.
This has motivated the use of unsupervised learning methods that do not make
assumptions about what triggers brain activations in the presented stimuli.

In this chapter, we first present \emph{independent component analysis} (ICA), a widely used
unsupervised method for neuroimaging studies routinely applied on individual
subject electroencephalography (EEG)~\cite{makeig1996independent},
magnetoencephalography (MEG)~\cite{vigario1998independent} or functional MRI
(fMRI)~\cite{mckeown1998independent} data. Then, we review \emph{multiview} unsupervised 
techniques that leverage the availability of data from multiple subjects
performing the same experiments. 

\section{Independent component analysis}
\label{sec:ica}
Independent component analysis (ICA) models a set of signals as the product of a \emph{mixing matrix} and a
\emph{component} matrix containing independent components. As will be seen in this
section, the required assumptions on the independent components to guarantee
identifiability are rather weak, making ICA a method of choice to analyze the
data of subjects exposed to a stimulus that is difficult to quantify.

ICA is applied to fMRI data to analyze resting state
data~\cite{beckmann2005investigations} or when subjects are
exposed to natural~\cite{malinen2007towards}~\cite{bartels2005brain} or complex stimuli~\cite{calhoun2002different}. 
In M/EEG processing, it is widely used to isolate acquisitions artifacts from neural signal~\cite{jung1998extended}, and to identify brain components of interest~\cite{vigario2000independent, delorme2012independent}.

Mainly for computational reasons, it is often assumed that the number of components
$k$ is much lower than the dimensionality of the data $v$.
However in ICA, the dimensionality of the data must be equal to the
number of components.
We therefore first present principal component analysis, a standard method to perform
dimension reduction, then present non-Gaussian ICA and non-stationary ICA.

\subsection{Principal component analysis (PCA)}
Let us assume our data are given by $n$ observations of a random vector $\xb \in
\RR^{v}$ that we stack into a matrix $X \in \RR^{v \times n}$. The data
are centered ($\EE[\xb] = 0$). Principal component analysis (PCA) yields an
orthonormal familly of $p$ vectors $R \in \RR^{v \times p}, R^{\top} R
= I_p$, such that the projected data $R R^{\top} \xb$ does not yield a large mean reconstruction error in Frobenius
norm.
The corresponding optimization problem is given by:
\begin{align}
  &\argmin_{R, R^{\top}R = I} \EE_{\xb}\|\xb - R R^{\top} \xb \|^2 \\
  &=\argmin_{R, R^{\top}R = I} \EE_{\xb} \trace(  \xb^{\top}(I - R R^{\top}) (I - R R^{\top}) \xb) \\
  &= \argmin_{R, R^{\top}R = I} \EE_{\xb}(\|\xb \|^2 - \|R^{\top} \xb \|^2) \\
  &= \argmax_{R, R^{\top}R = I} \EE_{\xb} \|R^{\top} \xb \|^2 \\
  &= \argmax_{R, R^{\top}R = I} \trace(R^{\top} \VV[\xb] R)
\end{align}
Therefore the matrix $R$ is just given by the first $p$ eigenvectors of
$\VV[\xb]$. In practice, we can use the sample variance $\frac1{n}X X^{\top}$ to estimate $\VV[\xb]$.
%
However, such an estimate is not practical in high dimension: when $v$ is very
large $XX^{\top}$ is a prohibitively large matrix.

In such case, assuming $n$ is small, we rely instead of the \emph{singular value
  decomposition} (SVD) of $X$:
\begin{align}
X= U D V
\end{align}
  where $U \in \RR^{v \times n}$ is an orthogonal matrix ($U^{\top} U
= I_t$) of
\emph{left-singular vectors}, $D \in \RR^{n}$ is a positive diagonal matrix of
\emph{singular values} and $V \in \RR^{n \times n}$ is an orthogonal matrix of
\emph{right-singular vectors}. Such a decomposition can be computed in $\bigO(v
n^2)$ operations which is not prohibitive when $n$ is small enough.
%
Every matrix has a singular value decomposition and, provided that all singular
values are distinct, this decomposition is unique up to a permutation and sign
indeterminacy.
More precisely, if $UDV$ and $U'D'V'$ are two singular decomposition of $X$ and
if $D$ has only distinct values, then $U'=U\Pi\Xi$, $D'=\Pi^{\top}D\Pi$ and $V'
= \Xi \Pi^{\top} V$ where $\Pi$ is a permutation matrix and $\Xi$ a diagonal
matrix with diagonal values in $\{-1, 1\}$.

Let $X = UDV$ be a singular value decomposition of $X$, we have $XX^{\top}= U
D^2 U^{\top}$ and therefore the left-singular vectors $U_p$ corresponding to the
largest $p$ singular values of $X$ yield  the $p$ largest eigenvectors of
$XX^{\top}$. Therefore $R$ is given by the left-singular vectors corresponding
to the largest $p$ singular values: $R=U_p$.

Sometimes the PCA includes whitening $R = U_p D_p^{-1}$ where $D_p$ contains the
 $p$ largest singular values of $X$ so that the components of $R^{\top} \xb$ are
uncorrelated (but $R$ is no longer orthonormal). In this thesis what we call PCA
does not include signal whitening.

After the data are reduced, we can perform independent component analysis (ICA).
ICA can exploit a number of different properties of the signal. We focus in the
next section on non-Gaussian ICA.

\subsection{Non Gaussian ICA}
ICA sees the data $\xb \in \RR^p$ as a linear mixtures of components $\sbb \in \RR^p$.
Data are therefore modeled as:
\begin{align}
  \xb = A \sbb
\end{align}
where $A$ is the unmixing matrix and $\sbb$ is a vector with independent components, of which at most one is Gaussian and whose densities are not reduced to a point-like mass.
Without loss of generality, the data $\xb$ are assumed centered ($\EE[\xb] = 0$).
This section relies heavily on the ICA review~\cite{hyvarinen2000independent}
and on~\cite{cardoso1997infomax}.

\subsubsection{Identifiability}
A matrix $P$ that can be written $P =
\Xi \Pi$, where $\Xi$ is a diagonal matrix and $\Pi$ is a permutation matrix, is
called a \emph{scale and permutation matrix}. If furthermore $\Xi$ only has
diagonal
values in $\{-1, 1\}$, then $P$ is a \emph{sign and permutation matrix}.

It is easily seen that if $\sbb$ is a vector with independent components, of
which at most one is Gaussian and whose densities are not reduced to a
point-like mass, so is $P \sbb$ where $P$ is a scale and permutation matrix.
Therefore, if $\xb = A \sbb$, take $A' = AP^{\top}$ and $\sbb'=P\sbb$ and we have
$\xb = A' \sbb'$.
In other words, there exists a permutation and scaling indeterminacy.

To fix the scaling indeterminacy, we assume that $\sbb$ has unit variance ($\VV[\sbb] = I$).
In addition, we assume that the data are whitened ($\VV[\xb] =
I$). Note that the whitening assumption does not imply any loss of generality. If a matrix $H$
is used to whiten the data, the mixing matrix of the unwhitened data is given by $H^{-1} A$ where $A$ is
the unmixing matrix obtained on the whitened data.
With these assumptions, $A$ is orthogonal.
Indeed, $\VV[\xb] = A^{\top} A = I$. 

Are there any other indeterminacies ? Assume that there exists two mixing matrices $A_1$ and $A_2$ such that
$\xb = A_1 \sbb_1$ and $\xb = A_2 \sbb_2$ with $A_1$ and $A_2$ orthogonal. Then
$\sbb_1 = O \sbb_2$ where $O=A_2 A_1^{\top}$ is an orthogonal matrix. 
The following theorem in~\cite{comon1994independent}
shows that $O$ is necessary a sign and permutation matrix.
\begin{theorem}
  Let $\sbb_2$  be a  vector  with  independent 
  components, of   which  at  most  one  is  Gaussian,  and whose  densities
  are  not  reduced  to  a  point-like  mass. Let $O$ be an orthogonal matrix
  and $\sbb$ such that $\sbb = O \sbb_2$.
  Then, $\sbb$ has independent component if and only if $O$ is a sign and
  permutation matrix.
\end{theorem}

As a result, if we assume that at most one component is Gaussian and that
components densities are not reduced to a point-like mass, ICA is identifiable up to a scale and permutation indeterminacy.

\subsubsection{Infomax: Maximum likelihood estimation}
Infomax is introduced in~\cite{bell1995information} and although initially formulated as a maximum entropy problem, it has been shown in~\cite{cois1997infomax} to be equivalent to maximum likelihood.

Assume for simplicity that the components have the same density $\delta$, denoting $f(x) =
-\log(\delta(x))$ and $f(\xb) = \sum_{i=1}^m f(x_i)$ the expected negative log-likelihood is given by:
\begin{align}
  \loss(W) &= \EE[-l(\xb, \theta)] = \EE[- \log(p(\xb))] \\
        &= -\log(|W|) - \EE[\log(\delta(W \xb))] \label{infomax:varchang} \\ 
        &= -\log(|W|) - \EE[\log(\delta(\yb))] \\
        &= -\log(|W|) + \EE[f(\yb)] \\
        &= -\log(|W|) + \EE[\sum_{i=1}^m f(y_i)]
\end{align}
where $\yb = W \xb$ and $y_i$ is the $i$-th component of $\yb$.
At line~\eqref{infomax:varchang} we used the change of variable $\sbb = W \xb$.

\subsubsection{Optimizing the maximum likelihood: relative gradient, Hessian and approximations}
\label{sec:opt:likelihood:relativegradient}
The matrix $W$ needs to be invertible. In practice, if the constraint is not
enforced, we could see numerical instabilities appear.
A simple rule that preserves the invertibility of $W$ is to use updates of the
form:
\begin{align}
  W \leftarrow (I + \alpha D)W \label{eq:mult:update}
\end{align}
where $\alpha$ is a small step-size and $D$ is a direction to be found.
Following the same reasonning as in section~\ref{sec:gd}, we assume a small step-size
and write at first order:
\begin{align}
  \argmin_{D, \|D\|=1} \loss((I + \alpha D)W) &= \argmin_{D, \|D\|=1} \loss(W) + \dotp{ \partialfrac{W}{\loss(W)} }{ \alpha D W } \\
                                              &= \argmin_{D, \|D\|=1} \loss(W) + \dotp{ \partialfrac{W}{\loss(W)} W^{\top} }{ \alpha D } \\
                                              &= -\frac{\partialfrac{W}{\loss(W)} W^{\top}}{\|\partialfrac{W}{\loss(W)} W^{\top} \|}
\end{align}
Therefore the steepest direction for that update rule is $D =
\partialfrac{W}{\loss(W)} W^{\top}$ and therefore updates are given by:
$W \leftarrow (I - \alpha G)W$ where $G = \partialfrac{W}{\loss(W)} W^{\top}$ is called the \emph{relative
  gradient}~\cite{cardoso1996equivariant}.
Second order extensions of the method are obtained by following the steps in
section~\ref{sec:qn}. The corresponding Hessian is called the \emph{relative
  Hessian}.


Let us follow~\cite{ablin2018faster} and use a quasi-Newton algorithm to minimize the likelihood.
The relative gradient and Hessian of $\loss$ are given by:
\begin{align}
  G = \EE[f'(\yb) \yb^{\top}] - I_p,
\end{align}
where $\yb = W \xb$ and $f'(\yb) = \partialfrac{\yb}{f(\yb)}$ 
and
\begin{align}
  H_{abcd} =  \delta_{ad}\delta_{bc} + \delta_{ac} \EE[f''(y_a)y_by_d]
\end{align}

Following~\cite{ablin2018faster}, we can approximate the Hessian by
\begin{align}
  \tilde{H}_{abcd} = \delta_{ad} \delta_{bc} + \delta_{ac} \delta_{bd} \Gamma_{ab}
\end{align}
where $\Gamma_{ab} = \EE[f''(y_a) y_b^2]$.
The approximation is exact when the true unmixing matrix is found since $\EE[s_b s_d] =  s_b^2\delta_{bd}$.

The updates are then given by:
\begin{align}
  W \leftarrow (I - \alpha \tilde{H}^{-1} G) W
\end{align}

The approximated Hessian $\tilde{H}$ is block diagonal.
Indeed we have for any matrix $M$:
\begin{align}
  &\begin{bmatrix} (\tilde{H} M)_{ab} \\  (\tilde{H} M)_{ba} \end{bmatrix} = \begin{bmatrix} \Gamma_{ab} & 1 \\ 1 & \Gamma_{ba} \end{bmatrix} \begin{bmatrix} M_{ab} \\ M_{ba} \end{bmatrix} && \text{ if } a \neq b  \nonumber \\
  &(\tilde{H} M)_{aa} =  (1 + \Gamma_{aa}) M_{aa}
 \label{eq:apphess:blocks}
\end{align}
So that each block corresponds to a pair $(a, b)$ and is of size $2$ if $a \neq b$ and of size $1$ when $a=b$.
Therefore $\tilde{H}$ can be easily regularized and inverted.

\subsubsection{Robustness to density mismatch}
In practice, we observe that sometimes, the quasi-Newton algorithm described in
the previous section fails to recover the true mixing matrix. This happens when the
sources used in the model are too far from the actual  generating sources.
%
In this section, we explain
why this happens and derive stability conditions. This follows the work done in~\cite{cardoso1998blind}.

When the density of the sources corresponds to the one used in the model, one
recovers the correct unmixing matrices via the maximum likelihood estimate in the
limit of large samples. This comes from the consistency of maximum likelihood
estimators.
When this is not the case, a mismatch appears which can be quantified. Let us
denote $\xb^*$ the true data. As highlighted in~\eqref{kl-likelihood} the expected
negative log-likelihood coincides, up to a constant, with the KL divergence between the true
distribution of the data and the distribution of the data as hypothesized in the
model:
\begin{align}
  &\loss(W) \\
  &=  D_{KL}(p(\xb^*), p(W^{-1} \sbb)) \\
             &=  D_{KL}(p(W\xb^*), p(\sbb)) \label{eq:robustness:invariance}\\
             &=  D_{KL}(p(W \xb^*), \prod_i p_i(\wb_i\xb^*)) + D_{KL}(\prod_i p_i(\wb_i\xb^*), p(\sbb)) \\
             &=  D_{KL}(p(W \xb^*), \prod_i p_i(\wb_i\xb^*)) + D_{KL}(\prod_i p_i(\wb_i\xb^*), \prod_i p_i(s_i)) \\
  &=  D_{KL}(p(W \xb^*), \prod_i p_i(\wb_i\xb^*)) + \sum_i D_{KL}(p_i(\wb_i\xb^*), p_i(s_i)) \label{eq:robustness:end}
\end{align}
where equation~\eqref{eq:robustness:invariance} comes from the invariance of the KL divergence and we
denote $\prod_i p_i(x_i)$ the product of the marginal densities of $\xb$.

The first term in equation~\eqref{eq:robustness:end} is the mutual information. It quantifies the
independence of unmixed data $W \xb$. The second term quantifies the mismatch
between the assumed distribution of the components and the marginals of unmixed data.
Therefore if the assumed components are too far from the true components, the
recovered unmixing matrices may be far from the true ones.


Looking at the relative gradient $G$, we see that if components are truly
independent, the true unmixing matrix will be a stationary point of the loss up
to a scaling: $G(\Lambda A^{-1})=0$ where $\Lambda=\diag(\lambda_1 \dots \lambda_p)$.
However, in order for the quasi-Newton to reach this point, it must be a local
minimum (the Hessian must be definite positive). 

Let us therefore consider the Hessian $H$ at point $W = \Lambda A^{-1}$ where
$\Lambda$ is chosen such that $G = 0$. The unmixed data $\yb = W \xb = \Lambda \sbb$ are independent and
therefore, we have $\tilde{H} = H$.
As already mentioned, $\tilde{H}$ is block diagonal with blocks described by equation~\eqref{eq:apphess:blocks}.
The condition for stability is that the Hessian is positive definite. Therefore
all the blocks need to be positive definite so we get the conditions:
\begin{align}
  & \forall a, \enspace 1 + \Gamma_{aa} > 0 && \text{ for blocks of size } 1 \\
  & \forall a \neq b, \enspace \Gamma_{ab}\Gamma_{ba} > 1 && \text{ for blocks of size } 2
\end{align}

When the conditions are satisfied, the quasi-Newton algorithms will recover the
true unmixing matrices if initialized close to a solution. However, we have no
theoretical guarantees of convergence because of the non-convexity of the problem.

\subsection{A variation : non-stationary ICA and joint diagonalization}
Up to now we have assumed that samples were independent an identically distributed. In non-stationary ICA, samples are no longer identically distributed as the distribution can vary over time.
This section follows the work in~\cite{pham2001blind}.
The components are assumed to be Gaussian with a variance that varies between
samples but is assumed to be piece-wise constant $\Sigma_t = \Sigma_k$ for $t \in
T_k$ where $(T_k)_k$ is a partition of $[1, 2, \dots, n]$ and $\Sigma_k$ is a
positive diagonal matrix.

The negative empirical expected log-likelihood is given by:
\begin{align}  
  \loss &= -\log(|W|)  + \frac1{n} \sum_k \big( \frac12 \sum_{i \in T_k}\|\Sigma_k^{-\frac12} W \xb^k_i \|^2 + \frac12 \log(|\Sigma_k|) \big)
\end{align}

Denoting $C_k = \sum_{i \in T_k} \xb_i^k (\xb_i^k)^{\top}$ we have 

\begin{align}  
  \loss &= -\log(|W|)  + \frac1{n} \sum_k \big( \frac1{2} \trace(\Sigma_k^{-1} W C_k W^{\top}) + \frac12 \log(|\Sigma_k|) \big)
\end{align}
Minimizing $\loss$ with respect to $\Sigma_k$ yields $\Sigma_k = \diag(W C_k
W^{\top})$ and therefore up to a constant:
\begin{align}  
  \loss &= -\log(|W|)  + \frac1{n} \sum_k \big(\frac12 \log(|\diag(W C_k W^{\top})|) \big)
\end{align}
which up to a constant can be rewritten:
\begin{align}  
  \loss &=\frac1{2n} \sum_k \big(\frac{\log(|\diag(W C_k W^{\top})|)}{\log(|W C_k W^{\top}|)} \big)
\end{align}

$\loss$ is a joint diagonalization criterion. The optimal $W$ can be found via a
quasi-Newton method very similar to the one we used for non-Gaussian
ICA~\cite{ablin2018beyond}. However, the updates only depend on the covariance
matrices $C_k$ and no longer on the number of samples making this approach very
fast when the number of samples is large.

\section{Analysis of MultiView data}
In this section, we present multiview unsupervised techniques suited to
analyze the data of multiple subjects exposed to the same complex stimuli. Such
techniques assume some similarity between the data of different subjects. This
assumption can be justified by the findings of \cite{hasson2004intersubject} showing that brains exposed to the same natural stimuli exhibit synchronous activity.
The task of finding common patterns or responses that are shared between
subjects is called \emph{shared response modeling}.

In the general linear model presented in
section~\ref{sec:glm}, the shared response is assumed to be known. Therefore,
multiple subjects can be studied separately assuming that the data of different
subjects are independent given the shared response.
In the unsupervised setting it may not be so straightforward to deal with
multiple subjects and therefore many different
methods for data-driven multivariate analysis of neuroimaging group studies
have been proposed.
We summarize the characteristics of some of the most commonly used ones.


\subsection{Multiset canonical correlation analysis}
\label{sec:mcca}
Canonical correlation analysis is initially designed to find a linear
combination that maximizes the correlation between two datasets.
The extension to more than two datasets is ambiguous, and many
different generalized CCA methods have been proposed. \cite{kettenring1971canonical} introduces 6 objective functions that reduce to CCA when $m=2$ and \cite{nielsen2002multiset} considered 4 different possible constrains leading to 24 different formulations of Multiset CCA.

In this section, we present the formulation refered to
in~\cite{nielsen2002multiset} as ``SUMCORR with constraint 4'' which is one of the
fastest to fit.

Let us consider $X_1, \dots, X_m \in \RR^{p, n}$, $m$ datasets and consider the following (SUMCORR)
objective:
\begin{align}
  \max_{\ab_1 \in \RR^{v}, \dots, \ab_m \in \RR^{v}} \sum_{i=1}^m \sum_{j=1}^m \dotp{ \ab_i }{ X_i X_j^{\top} \ab_j }
\end{align}
This objective can be arbitrarily large if not constrained. Constraint 4 is
given by:
\begin{align}
  \sum_{i=1}^m \dotp{ \ab_i }{ X_i X_i^{\top} \ab_i } = 1
\end{align}

The Lagrangian is given by:
\begin{align}
  \sum_{i=1}^m \sum_{j=1}^m \dotp{ \ab_i }{ X_i X_j^{\top} \ab_j } - \lambda (\sum_{i=1}^m
  \dotp{ \ab_i }{X_i X_i^{\top} \ab_i } - 1)
\end{align}

Taking the gradient with respect to $\ab_i$ we obtain
\begin{align}
  \sum_{j=1}^m X_i X_j^{\top} \ab_j = \lambda X_i X_i^{\top} \ab_i
\end{align}

This is a generalized eigenvalue problem of the form $C \ab = \lambda D \ab$
where $C$ is a block matrix where block $i,j$ is given by $X_i X_j^{\top}$, 
$D$ is the block diagonal matrix formed by the block $i, i$ of $C$ and $\ab \in
\RR^{m \times v}$ yields the dataset specific projections vectors: $\ab = \begin{bmatrix} \ab_1 \\ \cdots \\
  \ab_m \end{bmatrix}$.

    The leading eigenvector correspond to the first canonical vectors. The
    second canonical vectors is given by the second eigenvalues and so on. They
    are orthogonal for the scalar product: $\dotp{ \ab}{ \bb  }_D =
    \dotp{ \ab}{ D \bb  }$.

  
\subsection{Group independent component analysis}
\label{sec:groupica}
Given the success of ICA in analyzing the data of one subject. It is natural to
look for extensions of ICA in a multiview setting.
Several works assume that the subjects share a common mixing matrix, but with different components~\cite{pfister2019robustifying}~\cite{svensen2002ica}.
% 
Instead, we focus on models where the subjects share a common components matrix, but have different mixing matrices.

\subsubsection{CanICA and ConcatICA}
\label{sec:canicaandconcatica}
In the single subject setting, we reduce the data (for example using PCA) and apply ICA on
reduced data. Therefore a natural framework to perform group ICA is to first aggregate the
data of individual subjects into a single dataset, often resorting to dimension
reduction technique and then apply off-the-shelf ICA on the aggregated dataset.
When PCA is used to aggregate the data, the method is referred to as
ConcatICA~\cite{calhoun2001method}. An alternative is to use multiset canonical
correlation analysis (CCA) leading to a method called CanICA~\cite{varoquaux2009canica}.

This framework has the advantage of being simple and
straightforward to implement since it resorts to customary single-subject
ICA method.

When datasets are high-dimensional, a three steps procedure is often used: first
dimensionality reduction is performed on data of each subject  separately; then
the reduced data are merged into a common representation; finally, an ICA
algorithm is applied for shared components extraction.

CanICA and ConcatICA are popular methods for fMRI~\cite{calhoun2009review} and EEG~\cite{eichele2011eegift} group studies. These methods directly recover only group level, shared components; when individual components are needed, additional steps are required (back-projection \cite{calhoun2001method} or dual-regression \cite{beckmann2009group}).
% 

\subsubsection{Likelihood based method}
\label{sec:guo}
While CanICA and ConcatICA are simple to implement and very fast to fit, they do
not rely on maximum likelihood estimators. Therefore they do not benefit of
advantages of such estimators such as asymptotic efficiency.

The model of~\cite{guo2008unified} considers the very general model $\xb_i =
A_i\sbb + \nb_i$ where
$A_i$ represent mixing matrices, $\sbb$ the common sources and $\nb_i$ are view
specific noise. Therefore this model is an instance of noisy ICA as defined
in~\cite{hyvarinen1999Gaussian}. The noise covariance is learned from the data
and each source is assumed to be a mixture of Gaussians
$p(s_j) = \sum_{z=1}^q \Ncal(\mu_{zj}, \sigma_{zj})$. The parameters of the
Gaussian mixtures are learned which makes the E-step impossible to compute in closed form.
In order to solve this issue, an approximate E-step is introduced.
Unfortunately, it leads to an update rule involving a sum over $q^p$ terms making their algorithm intractable when the number of components $p$ is larger than $20$. 

In this section, we show why a sum of an exponential number of terms appears.
We start with the same model as in~\cite{guo2008unified} but to make the
computations more tractable, we assume that the Gaussian mixture is given by:
\begin{align}
  &p(s_j) = \frac1q \sum_{\alpha_j \in \mathcal{A}} p(s_j | \alpha_j) \\
  &p(s_j | \alpha_j) = \mathcal{N}( s_j; 0, \alpha_j),
\end{align}
where $\alpha_j$ takes its value in a known
discrete set $\mathcal{A}$ with equal probability $\frac1{q}$ where $q$ is the
cardinal of $\mathcal{A}$.
We call $\alphab$ the random vector with
independent coordinates such that coordinate $j$ is given by $\alpha_j$.
We further assume that the noise distribution is the same for all components and
all subjects leading to the formulation:
\begin{align}
  \xb = A \sbb + \nb
\end{align}
where $A = \begin{bmatrix} A_1 \\ \vdots  \\ A_m \end{bmatrix}$,
$\xb = \begin{bmatrix} \xb_1 \\ \vdots  \\ \xb_m \end{bmatrix}$ and $\nb
= \begin{bmatrix} \nb_1 \\ \vdots  \\ \nb_m \end{bmatrix}$ with $\nb \sim \Ncal(0, \sigma^2 I)$.
This formulation is a special case of~\cite{guo2008unified} and constitues a single subject
noisy ICA problem almost identical to~\cite{moulines1997maximum}.

We now follow~\cite{moulines1997maximum} and write:
\begin{align}
  &p(\xb, \sbb, \alphab) \\
  &= p(\xb | \sbb) p(\sbb|\alphab)p(\alphab) \\
                        &= \Ncal(\xb; A\sbb, \sigma^2 I_p)\mathcal{N}( \sbb; 0, \diag(\alphab)) \frac1{2^p} \\
  &\propto \frac{\exp\big(-\frac12(\frac1{ \sigma^2}\|\xb - A\sbb \|^2 + \dotp{ \sbb }{ \diag(\alphab)^{-1} \sbb})\big)}{(|\sigma^2 I_p| |\diag(\alphab)|)^{\frac12}}  \\
  &= \frac{\exp\big(-\frac12(\frac1{ \sigma^2}(\|\xb \|^2 - 2 \dotp{ \xb}{ A \sbb } + \|A \sbb \|^2)  + \dotp{ \sbb }{ \diag(\alphab)^{-1} \sbb}) \big)}{(|\sigma^2 I_p| |\diag(\alphab)|)^{\frac12}} \\
  &\propto \frac{\exp\big(-\frac12( \dotp{ \sbb - \mu_{\alphab}}{ V_{\alphab}^{-1} (\sbb - \mu_{\alphab}) } - \dotp{ \mu_{\alphab} }{V_{\alphab}^{-1} \mu_{\alphab} }) \big)}{(|\sigma^2 I_p| |\diag(\alphab)|)^{\frac12}},
\end{align}
where $V_{\alphab} = (\frac1{\sigma^2}A^{\top}A + \diag(\alphab)^{-1})^{-1}$, $\mu_{\alphab} = \frac1{\sigma^2}V_{\alphab}
A^{\top} \xb$ and the proportionality constant contains terms that do not depend
on $\sbb$ or $\alphab$.

From $p(\xb, \sbb, \alphab)$, we get:
\begin{align}
  p(\sbb | \xb, \alphab) =  \Ncal(\sbb, \mu_{\alphab}, \Sigma_{\alphab})
\end{align}

Then, we have that:
\begin{align}
  p(\alphab | \xb) &= \int_{\sbb} p(\alphab, \sbb | \xb) d\sbb \\
                    &\propto \int_{\sbb} p(\xb, \sbb ,\alphab) d\sbb \\ 
                   &\propto \frac{\exp\big(\frac12(\dotp{ \mu_{\alphab} }{V_{\alphab}^{-1} \mu_{\alphab} }) \big)}{(|\diag(\alphab)| |V_{\alphab}^{-1}|)^{\frac12}}\\ 
\end{align}
where we leave out terms that do not depend on $\alphab$.
The normalizing constant can be computed by summing over possible values of
$\alphab$:
\begin{align}
  p(\alphab | \xb) &= \frac{\frac{\exp\big(\frac12(\dotp{ \mu_{\alphab} }{V_{\alphab}^{-1} \mu_{\alphab} }) \big)}{(|\diag(\alphab)| |V_{\alphab}^{-1}|)^{\frac12}}}{\sum_{\alphab, \alpha_j \in \mathcal{A}} \frac{\exp\big(\frac12(\dotp{ \mu_{\alphab} }{V_{\alphab}^{-1} \mu_{\alphab} }) \big)}{(|\diag(\alphab)| |V_{\alphab}^{-1}|)^{\frac12}}}
\end{align}

Then, we can obtain a formula in closed form for $p(\sbb | \xb)$ using
\begin{align}
  p(\sbb | \xb) = \sum_{\alphab, \alpha_j \in \mathcal{A}} p(\sbb | \xb, \alphab) p(\alphab | \xb)
\end{align}

The problem here is that the size of the set $\{\alphab, \alpha_j \in
\mathcal{A} \}$ is $q^p$ where $q=|\mathcal{A}|$ is the cardinal of
$\mathcal{A}$. This quantity quickly gets large when $p$ increases making
$p(\sbb | \xb)$ difficult to compute.

In this section, we have studied a simplified version of the
model in~\cite{guo2008unified} and shown that it is difficult to fit. This is
often the case with maximum likelihood approaches. Despite, their advantages they
are often intractable.

\subsection{Independent vector analysis}
\label{sec:IVA}
Independent vector analysis~\cite{lee2008independent} (IVA) models the data as a
linear mixture of independent components $\xb_i = A_i \sbb_i$, where each
component $s_{ij}$ of a given view $i$ can depend on the corresponding component
in other views: $\sbb_{[j]} = (s_{ij})_{i=1}^m$ are not independent.

Introducing $\xb \in \RR^{m \times p}$ such that $\xb  = [\xb_1, \dots, \xb_m]^{\top}$,
$\sbb \in \RR^{m \times p}$ such that $\sbb = [\sbb_1, \dots, \sbb_m]^{\top}$ and $\yb
\in \RR^{m \times p}$ such that $\yb = [\yb_1, \dots, \yb_m]^{\top}$ where $\yb_i = W_i
\xb_i$ with $W_i = A_i^{-1}$, the
expected negative log-likelihood is given by:
\begin{align*}
  \loss &= - \EE[log(p(\xb))] \\
        &= \sum_{i=1}^m -\log(|W_i|) -\EE[\log(p(\yb))] \\
        &= \sum_{i=1}^m - \log(|W_i|) + \sum_{j=1}^p -\EE[\log(p_{\sbb_{[j]}}(\yb_{[j]}))]
\end{align*}
where we used the notation $\yb_{[j]} = (y_{ij})_{i=1}^m$.

The optimization can be carried out using alternate minimization keeping the
mixing matrices of all subjects fixed but one. We can rely on the relative
gradient as in section~\ref{sec:opt:likelihood:relativegradient} and use update
of the form $W_i \leftarrow (I - \alpha_i G_i) W_i$ where $\alpha_i$ is given by
backtracking line search and $G_i$ is the relative gradient given by:
\begin{align}
  G_i = -I_p + \EE[\phi_i(\yb_i) \yb_i^{\top}]
\end{align}
where component $j$ of $\phi_i(\yb_i)$ is given by
\begin{align}
  \phi_{ij}(\yb_i) = \partialfrac{y_{ij}}{-\log(p_{\sbb_{[j]}}(\yb_{[j]}))}
\end{align}

Practical implementations of this general model assume a distribution for
$p_{\sbb_{[j]}}$.
In IVA-L~\cite{lee2008independent},
\begin{align}
  p_{\sbb_{[j]}}(\yb_{[j]}) \propto \exp(-\sqrt{\sum_i (y_{ij})^2})
\end{align}
and therefore
\begin{align}
  \phi_{ij}(\yb_i) &= \frac{y_{ij}}{\sqrt{\sum_i (y_{ij})^2}}
\end{align}

In IVA-G~\cite{anderson2011joint}~\cite{via2011maximum},
\begin{align}
  p_{\sbb_{[j]}}(\yb_{[j]}) = \Ncal( \yb_{[j]}; 0, \Sigma_j)
\end{align}
and therefore
\begin{align}
  \phi_{ij}(\yb_i) &= \sum_l \Sigma_j^{-1}[il] y_{lj}
\end{align}
where  $\Sigma_j^{-1}[il]$ is the coordinate $i, l$ of $\Sigma_j^{-1}$ and
$y_{lj}$ is the $j$th coordinate of $\yb_i$.

In IVA-G, an estimate of $\Sigma_j$ is needed at each iteration. This is
computed using the sample covariance:
\begin{align}
\Sigma_j = \frac1{n} Y_{[j]} Y_{[j]}^{\top}
\end{align}

Second order extensions and Hessian approximations can be used in IVA as well.
This is described in~\cite{anderson2011joint}. Also note that although IVA-G and
IVA-L are the two most popular implementations of the IVA framework, others exist
(see for instance the work in~\cite{anderson2013independent}).

\subsection{Hyperalignment}
Hyperalignment is a model initially designed for fMRI data to reduce
inter-subject variability~\cite{haxby2011common}.

Let us assume we have access to the data of two subjects: $\xb_1, \xb_2$.
Assuming these subjects are exposed to a time-locked stimuli (such as a movie), a possible alignment is given by the Procrustes transform:
\begin{align}
min_{P \in \RR^{p \times p}, PP^{\top} = I_p} \EE[\|P \xb_1 - \xb_2 \|_2]
\end{align}
This can be solved efficiently by
\begin{align}
  P = \Pcal(\EE[\xb_2 \xb_1^{\top}])
\end{align}
where $\Pcal$ is the projection on the orthogonal manifold: $\Pcal(M) = M
(M^{\top}M)^{-\frac12}$. In practice $\Pcal(M)$ is computed by performing an SVD
of $M$, $M = U_M D_M V_M $ so that $\Pcal(M) = U_M V_M$.

Hyperalignment is the combination of the Procrustes transform and an iterative
procedure to produce a template from multiple alignments.
In an initialization step, a random subject $i$ is chosen and the alignment
between all subjects $s \neq i$ and the target are computed. The initial
template $\tb$ is given by the averaged aligned data. Then, all subjects are
aligned to the current template $\tb$ and the template is recomputed using the
averaged aligned data. This procedure is repeated for a given number of
iterations until convergence.

The intuition behind the iterative procedure is that averaging the aligned data
will tend to move the template away from the initial target. However, they are
no theoretical guarantees associated with this procedure and even no associated loss.
We describe formally
the method in Algorithm~\ref{algo:hyperalignment}.

\begin{algorithm}[H]
  \SetAlgoLined
  \caption{Hyperalignment}
  \label{algo:hyperalignment}
  \KwIn{Data $\Xb_1, \dots, \Xb_m \in \RR^{p \times n}$,  number of iterations
    $n_{iter}$}
  $\blacktriangleright$ Select a random subject \\
  $i \sim \Ucal(1, m)$ \\
  $\blacktriangleright$ Initialize the alignment operators \\
  \For{$s=1 \dots m$}{
    \If{$s=i$}{
      $P_{st} = I_p$ \\
    }
    \Else{
      $P_{st} = \Pcal(X_t X_s^{\top})$ \\
    }
  }
  $T = \frac{\sum_{s=1}^m P_{st} X_s}{m}$ \\

  $\blacktriangleright$ Main loop \\
  \For{$it=1 \dots n_{iter}$}
  {
    $\blacktriangleright$ Align data and the current template \\
    \For{$s=1 \dots m$}{
      $P_{st} = \Pcal(T X_s^{\top})$ \\
    }

    $\blacktriangleright$ Compute the template as the mean of aligned data \\
    $T = \frac{\sum_{s=1}^m P_{st} X_s}{m}$ \\
    }
  \Return{Estimated template $T$ and operators $P_{st}$}
\end{algorithm}



\subsection{The shared response model (SRM)}
\label{sec:srm:review}
The shared response model~\cite{chen2015reduced} is a multi-view latent factor
model. The data $\xb_1 \dots \xb_m$ are modeled as random vectors following the model:
\begin{align}
 &\xb_i = A_i \sbb + \nb_i \\
  &A_i^{\top}A_i = I_p
  \label{eq:model:srm}
\end{align}
where $\xb_i \in \RR^v$ is the data of view $i$, $A_i \in \RR^{p \times v}$ is the
mixing matrix of view $i$, $\nb_i$ is the noise of view $i$ and $\sbb \in \RR^p$ are the
shared components referred to as the \emph{shared response} in fMRI applications.
The mixing matrices
$A_i$ are assumed to be orthogonal so that $A_i^{\top}A_i = I_p$. However, in
general the matrix $A_i A_i^{\top}$ is different from identity. The noise
$\nb_i$ is assumed to be Gaussian with covariance $\Sigma_i$ and independent
across views. We assume the number of features $v$ to be much larger than the
number of components $p$: $v \gg p$.

The conceptual figure~\ref{fig:srm:conceptual_figure} illustrates an 
application of the shared response model to fMRI data. The mixing
matrices are spatial topographies specific to each subjects while the shared
components give the common timecourses.

\begin{figure}
  \centering
  \includegraphics[scale=0.3]{figures/srm/conceptual_figure31.png}
  \caption{\textbf{Shared response model}: The raw fMRI data are modeled as a weighted combination of subject-specific spatial components with additive noise. The weights are shared between subjects and constitute the shared response to the stimuli.}
  \label{fig:srm:conceptual_figure}
\end{figure}

In~\cite{chen2015reduced, anderson2016enabling}, two versions of the shared response model are
introduced which we now present.
\subsubsection{Deterministic shared response model}
\label{sec:deterministicsrm}
Let us consider $n$ observations of $\xb_i$ and $\sbb$ that we stack into
matrices $X_i \in \RR^{v, n}$ and $S \in \RR^{p, n}$.
The deterministic shared response model sees both the mixing matrices $A_i$ and
the $n$ observations of the shared response $S$ as parameters to be
estimated. The noise variance is fixed to a multiple of identity: $\forall i,
\Sigma_i=\sigma^2 I_v$ where $\sigma$ is an hyper-parameter to choose.
The model is optimized by maximizing the log-likelihood.
The likelihood is given by: $p(\xb) = \prod_i \Ncal(\xb_i; A_i \sbb, \sigma^2 I)$ and
therefore the empirical expected negative log-likelihood is given up to a constant independent of
$A_i$ and $S$ by:
\begin{align}
  \loss = \frac1{n} \sum_i \|A_i S - X_i \|^2 = \frac1{n} \big(\| S \|^2 -2 \dotp{ A_i S}{ X_i } + \| X_i \|^2 \big)
  \label{eq:detsrmloss}
\end{align}
The negative log-likelihood $\loss$ is optimized by performing alternate minimization on $(A_1 \dots A_m)$
and $S$. Note that the hyper-parameter $\sigma$ does not have an influence on
the loss and can therefore be safely ignored.

The gradient with respect to $S$ is given by $\sum_i A_i^{\top}(A_i S -
X_i) = \sum_i (S -
A_i^{\top} X_i)$
yielding the closed form updates:
\begin{equation}
  S \leftarrow  \frac1m \sum_i (A_i^{\top} X_i)
  \label{eq:srm:supdate}
\end{equation}

From \eqref{eq:detsrmloss}, minimizing $\loss$ with respect to $A_i$ is
equivalent to maximizing $\dotp{ A_i}{ X_i S^{\top} }$ and therefore we
have:
\begin{equation}
  A_i \leftarrow  \Pcal(\frac1{n}X_i S^{\top})
  \label{eq:detsrm:Aiupdate}
\end{equation}
where $\Pcal$ is the projection on the Stiefel manifold: $\Pcal(M) = M
(M^{\top}M)^{-\frac12}$.

The complexity of Deterministic SRM is in $\bigO(\mathrm{n_{iter}} mpvn)$ where
$n$ is the number of samples and $\mathrm{n_{iter}}$ the number of iterations.
We monitor the convergence by looking at the $\ell_{\infty}$ norm of the
gradient. Note that we can monitor the gradient without any increase in complexity.
Indeed, after the updates with respect to each mixing matrix have been
carried out, only the gradient with respect to $S$ remains: $\sum_i
(S - A_i^{\top}\xb_i)$. The algorithm is stopped when the
gradient falls below a chosen tolerance.

\subsubsection{Probabilistic SRM}
\label{sec:probabilisticsrm}
In Probabilistic SRM , $\Sigma_i=\sigma_i^2 I_v$ and the shared
components are assumed to be Gaussian $\sbb \sim \Ncal(0, \Sigma_s)$.
In~\cite{chen2015reduced}, $\Sigma_s$ is only assumed to be definite positive. However,
as will be seen in the FastSRM chapter (chapter~\ref{ch:fastsrm1}), enforcing a diagonal $\Sigma_s$ ensures
identifiability (provided the diagonal values are different). So we assume that $\Sigma_s$ is diagonal.

The model is optimized via the expectation maximization algorithm.
Denoting $\VV[\sbb | \xb] = (\sum_i \frac1{\sigma_i^2} I +
\Sigma_s^{-1})^{-1}$ and $\EE[\sbb | \xb] = \VV[\sbb | \xb] \sum_i \frac1{\sigma_i^2}
A_i^{\top}\xb_i$, we have
\begin{align}
  p(\xb, \sbb) &= \prod_i \frac{\exp(-\frac{\|\xb_i - A_i \sbb \|^2}{2 \sigma_i^2})}{(2 \pi \sigma_i^{2v})^{\frac12}} \frac{\exp(-\frac12 \dotp{ \sbb }{ \Sigma_s^{-1} \sbb } )}{(2 \pi | \Sigma_s|)^{\frac12}} \\
               &= c_1 \exp(-\frac12 \left( \sum_i \frac1{\sigma_i^2}\|\xb_i\|^2 - 2  \dotp{ \sum_i \frac1{\sigma_i^2} A_i^{\top}\xb_i}{ \sbb } \right. \\& \left.+ \sum_i \frac1{\sigma_i^2} \| \sbb \|^2 + \dotp{ \sbb}{ \Sigma_s^{-1} \sbb }  \right)) \\
               &= c_2(\xb) \exp(-\frac12 \left( \dotp{  \sbb - \EE[\sbb }{ \xb], \VV[\sbb | \xb]^{-1} ( \sbb - \EE[\sbb | \xb])  } \right)) \label{probsrmcompleted}
\end{align}
where $c_1 = \frac1{(2 \pi \sigma_i^{2v})^{\frac12}}\frac1{(2 \pi |
  \Sigma_s|)^{\frac12}}$ and $c_2(\xb) = c_1 \exp(-\frac12( \sum_i
\frac1{\sigma_i^2}\|\xb_i\|^2 - \dotp{  \EE[\sbb }{ \xb], \VV[\sbb | \xb]^{-1} \EE[\sbb | \xb] }))$ are independent of $\sbb$.
Therefore $\sbb| \xb \sim \Ncal(\EE[\sbb | \xb], \VV[\sbb, \xb])$

The negative expected completed log-likelihood is given by
\begin{align}
	\loss = \sum_i \frac12 v \log(\sigma_i^2) + \frac1{2 \sigma_i^2} \EE[\| \xb_i - A_i \sbb \|^2]
\end{align}
updates are therefore given by:
\begin{align}
&\sigma_i^2 \leftarrow \frac1{v} (\EE[\| \xb_i - A_i \EE[\sbb|\xb]\|^2] + \| \diag(\VV[\sbb | \xb]) \|^2) \\
  &A_i \leftarrow \Pcal(\EE[\xb_i \EE[\sbb|\xb]^{\top}]) \label{eq:psrm:Aiupdate} \\
  & \Sigma_s \leftarrow \VV[\sbb | \xb] + \EE[\EE[\sbb | \xb] \EE[\sbb | \xb]^{\top}]
\end{align}

It is useful to access the log-likelihood to check the implementation of the
algorithm and monitor the convergence. From equation~\eqref{probsrmcompleted},
the likelihood is given by:
\begin{align}
  p(\xb) &= c_2(\xb) \int_{\sbb} \exp(-\frac12 \left( \dotp{  \sbb - \EE[\sbb }{ \xb], \VV[\sbb | \xb]^{-1} ( \sbb - \EE[\sbb | \xb])  } \right)) d\sbb \\
         &= c_2(\xb) (2 \pi |\VV[\sbb | \xb]|)^{\frac12}
\end{align}
replacing $c_2(\xb)$ by its expression and taking the log, the expected negative
log-likelihood is (up to constants) given by:
\begin{align}
  \EE[-\log(&p(\xb))] = \sum_i \frac{v}{2} \log(\sigma_i^2) + \frac12 \log(|\Sigma_s|) - \frac12 \log(|\VV[\sbb | \xb]|) \nonumber \\ &+ \sum_i
  \frac12 \frac1{\sigma_i^2} \EE[\|\xb_i\|^2] - \frac12 \EE[\dotp{  \EE[\sbb }{ \xb], \VV[\sbb | \xb]^{-1} \EE[\sbb | \xb] }]
\end{align}

The complexity of Probabilistic SRM is $\bigO(\mathrm{n_{iter}} mpvn)$, the same as in
Deterministic SRM.
We can monitor the convergence by looking at the log-likelihood decrease at each iteration
and stop the algorithm when the magnitude of the decrease is below some
tolerance.
The storage requirements of Deterministic or Probabilistic SRM are in
$\bigO(mvn)$ which simply means that the dataset needs to hold in memory.

\section{Conclusion}
In this chapter, we have reviewed several methods to perform unsupervised
analysis of neuroimaging data. We have introduced methods most suited to the analysis
of the data of a single subject such as ICA and have explored some of the
extensions to multiple subjects such as CanICA, ConcatICA, IVA or SRM.
In the next chapter, we present our first contribution: an efficient
implementation of SRM that we call FastSRM.
% \section{Related Work}
% \label{sec:rel_work}
% %

% %
% %

% %
% %
% %

% \textbf{Structured mixing matrices} One strength of our model is that we only assume that the mixing matrices are invertible and still enjoy identifiability whereas some other approaches impose additional constraints. For instance tensorial methods~\cite{beckmann2005tensorial} assume that the mixing matrices are the same up to diagonal scaling.
% %
% Other methods impose a common mixing matrix~\cite{cong2013validating, grin2010independent, calhoun2001fmri, Monti18UAI}. Like PCA, the Shared Response Model~\cite{chen2015reduced} (SRM) assumes orthogonality of the mixing matrices. While the model defines a simple likelihood and provides an efficient way to reduce dimension, the SRM model is not identifiable as shown in appendix~\ref{sec:app_identifiability}, and the orthogonal constraint may not be plausible.
% %

% \textbf{Deep Learning} Deep Learning methods, such as convolutional auto-encoders (CAE), can also be used to find the subject specific unmixing~\cite{chen2016convolutional}. While these nonlinear extensions of the aforementioned methods are interesting, these models are hard to train and interpret. In the experiments on fMRI data in appendix~\ref{appendix_reproduce}, we obtain better accuracy with MultiView ICA than that of CAE reported in~\cite{chen2016convolutional}.

% \textbf{Correlated component analysis} Other methods can be used to recover the shared neural responses such as the correlated component approach of Dmochowski~\cite{dmochowski2012correlated}. We benchmark our method against its probabilistic version~\cite{kamronn2015multiview} called BCorrCA in Figure~\ref{fig:meg}. Our method yields much better results. 

% \textbf{Autocorrelation} Another way to perform ICA is to leverage spectral diversity of the components rather than non-Gaussianity.
% %
% These methods are popular alternative to non-Gaussian ICA in the single-subject setting~\cite{tong1991indeterminacy, belouchrani1997blind, pham1997blind} and they output significantly different components than non-Gaussian ICA~\cite{delorme2012independent}.
% %
% Extensions to multiview problems have been proposed~\cite{lukic2002ica, congedo2010group}.
