\chapter{MultiViewICA}
\section{Likelihood}
\label{sec:app_likelihood}
\subsection{Initial form of likelihood}\label{sec:appendix:likelihood_transform}

To derive the likelihood, we start by conditioning on $\sbb$. Then, we make a variable transformation from $\xb_i$ to $\nb_i=W_i\xb_i-\sbb$, as opposed to the transformation to $\sbb$ as is usual in ICA. Using the probability transformation formula, we obtain
\begin{equation}
p_{\xb_i|\sbb}(\xb_i|\sbb)=|W_i|p_{\nb_i}(W_i\xb_i-\sbb)    
\end{equation}
where $p_{\nb_i}$ is the density of $\nb_i$. Note that the $\xb_i$ are conditionally independent given $\sbb$, so we have:
\begin{equation}
  p_{\xb|\sbb}(\xb|\sbb)=\prod_{i=1}^m  |W_i| p_{\nb_i}(W_i\xb_i-\sbb)
\end{equation}
and we next get the joint density as:
\begin{equation}
  p_{\xb, \sbb}(\xb,\sbb)=p_{\sbb}(\sbb) \prod_{i=1}^m  |W_i| p_{\nb^i}(W_i\xb_i-\sbb)
\end{equation}
Integrating out $\sbb$ gives Eq.~(\ref{eq:likelihood}).


\subsection{Integrating out the components}\label{sec:appendix:integration}

The integral in question, after factorization, is given by
\begin{equation}
\int_{\sbb} \prod_{j=1}^k \exp \left( -\frac{1}{2\sigma^2} \sum_{i=1}^m ((\wb_j^i)^{\top}\xb_i-s_j)^2 \right) p_{s}(s_j) d\sbb
\end{equation}
which factorizes for each $j$. Denote $y^i_j=(\wb_j^i)^{\top}\xb_i$ and $\tilde{s_j}=\frac1m\sum_{i=1}^m y^i_j$.  Fix $j$, and drop it to simplify notation. Then we need to solve the integral
\begin{align*}
   &\int_s \exp \left(-\frac{1}{2\sigma^2} \sum_{i=1}^m (y^i-s)^2 \right) p_{s}(s)ds\\
   &=\int_s \exp \left(-\frac{1}{2\sigma^2} [ m(\tilde{s}-s)^2 + \sum_{i=1}^m (y^i-\tilde{s})^2] \right) p_{s}(s)ds \\ 
&= \exp \left(-\frac{1}{2\sigma^2}\sum_{i=1}^m (y^i-\tilde{s})^2 \right) 
\int_z \exp \left(-\frac{m}{2\sigma^2} z^2 \right) p_{s}(\tilde{s}-z) dz
\end{align*}

where we have made the change of variable $z=\tilde{s}-s$. The remaining integral simply means that $d$ is smoothed by a Gaussian kernel, which can be computed exactly if $d$ is a Gaussian mixture. We therefore define $f(s) = \log \left(\int_z \exp \left(-\frac{m}{2\sigma^2} z^2 \right) p_{s}(s-z) dz\right)$.
\section{Initialization of MultiViewICA}
\label{sec:app_init}
Since the cost function $\mathcal{L}$ is non-convex, having a good initialization can make a difference in the final result.
% 
We propose a two stage approach.
% 
We begin by applying PermICA on the datasets, which gives us a first set of unimixing matrices $W_1^1, \dots, W_1^m$.
% 
Note that we could also use GroupICA for this task.
% 
Next, we perform a diagonal scaling of the mixing matrices, i.e. we find the diagonal matrices $\Lambda^1, \dots, \Lambda^m$ such that $\mathcal{L}(\Lambda^1W_1^1, \dots, \Lambda^mW_1^m)$ is minimized.
% 
To do so, we employ Algorithm~\ref{algo:mv_ica} but only take into account the diagonal of the descent direction at each step: the update rule becomes $W_i \leftarrow (I_k + \rho \text{Diag}(D))W_i$.
% 
The initial unmixing matrices for Algorithm~\ref{algo:mv_ica} are then taken as $\Lambda^1W_1^1, \dots, \Lambda^mW_1^m$.

Empirically, we find that this two stage procedure allows for the algorithm to start close from a satisfactory solution.
\section{Proofs of Section~\ref{sec:mvica}}
\label{sec:app_proofs}
\subsection{Proof of Prop.~\ref{prop:identifiability}}
\label{app:proof:mvica:identifiability}
We fix a subject $i$. Since $\sbb$ has independent components, so does $\sbb + \nb_i$. Following~\cite{comon1994independent},
Theorem 11, there exists a scale-permutation matrix $P^i$ such that $A'_i =
A_iP^i$. As a consequence, we have $\sbb  + \nb_i = P^i(\sbb' + \nb'^i)$ for all
$i$.

Then, we focus on subject 1 and subject $i \neq 1$:
\begin{align}
  &\sbb + \nb^1 - (\sbb + \nb_i) = P^1(\sbb' + \nb'^1) - P^i(\sbb' + \nb'^i)\\
  &\nb^1 - \nb_i = P^1(\sbb' + \nb'^1) - P^i(\sbb' + \nb'^i)\\
  &\iff P^1\sbb' - P^i\sbb' = P^i \nb'^i - \nb_i + \nb^1 - P^1 \nb'^1 \label{eq:condition_gaussian}
\end{align}
% This shows that $P^1\sbb' - P^i\sbb'$ is Gaussian which can only happen if $P^1= P^i$. 
Since the right hand side of equation (\ref{eq:condition_gaussian}) is a linear combination of Gaussian random variables, this would imply that $P^1\sbb' - P^i\sbb'$ is also Gaussian. However, given that $\sbb'$ is assumed to be non-Gaussian, the equality can only hold if $P^1
= P^i$ and both the right and the left hand side vanish.
Therefore, the matrices $P^i$ are all equal, and there exists a scale and permutation matrix $P$ such that $A'_i = A_iP$.

\subsection{Proof of Prop.~\ref{prop:robust}}
\label{ref:robust}
We consider $W_i = \Lambda (A_i)^{-1}$, where $\Lambda$ is a diagonal matrix.
%
We recall $\xb_i= A_i (\sbb + \nb_i),$ so that $\yb_i = W_i\xb_i= \Lambda(\sbb + \nb_i)$.
%
The gradient of $\loss$ is given by equation~\eqref{eq:gradient}:
\begin{align}
  \mathcal{G}^i &= \frac{1}{m}f'(\tilde{\sbb})(\sbb + \nb_i)^{\top}\Lambda + \frac{1 - 1/m}{\sigma^2} \Lambda\left(\nb_i - \frac{1}{m-1}\sum_{j\neq i} \nb^j\right)(\sbb + \nb_i)^{\top}\Lambda - I_k \\
  & = \frac{1}{m}f'(\Lambda(\sbb + \frac1m\sum_j\nb^j))(\sbb+\nb_i)^{\top} \Lambda + \frac{\sigma'^2(1 - 1/m)}{\sigma^2} \Lambda^2 - I_k
\end{align}
where we write $f'(\sbb) = \begin{bmatrix}f'(s_1) \\ \vdots \\ f'(s_k) \end{bmatrix}$.
Therefore, $\mathcal{G}^i$ is diagonal and constant across subjects (because $f'(\Lambda(\sbb + \frac1m\sum_j\nb^j))(\nb_i)^{\top} = f'(\Lambda(\sbb + \frac1m\sum_j\nb^j))(\nb^{i'})^{\top}$).
Let us therefore consider only its coefficient $(a, a)$, and let $\lambda = \Lambda_{aa}$:
$$
\mathcal{G}^i_{aa} =G(\lambda) = \phi(\lambda)\lambda + \frac{\sigma'^2(1 - 1/m)}{\sigma^2}\lambda^2 - 1,
$$
where $\phi(\lambda) = \frac{1}{m}f'(\lambda(s_a + \frac1m\sum_j n^j_a))(s_a+n_a^i)$. One the one hand, we have $G(0) = -1$. On the other hand, if we assume for instance that $f'$ has sub linear growth (i.e. $|f'(x)| \leq c|x|^{\alpha} +d$ for some $\alpha < 1$) or that $\phi$ is positive, we find that $G(+\infty) = +\infty$.
Therefore, $G$ cancels, which concludes the proof.

\subsection{Stability conditions}
\label{sec:stability}
We consider $W_i = \Lambda (A_i)^{-1}$ where $\Lambda$ is such that the gradients $\mathcal{G}^i$ all cancel. We consider a small relative perturbation of $W_i $ of the form $W_i \leftarrow (I_k + E^i)W_i$, and consider the effect on the gradient.
We define $\Delta^i=\mathcal{G}^i\left((I_k + E^1)W_1, \dots, (I_k + E^m)W_m\right)$.
Denoting $C = \frac{1 - 1/ m}{\sigma^2}$ and $\tilde{\nb} = \frac1m\sum_{i=1}^m \nb_i$, we find:
\begin{align}
     &\Delta^i=\underbrace{\frac1m f'\left(\Lambda(\sbb + \tilde{\nb}) + \frac1m \sum_{j=1}^m E^j\Lambda(\sbb +\nb^j)\right)(\sbb +\nb_i)^{\top}\Lambda(I_k + E^i)^{\top} }_{\Delta_1^i} +\\
     &C\underbrace{\left(\Lambda\nb_i - \frac{1}{m-1}\sum_{j\neq i} \Lambda\nbb^j + E^i\Lambda(\sbb + \nb_i) - \frac{1}{m-1}\sum_{j\neq i} E^j \Lambda(\sbb + \nb^j)\right)(\sbb + \nb_i)^{\top}\Lambda(I_k + E^i)^{\top}}_{\Delta_2^i} \\
     &- I_k\\
\end{align}

The first term is expanded at the first order, denoting $S = \sum_{j=1}^m E^j$:

\begin{align}
    \Delta_1^i &= \frac1m \left(f'(\Lambda(\sbb + \tilde{\nb})) + f''(\Lambda(\sbb + \tilde{\nb}))\odot \left(\frac1m \sum_{j=1}^m E^j\Lambda(\sbb +\nb^j)\right)\right)(\sbb +\nb_i)^{\top}\Lambda(I_k + E^i)^{\top}\\
    &=\frac1m f'(\Lambda(\sbb + \tilde{\nb}))(\sbb + \nb_i)^{\top}\Lambda(I_k + E^i)^{\top} + \frac1{m^2}S\odot  \left(f''(\Lambda(\sbb + \tilde{\nb}))(\sbb^2)^{\top}\Lambda^2 \right)\\
    &+\frac{1}{m^2}E^i\odot\left(f''(\Lambda(\sbb + \tilde{\nb}))((\nb_i)^2)^{\top}\Lambda^2 \right)
\end{align}
The symbol $\odot$ denotes the element-wise multiplication, $f'(\sbb) = \begin{bmatrix}f'(s_1) \\ \vdots \\ f'(s_k) \end{bmatrix}$ and $f''(\sbb) = \begin{bmatrix}f''(s_1) \\ \vdots \\ f''(s_k) \end{bmatrix}$.
Similarly, the second term gives at the first order: 
\begin{align}
    \Delta_2^i &= \sigma'^2\Lambda^2(I_k + E^i)^{\top} + (1 + \sigma'^2)E^i\Lambda^2 - \frac{1}{m-1} (S - E^i) \Lambda^2
\end{align}

Combining this, we find:

\begin{align}
 \Delta^i = (E^i)^{\top} + E^i \odot\Gamma^E
 +S\odot \Gamma^S
\end{align}
where 
$$
\Gamma^E= \left(\frac1{m^2}f''(\Lambda(\sbb + \tilde{\nb}))((\nb_i)^2)^{\top} + (1-\frac1m)\frac{\sigma'^2}{\sigma^2} + \frac{1}{\sigma^2} \right)\Lambda^2
$$
$$
\Gamma^S =\left(\frac1{m^2}f''(\Lambda(\sbb + \tilde{\nb}))(\sbb^2)^{\top} -\frac{1}{m\sigma^2}  \right)\Lambda^2
$$

are $k\times k$ matrices, independent of the subject.
This linear operator is the Hessian block corresponding to the $i$-th subject:
Denoting $\mathcal{H}$ the Hessian, it is the mapping $\mathcal{H}(E^1, \dots, E^m) = (\Delta^1, \dots, \Delta^m)$.

The coefficient $\Delta^i_{ab}$ only depends on $(E^i_{ab}, E^i_{ba}, E^1_{ab},\dots, E^m_{ab})$. Therefore, the Hessian is block diagonal with respect to the blocks of coordinates $(E^1_{ab}, E^1_{ba}, \dots, E^m_{ab}, E^m_{ba})$. Denote $\varepsilon = \Gamma^E_{ab}$, $\varepsilon' = \Gamma^E_{ba}$, $\beta = \Gamma^S_{ab}$ and $\beta'= \Gamma^S_{ba}$. The linear operator for the block is:

$$
K(\varepsilon, \varepsilon', \beta, \beta')=
\left(
    \begin{array}{ll|ll|l|ll}
\varepsilon + \beta & 1       & \beta & 0       & \dots  & \beta & 0       \\
1      & \varepsilon' + \beta' & 0      & \beta' & \dots  & 0      & \beta' \\
\hline
\beta & 0       & \varepsilon + \beta & 1       &        & \beta & 0       \\
0      & \beta' & 1      & \varepsilon' + \beta' & \ddots & 0      & \beta' \\
\hline
\vdots & \vdots  &        & \ddots  & \ddots & \vdots & \vdots  \\
\hline
\beta & 0       & \beta & 0       & \dots  & \varepsilon + \beta & 1       \\
0      & \beta' & 0      & \beta' & \dots  & 1      & \varepsilon' + \beta'
    \end{array}
\right)
$$
The positivity of $\mathcal{H}$ is equivalent to the positivity of this operator for all pairs $a, b$.
We now assume $\beta \beta' > 0$.

First, we should note that $K(\varepsilon, \varepsilon', \beta, \beta') $ is congruent to $K(\varepsilon \sqrt{\frac{\beta'}{\beta}}, \varepsilon' \sqrt{\frac{\beta}{\beta'}}, \sqrt{\beta\beta'}, \sqrt{\beta\beta'})$ via the basis $\text{diag}((\frac{\beta'}{\beta})^{1/4}, (\frac{\beta}{\beta'})^{1/4}, \cdots,(\frac{\beta'}{\beta})^{1/4}, (\frac{\beta}{\beta'})^{1/4})$.
%
We denote to simplify notation $\alpha = \varepsilon \sqrt{\frac{\beta'}{\beta}}$, $\alpha' = \varepsilon' \sqrt{\frac{\beta}{\beta'}}$ and $\gamma = \sqrt{\beta\beta'}$. We only have to study the positivity of $K(\alpha, \alpha', \gamma, \gamma)$.
We have:
$$
K(\alpha, \alpha', \gamma, \gamma) =  I_m  \otimes M_\alpha+ \gamma  \mathbb{1}\otimes I_2, \enspace M_\alpha = 
\begin{pmatrix}
\alpha & 1 \\
1 & \alpha'
\end{pmatrix}
$$
Since $I_m\otimes M_\alpha$ and $\gamma \mathbb{1}\otimes I_2$ commute, the minimum value of $\text{Sp}(K)$ is $\text{min}(I_m\otimes M_\alpha) + \text{min}(\gamma\text{Sp}(\mathbb{1}))=\frac12(\alpha + \alpha' - \sqrt{(\alpha - \alpha')^2 + 4}) + m\min(0, \gamma)$.
Since we assumed $\beta \beta' > 0$ we have $\gamma > 0$. This is similar to the usual ICA case, we find that the condition is $\alpha\alpha' > 1$.

If the following conditions hold for all pair of components $a, b$, the components are a local minimum of the cost function:
\begin{itemize}
    \item $\Gamma^S_{ab}\Gamma^S_{ba}\geq 0$
    \item $\Gamma^E_{ab}\Gamma^E_{ba} > 1$
\end{itemize}
\section{Identifiability for Shared Response Model}
\label{sec:app_identifiability}
The shared response model~\cite{chen2015reduced} (SRM) models the data $\xb_i \in \bbR^v$ of subject $i$ for $i = 1,\dots, m$ as
\begin{align*}
    \xb_i = A_i \sbb + \nb_i \enspace \text{with} \enspace \sbb \sim \Ncal(0, \Sigma), \enspace\nb_i \sim \Ncal(0, \rho_i^2 I_v), \enspace {A_i}^{\top}A_i = I_k
\end{align*}
where $A_i \in \bbR^{v, k}$, $\sbb \in \bbR^p$ and  $\Sigma \in \mathbb{R}^{k, k}$ is a symmetric positive definite matrix.

\begin{proposition}
SRM is not identifiable
\end{proposition}
\begin{proof}
Let us assume the data $\xb_i \enspace i=1, \dots, m$ follow the SRM model with parameters $\Sigma, A_i, \rho_i^2 \enspace i=1, \dots, m$. 

Let us consider an orthogonal matrix $O \in \Ocal_k$.
We call $A'_i = A_i O$ and $\Sigma' = O^{\top} \Sigma O$. 
$\Sigma'$ is trivially symmetric positive definite.

Then the data also follows the SRM model with different parameters $\Sigma', A'_i, \rho_i^2 \enspace i=1, \dots, m$.
\end{proof}

\begin{proposition}
We consider the decorrelated SRM model with an additional decorrelation assumption on the shared responses.
\begin{align*}
\xb_i = A_i \sbb + \nb_i \enspace \text{with} \enspace \sbb \sim \Ncal(0, \Sigma), \enspace\nb_i \sim \Ncal(0, \rho_i^2 I_v), \enspace {A_i}^{\top}A_i = I_k
\end{align*}
where $\Sigma$ is a positive \emph{diagonal} matrix. We further assume that the values in $\Sigma$ are all distinct and ranked in ascending order.
The decorrelated SRM is identifiable up to sign indeterminacies on the columns of 
$\begin{bmatrix}
A_1 \\
\vdots \\
A_m
\end{bmatrix}
$.
\end{proposition}
\begin{proof}
The decorrelated SRM model can be written
\begin{align*}
    &\xb_i \sim \Ncal(0, A_i \Sigma {A_i}^{\top} + \rho_i^2 I_v) \enspace \text{with}\enspace  {A_i}^{\top}A_i = I_k
\end{align*}
where $\Sigma$ is a positive diagonal matrix with distincts values ranked in ascending order.

Let us assume the data $\xb_i \enspace i=1, \dots, m$ follow the decorrelated SRM model with parameters $\Sigma, A_i, {\rho_i}^2 \enspace i=1, \dots, m$. Let us further assume that the data $\xb_i \enspace i=1, \dots, m$ follow the decorrelated SRM model with an other set of parameters $\Sigma', A'_i, {\rho'_i}^2 \enspace i=1, \dots, m$.

Since the model is Gaussian, we look at the covariances.
We have for $i \neq j$
\begin{align*}
    \bbE[\xb_i\left(\xb_j\right)^{\top}] = A_i\Sigma {A^j}^{\top} = A'_i \Sigma'{A'^j}^{\top} \enspace, 
\end{align*}
The singular value decomposition is unique up to sign flips and permutation. Since eigenvalues are positive and ranked the only indeterminacies left are on the eigenvectors. For each eigenvalue a sign flip can occur simultaneously on the corresponding left and right eigenvector.

Therefore we have $\Sigma' = \Sigma$, $A_i = A'_i D^{ij}$ and $A^j = A'^j D^{ij}$ where $D^{ij} \in \bbR^{k, k}$ is a diagonal matrix with values in $\{-1, 1\}$. This analysis holds for every $j \neq i$ and therefore $D^{ij} = D$ is the same for all subjects.

We also have for all $i$
\begin{align*}
    \bbE[\xb_i \left(\xb_i\right)^{\top}] = A_i \Sigma {A_i}^{\top} + \rho_i^2 I_v =  A'_i \Sigma' {A'_i}^{\top}  + {\rho'}_i^2 I_v\\
\end{align*}
We therefore conclude ${\rho'}_i^2 = \rho_i^2, i=1 \dots m$.

Note that if the diagonal subject specific noise covariance $\rho_i^2 I_v$ is replaced by any positive definite matrix, the model still enjoys identifiability.
\end{proof}

\section{fMRI experiments}
\label{sec:app_expts}
\subsection{Dataset description and preprocessing}
\label{preprocessing}
The full brain mask used to select brain regions is available in the Python package associated with the paper.

\paragraph{Sherlock}
In \emph{sherlock} dataset, 17 participants are watching "Sherlock" BBC TV show (beginning of episode 1). 
%
These data are downloaded from \url{http://arks.princeton.edu/ark:/88435/dsp01nz8062179}. 
%
Data were acquired using a 3T scanner with an isotropic spatial resolution of 3 mm. 
%
More information including the preprocessing pipeline is available in~\cite{chen2017shared}.
%
Subject 5 is removed because of missing data leaving us with 16 participants.
%
Although \emph{sherlock} data are downloaded as a temporal concatenation of two runs, we split it manually into 4 runs of 395 timeframes and one run of 396 timeframes so that we can perform 5 fold cross-validation in our experiments.


\paragraph{FORREST}
In FORREST dataset 20 participants are listening to an audio version of the Forrest Gump  movie.
%
FORREST data are downloaded from OpenfMRI~\cite{poldrack2013toward}. 
%
Data were acquired using a 7T scanner with an isotropic spatial resolution of 1~mm (see more details in~\cite{hanke2014high}) and resampled to an isotropic spatial resolution of 3~mm.
%
More information about the forrest project can be found at \url{http://studyforrest.org}.
%
Subject 10 is discarded because not all runs available for other subjects were available for subject 10 at the time of writing.
%
Run 8 is discarded because it is not present in most subjects.
 
\paragraph{RAIDERS}
In RAIDERS dataset, 11 participants are watching the movie "Raiders of the lost ark".
% 
The RAIDERS dataset belongs to the Individual Brain Charting dataset~\cite{ibc}.
% 
Data were acquired using a 3T scanner and resampled to an isotropic spatial resolution of 3~mm.
% 
The RAIDERS dataset reproduces the protocol described in~\cite{haxby2011common}.
%
Preprocessing details are described in~\cite{ibc}.

\paragraph{CLIPS}
In CLIPS dataset, 12 participants are exposed to short video clips. 
%
The CLIPS dataset also belongs to the Individual Brain Charting dataset (\cite{ibc}).
%
Data were acquired using a 3T scanner and resampled to an isotropic spatial resolution of 3~mm.
%
It reproduces the protocol of original studies described in \cite{nishimoto2011reconstructing} and \cite{huth2012continuous}.
%
Preprocessing details are described in~\cite{ibc}.

At the time of writing, the CLIPS and RAIDERS dataset from the individual brain charting dataset \url{https://project.inria.fr/IBC/} are available at \url{https://openneuro.org/datasets/ds002685}.
%
Protocols on the visual stimuli presented are available in a dedicated repository on Github: \url{https://github.com/hbp-brain-charting/public_protocols}.
% The informed consent of all subjects was obtained before scanning.
% Bertrand: There is no reason you deal with that

\subsection{Reconstructing the BOLD signal of missing subjects: Discussion on ROIs choice}
\label{brainmaps}

The quality of the reconstructed BOLD signal varies depending on the choice of the region of interest. In Figure~\ref{fig:brainmaps}, we plot for GroupICA, SRM and MultiViewICA, the R2 score per voxel using 50 components for datasets \emph{sherlock}, \emph{forrest}, \emph{raiders} and \emph{clips}. As could be anticipated from the task definition, \emph{forrest} obtains high reconstruction accuracy in the auditory cortices, while \emph{clips} shows good reconstruction in the visual cortex (occipital lobe mostly); the richer \emph{sherlock} and \emph{raiders} datasets yield good reconstructions in both domains, but also in other systems (language, motor).
%
We also see visually see that data reconstructed by MultiViewICA are
a better approximation of the original data than other methods.
%
This is particularly obvious for the \emph{clips} datasets where it is
clear that voxels in the posterior part of the superior
temporal sulcus are better recovered by MultiViewICA than by SRM or
GroupICA.

In order to determine the ROIs, we focus on the R2 score per voxel between the BOLD signal reconstructed by GroupICA and the actual bold signal. We run GroupICA with $10, 20$ and $50$ components and select the voxels that obtained a positive R2 score for all sets of components.
%
We discard voxels with an R2 score above 80\% as they visually correspond to artefacts and apply a binary opening using a unit cube as the structuring element. The chosen regions are plotted in figure~\ref{fig:roi}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/mvica/reconstruction_score_fullbrain.pdf}
  \caption{\textbf{Reconstructing the BOLD signal of missing subjects: Reconstruction R2 score per voxel} We plot for GroupICA, SRM and MultiViewICA, the R2 score per voxel using 50 components for datasets \emph{sherlock}, \emph{forrest}, \emph{raiders} and \emph{clips}. We visually see that data reconstructed by MultiViewICA are more faithful reproduction of the original data than other methods.}
  \label{fig:brainmaps}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/mvica/reconstruction_score_roi.pdf}
  \caption{\textbf{Data-driven choice of ROI} Chosen ROIs for the experiment: Reconstructing the BOLD signal of missing subjects.}
  \label{fig:roi}
\end{figure}

\subsection{Between-runs time-segment matching}
\label{app_spatialmaps}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/mvica/swetha_exp_full_fit.pdf}
  \caption{\textbf{Between runs time-segment matching}. Interesting components correlates more when they correspond to the same stimulus (same scenes of the movie) than when they correspond to distinct stimuli (different scenes).
  %We show that when MultiView ICA is used, we can locate a target time-segment in one session using the data of another session corresponding to the same stimuli.
  We extract 20 components and report the mean accuracy of the 3 best performing components}
  \label{fig:swetha}
\end{figure}

We measure the ability of each algorithm to extract meaningful shared components that correlate more when they correspond to the same stimulus than when they correspond to distinct stimuli. We use the \emph{raiders-full} dataset, which allows this kind of analysis because subjects watch some selected scenes from the movie twice, during the first two runs (1 and 2) and the last two (11 and 12).
%
First, the forward operators are learned by fitting each algorithm with 20 components on the data of all 11 subjects using all 12 runs. We then select a subset of 8 subjects and the shared components are computed by applying the forward operators and averaging.
%
We select a large target time-segment ($50$
timeframes) taken at random from run 1 and 2, and we try to localize the corresponding sample time-segment from the 10 last runs using a single component of the shared components.
%
The time-segment is said to be
correctly classified if the correlation between the target and corresponding sample
time-segment is higher than with any other time-segment (partially overlapping windows are excluded).
%
In contrast to the \emph{between subject time-segment matching} experiment, we obtain one accuracy score per component.
%
We repeat the experiment 10 times with different subsets of subjects randomly chosen and report the mean accuracy of the 3 best performing components in Figure~\ref{fig:swetha}. Error bars correspond to a 95~\% confidence interval.
%
MultiView ICA achieves the highest accuracy.

We then focus on the 3 best performing components of MultiView ICA. For each component, we plot in Figure~\ref{fig:app_spatialmaps} (left) the shared components during two sets of runs where subjects were exposed to the same scenes of the movie. We then study the localisation of these components.
%
We average the forward operators across subjects and plot the columns corresponding to the components of interest in Figure~\ref{fig:app_spatialmaps} (right).
%
As each column is seen as a set of weights over all voxels, it represents a spatial map.

The component 1 of the shared responses follows almost the same pattern in the two set of runs corresponding to the same scenes of the movie. The spatial map corresponding to component 1 highlights the language network.
%
In component 2, the temporal patterns during the viewing of identical scenes are also very similar. The corresponding spatial map highlights the visual network especially the visual dorsal pathway.
%
In component 3, there exists a similarity however less striking than with the two previous components. The corresponding spatial map highlights a contrast between the spatial attention network and the auditory network.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/mvica/swetha_exp_full_fit_appendix.pdf}
  \caption{\textbf{Between-runs time segment matching: spatial maps and timecourses} \emph{Left:} Timecourses of the 3 shared components yielding the highest accuracy. The two displayed set of runs correspond to the same scenes in the movie. \emph{Right:} Localisation of the same shared components in the brain}
  \label{fig:app_spatialmaps}
\end{figure}

\subsection{Reproducing time-segment matching experiment}
\label{appendix_reproduce}
We reproduce the time-segment matching experiments described in \cite{chen2016convolutional} and \cite{zhang2016searchlight} and use two fold classification over runs instead of 5-fold as we have done in the main paper. We used the sherlock data available at \url{http://arks.princeton.edu/ark:/88435/dsp01nz8062179} and the full brain mask provided in the Python package associated with the paper. We applied high-pass filtering (140 s cutoff) and the time series of each voxel were normalized to zero mean and unit variance.

The results are available in Figure~\ref{fig:supp_timesegment}.

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figures/mvica/timesegment_matching_cae.pdf}
  \caption{\textbf{Reproducing the time-segment matching experiment of \cite{chen2016convolutional}~\cite{zhang2016searchlight}} Mean classification accuracy - error bars represent 95\% confidence interval}
  \label{fig:supp_timesegment}
\end{figure}

\subsection{Impact of the hyperparameter $\sigma$ }
\label{sec:app_sigma_impact}
On top of the theoretical guarantees about the robustness of our method to the choice of the $\sigma$ parameter, we investigate its practical impact on the time-matching segment experiment, on the Sherlock dataset with $10$ components.
%
We compute the accuracy of the multi-view ICA pipeline with different choice of $\sigma$.
%
This is reported in Fig.~\ref{fig:supp_noise_sensitivity}. 
%
The accuracy is constant for a wide range of $\sigma$, only decreasing when $\sigma$ attains very high values.
\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figures/mvica/noise_sensitivity_reb.pdf}
  \caption{\textbf{Effect of the parameter $\sigma$}: We compute the accuracy of the multiview-ICA pipeline on the time-segment matching experiment for various values of the $\sigma$ hyperparameter over a grid. The accuracy varies only marginally with $\sigma$.}
  \label{fig:supp_noise_sensitivity}
\end{figure}

\section{Related Work}
\label{sec:app_rel_work}
The following table describes some usual method for extracting shared components from multiple subjects datasets.
The column "Modality/Components" describes the type of data for which each algorithm was \emph{initially} proposed, even though each algorithm could be applied on any type of data. 
%
The components type can be either temporal if extracted components are time courses or spatial if they are spatial patterns. 
%
\begin{center}
\begin{longtable}{ |p{.15\textwidth} | p{.2\textwidth} |p{.2\textwidth}| p{.3\textwidth}|}
\hline
\textbf{Method} & \textbf{Modality/Components} &\textbf{Dimension reduction} & \textbf{Description}  \\
\hline
SRM \cite{chen2015reduced} & 
fMRI/Temporal
&
SRM
&
The model is $\xb_i = A_i\sbb + \nb_i$, with \emph{Gaussian} components and \emph{orthogonal} mixing matrices $A_i$\\
\hline
GroupPCA~\cite{smith2014group} &
fMRI/Spatial
&
GroupPCA
&
A memory efficient implementation of PCA applied on temporally concatenated data.\\
\hline
GIFT~ \cite{calhoun2001method} & 
fMRI/Spatial
&
Individual PCA + Group PCA (on component-wise concatenated data)
&
Single-subject ICA is applied on the aggregated data\\
\hline
EEGIFT~ \cite{eichele2011eegift} & 
EEG/Temporal
&
Individual PCA + Group PCA (on component-wise concatenated data)
&
Single-subject ICA is applied on the aggregated data\\
\hline
PermICA &
Any
&
Any
&
Single-subject ICA is applied on each subject's data, and the components are matched using the Hungarian algorithm\\
\hline
Clustering approach~\cite{esposito2005independent}&
fMRI/Spatial
&
Individual PCA
&
Single-subject ICA is applied on each subject's data, and the components are matched using a hierarchical clustering algorithm.\\
\hline
Measure projection analysis~\cite{bigdely2013measure}&
EEG/Temporal
&
Individual PCA
&
Single-subject ICA is applied on each subject's data, and the components are matched using a hierarchical clustering algorithm.\\
\hline
TensorICA \cite{beckmann2005tensorial} &
fMRI/Spatial
&
Group PCA (on spatially concatenated data)
&
TensorICA incorporates ICA assumptions into the PARAFAC model. The mixing matrices $A_1 \cdots A_n$ are such that $A_i = A D_i$ where $A$ is common to all subjects and $D_i$ are subject specific diagonal matrices.\\  
\hline
Unifying Approach of \cite{guo2008unified} &
fMRI/Spatial
&
Group PCA (on spatially concatenated data) + GroupPCA (on component-wise concatenated data).
&
The model is $\xb_i = A_i\sbb + \nb_i$ with a Gaussian mixture model on independent components and a matrix normal prior on the noise. \\
\hline
SR-ICA \cite{zhang2016searchlight} &
fMRI/Temporal
&
SR-ICA
&
SR-ICA incorporates ICA assumptions into the shared response model.  \\
\hline
CAE-SRM \cite{chen2016convolutional}
&
fMRI/Temporal
&
CAE-SRM
&
A convolutional auto-encoder is used to perform the unmixing. \\  
\hline
CanICA \cite{varoquaux2009canica} &
fMRI/Spatial
&
Individual PCA + multi set CCA (on component-wise concatenated data)
&
CanICA applies single-subject ICA on data reduced with PCA and CCA.
 \\  
\hline
Spatial ConcatICA~\cite{svensen2002ica} &
fMRI/Spatial
&
Group PCA (on spatially concatenated data)
&
ICA is applied on spatially concatenated data. The mixing is constrained to be the same across all subjects.
 \\  
 \hline
Temporal ConcatICA~\cite{cong2013validating} &
EEG/Temporal
&
Group PCA (on temporally concatenated data)
&
ICA is applied on temporaly concatenated data. The mixing is constrained to be the same across all subjects.
 \\  
\hline
coroICA \cite{pfister2019robustifying} &
Any
&
Any
&
The model is  $\xb_i = A\sbb_i + \nb_i$. The mixing is constrained to be the same across all subjects. \\
%\hline
%Arbitrary correlation factor analysis of~\cite{Monti18UAI} &
%fMRI/Temporal &
%Any &
%The probabilistic model of~\cite{Monti18UAI} assumes the mixing matrix is orthogonal, positive and shared across subjects but not necessarily square. It learns subjects specific co-variance matrices.  \\
\hline
\end{longtable}
\end{center}
\vspace{-10mm}
An additional related model is described in~\cite{gresele2019incomplete}. Similarly to our work, the ICA model has noise on the components side. However, the model involves nonlinear mixings, which are computationally unfeasible to optimize via maximum likelihood; a contrastive learning scheme is therefore adopted, and the likelihood is not derived in closed form. No evaluation on neuroimaging datasets is presented.

\section{Detailed Cam-CAN components}
\label{sec:app_montages}
We display each of the 11 shared components found by Multiview ICA on the Cam-CAN. The time-courses are on the left, the corresponding brain maps are on the right.


{\centering
\includegraphics[width=0.3\textwidth]{figures/mvica/camcan_source_0.pdf}%
\raisebox{0.2\height}{\includegraphics[width=0.68\textwidth]{figures/mvica/montage_0.png}} \\
\includegraphics[width=0.3\textwidth]{figures/mvica/camcan_source_1.pdf}%
\raisebox{0.2\height}{\includegraphics[width=0.68\textwidth]{figures/mvica/montage_1.png}} \\
\includegraphics[width=0.3\textwidth]{figures/mvica/camcan_source_2.pdf}%                        
\raisebox{0.2\height}{\includegraphics[width=0.68\textwidth]{figures/mvica/montage_2.png}} \\
\includegraphics[width=0.3\textwidth]{figures/mvica/camcan_source_3.pdf}%  
\raisebox{0.2\height}{\includegraphics[width=0.68\textwidth]{figures/mvica/montage_3.png}} \\
\includegraphics[width=0.3\textwidth]{figures/mvica/camcan_source_4.pdf}%
\raisebox{0.2\height}{\includegraphics[width=0.68\textwidth]{figures/mvica/montage_4.png}} \\
\includegraphics[width=0.3\textwidth]{figures/mvica/camcan_source_5.pdf}%
\raisebox{0.2\height}{\includegraphics[width=0.68\textwidth]{figures/mvica/montage_5.png}} \\
\includegraphics[width=0.3\textwidth]{figures/mvica/camcan_source_6.pdf}%
\raisebox{0.2\height}{\includegraphics[width=0.68\textwidth]{figures/mvica/montage_6.png}} \\
\includegraphics[width=0.3\textwidth]{figures/mvica/camcan_source_7.pdf}%
\raisebox{0.2\height}{\includegraphics[width=0.68\textwidth]{figures/mvica/montage_7.png}} \\
\includegraphics[width=0.3\textwidth]{figures/mvica/camcan_source_8.pdf}%
\raisebox{0.2\height}{\includegraphics[width=0.68\textwidth]{figures/mvica/montage_8.png}} \\
\includegraphics[width=0.3\textwidth]{figures/mvica/camcan_source_9.pdf}%
\raisebox{0.2\height}{\includegraphics[width=0.68\textwidth]{figures/mvica/montage_9.png}} \\
\includegraphics[width=0.3\textwidth]{figures/mvica/camcan_source_10.pdf}%
\raisebox{0.2\height}{\includegraphics[width=0.68\textwidth]{figures/mvica/montage_10.png}} \\
}

\section{Average forward operators on fMRI datasets}
\label{sec:spatial_maps}
We display the average forward operator across subjects on the Raiders, Forrest, Clips and Sherlock datasets obtained with MultiViewICA and GroupICA with 5 components. A 5~mm spatial smoothing was applied on all datasets, and the confound signals corresponding to the 5 components with the highest variance were removed before applying MultiViewICA or GroupICA.

{\centering

  \includegraphics[width=\textwidth]{figures/mvica/maps_forrest.png} \\
  \includegraphics[width=\textwidth]{figures/mvica/maps_sherlock.png} \\
  \includegraphics[width=\textwidth]{figures/mvica/maps_gallant.png} \\
  \includegraphics[width=\textwidth]{figures/mvica/maps_raiders.png} \\
}

\section{Synthetic benchmark using additive noise on the sensors}
\label{app:complex_cov}
We generate data according to the model $\xb_i = A_i\sbb + \nb_i$, where $\xb_i \in \mathbb{R}^{50}$, $\sbb \in \mathbb{R}^{20}$, and $\nb_i\sim \mathcal{N}(0, \sigma^2 I_{50})$. After applying individual PCA to obtain signals of dimension $20$, we apply the different ICA algorithms and report the reconstruction error in fig.~\ref{fig:reconstruction_synth}.

\begin{figure}
  \center
  \includegraphics[width=0.5\linewidth]{figures/mvica/distance.pdf}
  \caption{Synthetic experiment with model $\xb_i = A_i\sbb^i + \nb_i$} %{Light Unit}
  \label{fig:reconstruction_synth}
\end{figure}

\section{Summary of our quantitative results}
\label{sec:app_real_data}
Our quantitative results for the fMRI experiments of time-segment matching and BOLD signal reconstruction and on for the MEG phantom data experiment are summarized, respectively, in Table~\ref{tab:timeseg}, Table~\ref{tab:recon} and Table~\ref{tab:meg}. All methods are compared upon extraction of components with the same dimensionality ($20$ components).

\begin{table}
    \centering
    \begin{tabular}{|c|c | c | c|}
            \hline
         \textbf{Dataset} & \textbf{Method} & \textbf{Accuracy} & \textbf{Confidence interval} \\
         \hline
clips   & Chance      & 0.002&[0.001, 0.003] \\
        & CanICA    & 0.130&[0.112, 0.147] \\
        & PCA + GroupICA      & 0.124&[0.109, 0.139] \\
        & GroupICA    & 0.152&[0.133, 0.171] \\

        & PermICA     & 0.147&[0.126, 0.169] \\
        & SRM         & 0.115&[0.104, 0.126] \\
        & MultiViewICA& \textbf{0.167}&[0.142, 0.192] \\
        \hline
forrest & Chance      & 0.002&[0.001, 0.002] \\
        & CanICA    & 0.192&[0.170, 0.214] \\
        & PCA + GroupICA      & 0.088&[0.077, 0.098] \\
        & GroupICA    & 0.154&[0.137, 0.170] \\
        & PermICA     & 0.135&[0.118, 0.152] \\
        & SRM         & 0.188&[0.173, 0.203] \\
        & MultiViewICA& \textbf{0.448}&[0.411, 0.484] \\
        \hline
raiders & Chance      & 0.002&[0.001, 0.003] \\
        & CanICA    & 0.256&[0.220, 0.291] \\
        & PCA + GroupICA      & 0.331&[0.289, 0.372] \\
        & GroupICA    & 0.321&[0.281, 0.361] \\
        & PermICA     & 0.381&[0.341, 0.421] \\
        & SRM         & 0.265&[0.240, 0.289] \\
         & MultiViewICA& \textbf{0.408}&[0.358, 0.458] \\
         \hline
sherlock& Chance      & 0.005&[0.003, 0.006] \\
        & CanICA    & 0.607&[0.567, 0.648] \\
        & PCA + GroupICA      & 0.454&[0.416, 0.492] \\
        & GroupICA    & 0.519&[0.481, 0.556] \\
        & PermICA     & 0.399&[0.365, 0.434] \\
        & SRM         & 0.493&[0.465, 0.520] \\
        & MultiViewICA& \textbf{0.873}&[0.844, 0.903] \\
\hline
    \end{tabular}
    \caption{Timesegment matching: Summary of our quantitative results. We report the mean accuracy across cross-validation splits.}
    \label{tab:timeseg}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|c|c | c | c|}
            \hline
         \textbf{Dataset} & \textbf{Method} & \textbf{R2 score} & \textbf{Confidence interval} \\
         \hline
         clips   & Chance              & 0.000&[0.000 ,0.000] \\
        & CanICA            &  0.110&[ 0.097 , 0.123] \\
        & PCA + GroupICA              &  0.075&[ 0.058 , 0.092] \\
        & GroupICA            &  0.077&[ 0.059 , 0.094] \\
        & PermICA             &  0.099&[ 0.087 , 0.111] \\
        & SRM                 &  0.081&[ 0.069 , 0.094] \\
        & MultiViewICA        &  \textbf{0.114}&[ 0.099 , 0.128] \\
        \hline
forrest & Chance              & 0.000&[0.000 ,0.000] \\
        & CanICA            &  0.181&[ 0.169 , 0.193] \\
        & PCA + GroupICA              &  0.072&[ 0.054 , 0.090] \\
        & GroupICA            &  0.081&[ 0.062 , 0.099] \\
        & PermICA             &  0.098&[ 0.090 , 0.106] \\
        & SRM                 &  0.180&[ 0.168 , 0.193] \\
        & MultiViewICA        &  \textbf{0.191}&[ 0.177 , 0.204] \\
        \hline
raiders & Chance              & 0.000&[0.000 ,0.000] \\
        & CanICA            &  0.136&[ 0.122 , 0.149] \\
        & PCA + GroupICA              &  0.063&[ 0.045 , 0.080] \\
        & GroupICA            &  0.062&[ 0.043 , 0.081] \\
        & PermICA             &  0.107&[ 0.091 , 0.124] \\
        & SRM                 &  0.138&[ 0.121 , 0.154] \\
        & MultiViewICA        &  \textbf{0.144}&[ 0.124 , 0.164] \\
        \hline
sherlock& Chance              & 0.000&[0.000 ,0.000] \\
        & CanICA            &  0.156&[ 0.141 , 0.172] \\
        & PCA + GroupICA              &  0.087&[ 0.065 , 0.108] \\
        & GroupICA            &  0.091&[ 0.070 , 0.112] \\
        & PermICA             &  0.067&[ 0.055 , 0.078] \\
        & SRM                 &  \textbf{0.164}&[ 0.147 , 0.181] \\
        & MultiViewICA        &  0.161&[ 0.142 , 0.180] \\
        \hline

         
    \end{tabular}
    \caption{Reconstructing the BOLD signal of missing subjects: Summary of our quantitative results. We report the mean R2 score across cross-validation splits.}
    \label{tab:recon}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c}
    \hline
         \textbf{Method} & \textbf{Reconstruction error} & \textbf{1st and 3d quartiles} 
         \\
         \hline
         MultiViewICA & \textbf{0.0045} & [0.0039, 0.0052] \\ 
GroupICA & 0.1098 & [0.0549, 0.1734] \\ 
PCA+GroupICA & 0.1111 & [0.0760, 0.1502] \\ 
PermICA & 0.0730 & [0.0423, 0.1037] \\ 
\hline
    \end{tabular}
    \caption{Phantom MEG data: Summary of our quantitative results with 2 epochs. We report the median reconstruction error across cross-validation splits.}
    \label{tab:meg}
\end{table}

